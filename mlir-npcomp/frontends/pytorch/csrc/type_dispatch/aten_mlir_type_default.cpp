//===- aten_mlir_type_default.cpp -------------------------------*- C++ -*-===//
//
// This file is licensed under a pytorch-style license
// See frontends/pytorch/LICENSE for license information.
//
//===----------------------------------------------------------------------===//

#include "aten_mlir_type_default.h"

#include <ATen/CPUGenerator.h>
#include <ATen/Context.h>
#include <ATen/Functions.h>
#include <ATen/core/op_registration/op_registration.h>

#include "aten_mlir_bridge.h"
#include "aten_mlir_type.h"

namespace torch_mlir {

at::Tensor ATenMLIRTypeDefault::_cast_Byte(const at::Tensor &self,
                                           bool non_blocking) {
  std::cout << "aten::_cast_Byte" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Byte(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cast_Char(const at::Tensor &self,
                                           bool non_blocking) {
  std::cout << "aten::_cast_Char" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Char(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cast_Double(const at::Tensor &self,
                                             bool non_blocking) {
  std::cout << "aten::_cast_Double" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Double(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cast_Float(const at::Tensor &self,
                                            bool non_blocking) {
  std::cout << "aten::_cast_Float" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Float(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cast_Int(const at::Tensor &self,
                                          bool non_blocking) {
  std::cout << "aten::_cast_Int" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Int(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cast_Long(const at::Tensor &self,
                                           bool non_blocking) {
  std::cout << "aten::_cast_Long" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Long(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cast_Short(const at::Tensor &self,
                                            bool non_blocking) {
  std::cout << "aten::_cast_Short" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Short(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cast_Half(const at::Tensor &self,
                                           bool non_blocking) {
  std::cout << "aten::_cast_Half" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cast_Half(mlirtens[0], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

void ATenMLIRTypeDefault::backward(const at::Tensor &self,
                                   const at::Tensor &gradient, bool keep_graph,
                                   bool create_graph) {
  std::cout << "aten::backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, gradient};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  mlirtens[0].backward(mlirtens[1], keep_graph, create_graph);
}

void ATenMLIRTypeDefault::set_data(const at::Tensor &self,
                                   const at::Tensor &new_data) {
  std::cout << "aten::set_data" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, new_data};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  mlirtens[0].set_data(mlirtens[1]);
}

at::Tensor ATenMLIRTypeDefault::data(const at::Tensor &self) {
  std::cout << "aten::data" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].data();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

int64_t
ATenMLIRTypeDefault::_debug_has_internal_overlap(const at::Tensor &self) {
  std::cout << "aten::_debug_has_internal_overlap" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_debug_has_internal_overlap(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_fused_dropout(const at::Tensor &self, double p,
                                    at::Generator *generator) {
  std::cout << "aten::_fused_dropout" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_fused_dropout(mlirtens[0], p, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::_masked_scale(const at::Tensor &self,
                                              const at::Tensor &mask,
                                              double scale) {
  std::cout << "aten::_masked_scale" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_masked_scale(mlirtens[0], mlirtens[1], scale);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::_sobol_engine_draw(
    const at::Tensor &quasi, int64_t n, const at::Tensor &sobolstate,
    int64_t dimension, int64_t num_generated,
    c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::_sobol_engine_draw" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {quasi, sobolstate};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sobol_engine_draw(mlirtens[0], n, mlirtens[1],
                                           dimension, num_generated, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(quasi)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(quasi)));
}

at::Tensor &ATenMLIRTypeDefault::_sobol_engine_ff_(at::Tensor &self, int64_t n,
                                                   const at::Tensor &sobolstate,
                                                   int64_t dimension,
                                                   int64_t num_generated) {
  std::cout << "aten::_sobol_engine_ff_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, sobolstate};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sobol_engine_ff_(mlirtens[0], n, mlirtens[1],
                                          dimension, num_generated);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::_sobol_engine_scramble_(at::Tensor &self,
                                                         const at::Tensor &ltm,
                                                         int64_t dimension) {
  std::cout << "aten::_sobol_engine_scramble_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, ltm};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sobol_engine_scramble_(mlirtens[0], mlirtens[1], dimension);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &
ATenMLIRTypeDefault::_sobol_engine_initialize_state_(at::Tensor &self,
                                                     int64_t dimension) {
  std::cout << "aten::_sobol_engine_initialize_state_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sobol_engine_initialize_state_(mlirtens[0], dimension);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::_reshape_from_tensor(const at::Tensor &self,
                                                     const at::Tensor &shape) {
  std::cout << "aten::_reshape_from_tensor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, shape};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_reshape_from_tensor(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_shape_as_tensor(const at::Tensor &self) {
  std::cout << "aten::_shape_as_tensor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_shape_as_tensor(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::dropout(const at::Tensor &input, double p,
                                        bool train) {
  std::cout << "aten::dropout" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::dropout(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor &ATenMLIRTypeDefault::dropout_(at::Tensor &self, double p,
                                          bool train) {
  std::cout << "aten::dropout_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::dropout_(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::feature_dropout(const at::Tensor &input,
                                                double p, bool train) {
  std::cout << "aten::feature_dropout" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::feature_dropout(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor &ATenMLIRTypeDefault::feature_dropout_(at::Tensor &self, double p,
                                                  bool train) {
  std::cout << "aten::feature_dropout_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::feature_dropout_(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::alpha_dropout(const at::Tensor &input, double p,
                                              bool train) {
  std::cout << "aten::alpha_dropout" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::alpha_dropout(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor &ATenMLIRTypeDefault::alpha_dropout_(at::Tensor &self, double p,
                                                bool train) {
  std::cout << "aten::alpha_dropout_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::alpha_dropout_(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::feature_alpha_dropout(const at::Tensor &input,
                                                      double p, bool train) {
  std::cout << "aten::feature_alpha_dropout" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::feature_alpha_dropout(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor &ATenMLIRTypeDefault::feature_alpha_dropout_(at::Tensor &self,
                                                        double p, bool train) {
  std::cout << "aten::feature_alpha_dropout_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::feature_alpha_dropout_(mlirtens[0], p, train);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::abs(const at::Tensor &self) {
  std::cout << "aten::abs" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::abs(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::abs_(at::Tensor &self) {
  std::cout << "aten::abs_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::abs_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::abs_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::abs_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::abs_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::acos(const at::Tensor &self) {
  std::cout << "aten::acos" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::acos(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::acos_(at::Tensor &self) {
  std::cout << "aten::acos_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::acos_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::acos_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::acos_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::acos_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::avg_pool1d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
  std::cout << "aten::avg_pool1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::avg_pool1d(mlirtens[0], kernel_size, stride, padding,
                                   ceil_mode, count_include_pad);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor
ATenMLIRTypeDefault::adaptive_avg_pool1d(const at::Tensor &self,
                                         at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_avg_pool1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_avg_pool1d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::adaptive_max_pool1d(const at::Tensor &self,
                                         at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_max_pool1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_max_pool1d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::add(const at::Tensor &self,
                                    const at::Tensor &other, at::Scalar alpha) {
  std::cout << "aten::add" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::add(mlirtens[0], mlirtens[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::add_(at::Tensor &self, const at::Tensor &other,
                                      at::Scalar alpha) {
  std::cout << "aten::add_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].add_(mlirtens[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::add_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &other,
                                         at::Scalar alpha) {
  std::cout << "aten::add_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::add_out(mlirtens[0], mlirtens[1], mlirtens[2], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::add(const at::Tensor &self, at::Scalar other,
                                    at::Scalar alpha) {
  std::cout << "aten::add" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::add(mlirtens[0], other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::add_(at::Tensor &self, at::Scalar other,
                                      at::Scalar alpha) {
  std::cout << "aten::add_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].add_(other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::addmv(const at::Tensor &self,
                                      const at::Tensor &mat,
                                      const at::Tensor &vec, at::Scalar beta,
                                      at::Scalar alpha) {
  std::cout << "aten::addmv" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat, vec};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::addmv(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::addmv_(at::Tensor &self, const at::Tensor &mat,
                                        const at::Tensor &vec, at::Scalar beta,
                                        at::Scalar alpha) {
  std::cout << "aten::addmv_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat, vec};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::addmv_(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::addmv_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           const at::Tensor &mat,
                                           const at::Tensor &vec,
                                           at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::addmv_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mat, vec};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addmv_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                  mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::addr(const at::Tensor &self,
                                     const at::Tensor &vec1,
                                     const at::Tensor &vec2, at::Scalar beta,
                                     at::Scalar alpha) {
  std::cout << "aten::addr" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, vec1, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::addr(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::addr_(at::Tensor &self, const at::Tensor &vec1,
                                       const at::Tensor &vec2, at::Scalar beta,
                                       at::Scalar alpha) {
  std::cout << "aten::addr_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, vec1, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].addr_(mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::addr_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          const at::Tensor &vec1,
                                          const at::Tensor &vec2,
                                          at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::addr_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, vec1, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addr_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                 mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::affine_grid_generator(const at::Tensor &theta,
                                                      at::IntArrayRef size,
                                                      bool align_corners) {
  std::cout << "aten::affine_grid_generator" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {theta};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::affine_grid_generator(mlirtens[0], size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(theta));
}

at::Tensor ATenMLIRTypeDefault::affine_grid_generator_backward(
    const at::Tensor &grad, at::IntArrayRef size, bool align_corners) {
  std::cout << "aten::affine_grid_generator_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::affine_grid_generator_backward(mlirtens[0], size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::all(const at::Tensor &self, int64_t dim,
                                    bool keepdim) {
  std::cout << "aten::all" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::all(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::all_out(at::Tensor &out,
                                         const at::Tensor &self, int64_t dim,
                                         bool keepdim) {
  std::cout << "aten::all_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::all_out(mlirtens[0], mlirtens[1], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

bool ATenMLIRTypeDefault::allclose(const at::Tensor &self,
                                   const at::Tensor &other, double rtol,
                                   double atol, bool equal_nan) {
  std::cout << "aten::allclose" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::allclose(mlirtens[0], mlirtens[1], rtol, atol, equal_nan);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::any(const at::Tensor &self, int64_t dim,
                                    bool keepdim) {
  std::cout << "aten::any" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::any(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::any_out(at::Tensor &out,
                                         const at::Tensor &self, int64_t dim,
                                         bool keepdim) {
  std::cout << "aten::any_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::any_out(mlirtens[0], mlirtens[1], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::arange(at::Scalar end,
                                       const at::TensorOptions &options) {
  std::cout << "aten::arange" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::arange(end, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::arange(at::Scalar start, at::Scalar end,
                                       const at::TensorOptions &options) {
  std::cout << "aten::arange" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::arange(start, end, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::arange(at::Scalar start, at::Scalar end,
                                       at::Scalar step,
                                       const at::TensorOptions &options) {
  std::cout << "aten::arange" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::arange(start, end, step, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::arange_out(at::Tensor &out, at::Scalar end) {
  std::cout << "aten::arange_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::arange_out(mlirtens[0], end);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::arange_out(at::Tensor &out, at::Scalar start,
                                            at::Scalar end, at::Scalar step) {
  std::cout << "aten::arange_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::arange_out(mlirtens[0], start, end, step);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::_dim_arange(const at::Tensor &like,
                                            int64_t dim) {
  std::cout << "aten::_dim_arange" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {like};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_dim_arange(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(like));
}

at::Tensor ATenMLIRTypeDefault::argmax(const at::Tensor &self,
                                       c10::optional<int64_t> dim,
                                       bool keepdim) {
  std::cout << "aten::argmax" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::argmax(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::argmin(const at::Tensor &self,
                                       c10::optional<int64_t> dim,
                                       bool keepdim) {
  std::cout << "aten::argmin" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::argmin(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor
ATenMLIRTypeDefault::as_strided(const at::Tensor &self, at::IntArrayRef size,
                                at::IntArrayRef stride,
                                c10::optional<int64_t> storage_offset) {
  std::cout << "aten::as_strided" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::as_strided(mlirtens[0], size, stride, storage_offset);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &
ATenMLIRTypeDefault::as_strided_(at::Tensor &self, at::IntArrayRef size,
                                 at::IntArrayRef stride,
                                 c10::optional<int64_t> storage_offset) {
  std::cout << "aten::as_strided_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::as_strided_(mlirtens[0], size, stride, storage_offset);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::asin(const at::Tensor &self) {
  std::cout << "aten::asin" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::asin(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::asin_(at::Tensor &self) {
  std::cout << "aten::asin_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::asin_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::asin_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::asin_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::asin_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::atan(const at::Tensor &self) {
  std::cout << "aten::atan" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::atan(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::atan_(at::Tensor &self) {
  std::cout << "aten::atan_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::atan_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::atan_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::atan_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::atan_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::baddbmm(const at::Tensor &self,
                                        const at::Tensor &batch1,
                                        const at::Tensor &batch2,
                                        at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::baddbmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, batch1, batch2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::baddbmm(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::baddbmm_(at::Tensor &self,
                                          const at::Tensor &batch1,
                                          const at::Tensor &batch2,
                                          at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::baddbmm_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, batch1, batch2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].baddbmm_(mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::_baddbmm_mkl_(at::Tensor &self,
                                               const at::Tensor &batch1,
                                               const at::Tensor &batch2,
                                               at::Scalar beta,
                                               at::Scalar alpha) {
  std::cout << "aten::_baddbmm_mkl_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, batch1, batch2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_baddbmm_mkl_(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::baddbmm_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &batch1,
    const at::Tensor &batch2, at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::baddbmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, batch1, batch2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::baddbmm_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                    mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::bartlett_window(int64_t window_length,
                                     const at::TensorOptions &options) {
  std::cout << "aten::bartlett_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bartlett_window(window_length, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor
ATenMLIRTypeDefault::bartlett_window(int64_t window_length, bool periodic,
                                     const at::TensorOptions &options) {
  std::cout << "aten::bartlett_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bartlett_window(window_length, periodic, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::batch_norm(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    const at::Tensor &running_mean, const at::Tensor &running_var,
    bool training, double momentum, double eps, bool cudnn_enabled) {
  std::cout << "aten::batch_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias, running_mean,
                                              running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::batch_norm(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3],
                     mlirtens[4], training, momentum, eps, cudnn_enabled);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, int64_t>
ATenMLIRTypeDefault::_batch_norm_impl_index(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    const at::Tensor &running_mean, const at::Tensor &running_var,
    bool training, double momentum, double eps, bool cudnn_enabled) {
  std::cout << "aten::_batch_norm_impl_index" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias, running_mean,
                                              running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_batch_norm_impl_index(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4], training,
      momentum, eps, cudnn_enabled);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor, int64_t>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)),
      std::get<3>(x_result));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_batch_norm_impl_index_backward(
    int64_t impl_index, const at::Tensor &input, const at::Tensor &grad_output,
    const at::Tensor &weight, const at::Tensor &running_mean,
    const at::Tensor &running_var, const at::Tensor &save_mean,
    const at::Tensor &save_var_transform, bool train, double eps,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::_batch_norm_impl_index_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      input,     grad_output,       weight, running_mean, running_var,
      save_mean, save_var_transform};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_batch_norm_impl_index_backward(
      impl_index, mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3],
      mlirtens[4], mlirtens[5], mlirtens[6], train, eps, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor ATenMLIRTypeDefault::bernoulli(const at::Tensor &self,
                                          at::Generator *generator) {
  std::cout << "aten::bernoulli" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bernoulli(mlirtens[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::bernoulli_out(at::Tensor &out,
                                               const at::Tensor &self,
                                               at::Generator *generator) {
  std::cout << "aten::bernoulli_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bernoulli_out(mlirtens[0], mlirtens[1], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::bernoulli_(at::Tensor &self,
                                            const at::Tensor &p,
                                            at::Generator *generator) {
  std::cout << "aten::bernoulli_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, p};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].bernoulli_(mlirtens[1], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::bernoulli_(at::Tensor &self, double p,
                                            at::Generator *generator) {
  std::cout << "aten::bernoulli_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].bernoulli_(p, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::bernoulli(const at::Tensor &self, double p,
                                          at::Generator *generator) {
  std::cout << "aten::bernoulli" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bernoulli(mlirtens[0], p, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::bilinear(const at::Tensor &input1,
                                         const at::Tensor &input2,
                                         const at::Tensor &weight,
                                         const at::Tensor &bias) {
  std::cout << "aten::bilinear" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input1, input2, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::bilinear(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input1));
}

at::Tensor ATenMLIRTypeDefault::binary_cross_entropy_with_logits(
    const at::Tensor &self, const at::Tensor &target, const at::Tensor &weight,
    const at::Tensor &pos_weight, int64_t reduction) {
  std::cout << "aten::binary_cross_entropy_with_logits" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target, weight, pos_weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::binary_cross_entropy_with_logits(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::binary_cross_entropy_with_logits_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, const at::Tensor &weight,
    const at::Tensor &pos_weight, int64_t reduction) {
  std::cout << "aten::binary_cross_entropy_with_logits_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target, weight,
                                              pos_weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::binary_cross_entropy_with_logits_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor ATenMLIRTypeDefault::bincount(const at::Tensor &self,
                                         const at::Tensor &weights,
                                         int64_t minlength) {
  std::cout << "aten::bincount" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weights};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bincount(mlirtens[0], mlirtens[1], minlength);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::bitwise_not(const at::Tensor &self) {
  std::cout << "aten::bitwise_not" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bitwise_not(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::bitwise_not_(at::Tensor &self) {
  std::cout << "aten::bitwise_not_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].bitwise_not_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::bitwise_not_out(at::Tensor &out,
                                                 const at::Tensor &self) {
  std::cout << "aten::bitwise_not_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bitwise_not_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::logical_not(const at::Tensor &self) {
  std::cout << "aten::logical_not" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logical_not(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::logical_not_(at::Tensor &self) {
  std::cout << "aten::logical_not_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].logical_not_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::logical_not_out(at::Tensor &out,
                                                 const at::Tensor &self) {
  std::cout << "aten::logical_not_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logical_not_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::logical_xor(const at::Tensor &self,
                                            const at::Tensor &other) {
  std::cout << "aten::logical_xor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logical_xor(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::logical_xor_(at::Tensor &self,
                                              const at::Tensor &other) {
  std::cout << "aten::logical_xor_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].logical_xor_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::logical_xor_out(at::Tensor &out,
                                                 const at::Tensor &self,
                                                 const at::Tensor &other) {
  std::cout << "aten::logical_xor_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logical_xor_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::blackman_window(int64_t window_length,
                                     const at::TensorOptions &options) {
  std::cout << "aten::blackman_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::blackman_window(window_length, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor
ATenMLIRTypeDefault::blackman_window(int64_t window_length, bool periodic,
                                     const at::TensorOptions &options) {
  std::cout << "aten::blackman_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::blackman_window(window_length, periodic, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::bmm(const at::Tensor &self,
                                    const at::Tensor &mat2) {
  std::cout << "aten::bmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bmm(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::bmm_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &mat2) {
  std::cout << "aten::bmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::bmm_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::vector<at::Tensor>
ATenMLIRTypeDefault::broadcast_tensors(at::TensorList tensors) {
  std::cout << "aten::broadcast_tensors" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::broadcast_tensors(tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::cat(at::TensorList tensors, int64_t dim) {
  std::cout << "aten::cat" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cat(tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(tensors));
}

at::Tensor &ATenMLIRTypeDefault::cat_out(at::Tensor &out,
                                         at::TensorList tensors, int64_t dim) {
  std::cout << "aten::cat_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cat_out(mlirtens[0], tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::ceil(const at::Tensor &self) {
  std::cout << "aten::ceil" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ceil(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::ceil_(at::Tensor &self) {
  std::cout << "aten::ceil_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ceil_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::ceil_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::ceil_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ceil_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::chain_matmul(at::TensorList matrices) {
  std::cout << "aten::chain_matmul" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::chain_matmul(matrices);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(matrices));
}

std::vector<at::Tensor> ATenMLIRTypeDefault::chunk(const at::Tensor &self,
                                                   int64_t chunks,
                                                   int64_t dim) {
  std::cout << "aten::chunk" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::chunk(mlirtens[0], chunks, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::clamp(const at::Tensor &self,
                                      c10::optional<at::Scalar> min,
                                      c10::optional<at::Scalar> max) {
  std::cout << "aten::clamp" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp(mlirtens[0], min, max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::clamp_(at::Tensor &self,
                                        c10::optional<at::Scalar> min,
                                        c10::optional<at::Scalar> max) {
  std::cout << "aten::clamp_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_(mlirtens[0], min, max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::clamp_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           c10::optional<at::Scalar> min,
                                           c10::optional<at::Scalar> max) {
  std::cout << "aten::clamp_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_out(mlirtens[0], mlirtens[1], min, max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::clamp_max(const at::Tensor &self,
                                          at::Scalar max) {
  std::cout << "aten::clamp_max" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_max(mlirtens[0], max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::clamp_max_(at::Tensor &self, at::Scalar max) {
  std::cout << "aten::clamp_max_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_max_(mlirtens[0], max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::clamp_max_out(at::Tensor &out,
                                               const at::Tensor &self,
                                               at::Scalar max) {
  std::cout << "aten::clamp_max_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_max_out(mlirtens[0], mlirtens[1], max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::clamp_min(const at::Tensor &self,
                                          at::Scalar min) {
  std::cout << "aten::clamp_min" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_min(mlirtens[0], min);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::clamp_min_(at::Tensor &self, at::Scalar min) {
  std::cout << "aten::clamp_min_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_min_(mlirtens[0], min);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::clamp_min_out(at::Tensor &out,
                                               const at::Tensor &self,
                                               at::Scalar min) {
  std::cout << "aten::clamp_min_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clamp_min_out(mlirtens[0], mlirtens[1], min);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::constant_pad_nd(const at::Tensor &self,
                                                at::IntArrayRef pad,
                                                at::Scalar value) {
  std::cout << "aten::constant_pad_nd" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::constant_pad_nd(mlirtens[0], pad, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::contiguous(const at::Tensor &self,
                                           at::MemoryFormat memory_format) {
  std::cout << "aten::contiguous" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].contiguous(memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::convolution(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    bool transposed, at::IntArrayRef output_padding, int64_t groups) {
  std::cout << "aten::convolution" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::convolution(mlirtens[0], mlirtens[1], mlirtens[2], stride, padding,
                      dilation, transposed, output_padding, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::convolution_overrideable(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    bool transposed, at::IntArrayRef output_padding, int64_t groups) {
  std::cout << "aten::convolution_overrideable" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::convolution_overrideable(
      mlirtens[0], mlirtens[1], mlirtens[2], stride, padding, dilation,
      transposed, output_padding, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::convolution_backward_overrideable(
    const at::Tensor &grad_output, const at::Tensor &input,
    const at::Tensor &weight, at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding,
    int64_t groups, std::array<bool, 3> output_mask) {
  std::cout << "aten::convolution_backward_overrideable" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, input, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::convolution_backward_overrideable(
      mlirtens[0], mlirtens[1], mlirtens[2], stride, padding, dilation,
      transposed, output_padding, groups, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor ATenMLIRTypeDefault::_convolution(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    bool transposed, at::IntArrayRef output_padding, int64_t groups,
    bool benchmark, bool deterministic, bool cudnn_enabled) {
  std::cout << "aten::_convolution" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_convolution(mlirtens[0], mlirtens[1], mlirtens[2], stride, padding,
                       dilation, transposed, output_padding, groups, benchmark,
                       deterministic, cudnn_enabled);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::_convolution_nogroup(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    bool transposed, at::IntArrayRef output_padding) {
  std::cout << "aten::_convolution_nogroup" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_convolution_nogroup(mlirtens[0], mlirtens[1], mlirtens[2], stride,
                               padding, dilation, transposed, output_padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_convolution_double_backward(
    const at::Tensor &ggI, const at::Tensor &ggW, const at::Tensor &ggb,
    const at::Tensor &gO, const at::Tensor &weight, const at::Tensor &self,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    bool transposed, at::IntArrayRef output_padding, int64_t groups,
    bool benchmark, bool deterministic, bool cudnn_enabled,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::_convolution_double_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {ggI, ggW, ggb, gO, weight, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_convolution_double_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], stride, padding, dilation, transposed, output_padding,
      groups, benchmark, deterministic, cudnn_enabled, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(ggI)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(ggI)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(ggI)));
}

at::Tensor
ATenMLIRTypeDefault::conv1d(const at::Tensor &input, const at::Tensor &weight,
                            const at::Tensor &bias, at::IntArrayRef stride,
                            at::IntArrayRef padding, at::IntArrayRef dilation,
                            int64_t groups) {
  std::cout << "aten::conv1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::conv1d(mlirtens[0], mlirtens[1], mlirtens[2], stride,
                               padding, dilation, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor
ATenMLIRTypeDefault::conv2d(const at::Tensor &input, const at::Tensor &weight,
                            const at::Tensor &bias, at::IntArrayRef stride,
                            at::IntArrayRef padding, at::IntArrayRef dilation,
                            int64_t groups) {
  std::cout << "aten::conv2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::conv2d(mlirtens[0], mlirtens[1], mlirtens[2], stride,
                               padding, dilation, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor
ATenMLIRTypeDefault::conv3d(const at::Tensor &input, const at::Tensor &weight,
                            const at::Tensor &bias, at::IntArrayRef stride,
                            at::IntArrayRef padding, at::IntArrayRef dilation,
                            int64_t groups) {
  std::cout << "aten::conv3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::conv3d(mlirtens[0], mlirtens[1], mlirtens[2], stride,
                               padding, dilation, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::conv_tbc(const at::Tensor &self,
                                         const at::Tensor &weight,
                                         const at::Tensor &bias, int64_t pad) {
  std::cout << "aten::conv_tbc" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::conv_tbc(mlirtens[0], mlirtens[1], mlirtens[2], pad);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::conv_tbc_backward(const at::Tensor &self,
                                       const at::Tensor &input,
                                       const at::Tensor &weight,
                                       const at::Tensor &bias, int64_t pad) {
  std::cout << "aten::conv_tbc_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::conv_tbc_backward(mlirtens[0], mlirtens[1], mlirtens[2],
                                          mlirtens[3], pad);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::conv_transpose1d(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
  std::cout << "aten::conv_transpose1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::conv_transpose1d(mlirtens[0], mlirtens[1], mlirtens[2], stride,
                           padding, output_padding, groups, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::conv_transpose2d(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
  std::cout << "aten::conv_transpose2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::conv_transpose2d(mlirtens[0], mlirtens[1], mlirtens[2], stride,
                           padding, output_padding, groups, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::conv_transpose3d(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
  std::cout << "aten::conv_transpose3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::conv_transpose3d(mlirtens[0], mlirtens[1], mlirtens[2], stride,
                           padding, output_padding, groups, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor &ATenMLIRTypeDefault::copy_(at::Tensor &self, const at::Tensor &src,
                                       bool non_blocking) {
  std::cout << "aten::copy_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, src};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].copy_(mlirtens[1], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::_copy_from(const at::Tensor &self,
                                           const at::Tensor &dst,
                                           bool non_blocking) {
  std::cout << "aten::_copy_from" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, dst};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_copy_from(mlirtens[0], mlirtens[1], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::cos(const at::Tensor &self) {
  std::cout << "aten::cos" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cos(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::cos_(at::Tensor &self) {
  std::cout << "aten::cos_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cos_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::cos_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::cos_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cos_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::cosh(const at::Tensor &self) {
  std::cout << "aten::cosh" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cosh(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::cosh_(at::Tensor &self) {
  std::cout << "aten::cosh_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cosh_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::cosh_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::cosh_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cosh_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::cosine_embedding_loss(const at::Tensor &input1,
                                                      const at::Tensor &input2,
                                                      const at::Tensor &target,
                                                      double margin,
                                                      int64_t reduction) {
  std::cout << "aten::cosine_embedding_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input1, input2, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cosine_embedding_loss(mlirtens[0], mlirtens[1],
                                              mlirtens[2], margin, reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input1));
}

at::Tensor ATenMLIRTypeDefault::cumsum(const at::Tensor &self, int64_t dim,
                                       c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::cumsum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cumsum(mlirtens[0], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &
ATenMLIRTypeDefault::cumsum_out(at::Tensor &out, const at::Tensor &self,
                                int64_t dim,
                                c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::cumsum_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cumsum_out(mlirtens[0], mlirtens[1], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::cumprod(const at::Tensor &self, int64_t dim,
                                        c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::cumprod" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cumprod(mlirtens[0], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &
ATenMLIRTypeDefault::cumprod_out(at::Tensor &out, const at::Tensor &self,
                                 int64_t dim,
                                 c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::cumprod_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cumprod_out(mlirtens[0], mlirtens[1], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::ctc_loss(const at::Tensor &log_probs,
                                         const at::Tensor &targets,
                                         at::IntArrayRef input_lengths,
                                         at::IntArrayRef target_lengths,
                                         int64_t blank, int64_t reduction,
                                         bool zero_infinity) {
  std::cout << "aten::ctc_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {log_probs, targets};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::ctc_loss(mlirtens[0], mlirtens[1], input_lengths, target_lengths,
                   blank, reduction, zero_infinity);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(log_probs));
}

at::Tensor ATenMLIRTypeDefault::ctc_loss(const at::Tensor &log_probs,
                                         const at::Tensor &targets,
                                         const at::Tensor &input_lengths,
                                         const at::Tensor &target_lengths,
                                         int64_t blank, int64_t reduction,
                                         bool zero_infinity) {
  std::cout << "aten::ctc_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {log_probs, targets, input_lengths,
                                              target_lengths};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ctc_loss(mlirtens[0], mlirtens[1], mlirtens[2],
                                 mlirtens[3], blank, reduction, zero_infinity);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(log_probs));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::_ctc_loss(
    const at::Tensor &log_probs, const at::Tensor &targets,
    at::IntArrayRef input_lengths, at::IntArrayRef target_lengths,
    int64_t blank, bool zero_infinity) {
  std::cout << "aten::_ctc_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {log_probs, targets};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_ctc_loss(mlirtens[0], mlirtens[1], input_lengths,
                                  target_lengths, blank, zero_infinity);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(log_probs)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(log_probs)));
}

at::Tensor ATenMLIRTypeDefault::_ctc_loss_backward(
    const at::Tensor &grad, const at::Tensor &log_probs,
    const at::Tensor &targets, at::IntArrayRef input_lengths,
    at::IntArrayRef target_lengths, const at::Tensor &neg_log_likelihood,
    const at::Tensor &log_alpha, int64_t blank, bool zero_infinity) {
  std::cout << "aten::_ctc_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, log_probs, targets,
                                              neg_log_likelihood, log_alpha};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_ctc_loss_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], input_lengths, target_lengths,
      mlirtens[3], mlirtens[4], blank, zero_infinity);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::det(const at::Tensor &self) {
  std::cout << "aten::det" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::det(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::diag_embed(const at::Tensor &self,
                                           int64_t offset, int64_t dim1,
                                           int64_t dim2) {
  std::cout << "aten::diag_embed" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::diag_embed(mlirtens[0], offset, dim1, dim2);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::diagflat(const at::Tensor &self,
                                         int64_t offset) {
  std::cout << "aten::diagflat" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::diagflat(mlirtens[0], offset);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::diagonal(const at::Tensor &self, int64_t offset,
                                         int64_t dim1, int64_t dim2) {
  std::cout << "aten::diagonal" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::diagonal(mlirtens[0], offset, dim1, dim2);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::fill_diagonal_(at::Tensor &self,
                                                at::Scalar fill_value,
                                                bool wrap) {
  std::cout << "aten::fill_diagonal_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].fill_diagonal_(fill_value, wrap);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::div(const at::Tensor &self,
                                    const at::Tensor &other) {
  std::cout << "aten::div" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::div(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::div_(at::Tensor &self,
                                      const at::Tensor &other) {
  std::cout << "aten::div_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].div_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::div_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &other) {
  std::cout << "aten::div_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::div_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::div(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::div" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::div(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::div_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::div_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].div_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::dot(const at::Tensor &self,
                                    const at::Tensor &tensor) {
  std::cout << "aten::dot" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, tensor};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::dot(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::dot_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &tensor) {
  std::cout << "aten::dot_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, tensor};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::dot_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::einsum(std::string equation,
                                       at::TensorList tensors) {
  std::cout << "aten::einsum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::einsum(equation, tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(tensors));
}

at::Tensor ATenMLIRTypeDefault::embedding(const at::Tensor &weight,
                                          const at::Tensor &indices,
                                          int64_t padding_idx,
                                          bool scale_grad_by_freq,
                                          bool sparse) {
  std::cout << "aten::embedding" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {weight, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::embedding(mlirtens[0], mlirtens[1], padding_idx,
                                  scale_grad_by_freq, sparse);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(weight));
}

at::Tensor ATenMLIRTypeDefault::embedding_backward(
    const at::Tensor &grad, const at::Tensor &indices, int64_t num_weights,
    int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
  std::cout << "aten::embedding_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::embedding_backward(mlirtens[0], mlirtens[1], num_weights, padding_idx,
                             scale_grad_by_freq, sparse);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::embedding_dense_backward(
    const at::Tensor &grad_output, const at::Tensor &indices,
    int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
  std::cout << "aten::embedding_dense_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::embedding_dense_backward(
      mlirtens[0], mlirtens[1], num_weights, padding_idx, scale_grad_by_freq);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::embedding_renorm_(at::Tensor &self,
                                                   const at::Tensor &indices,
                                                   double max_norm,
                                                   double norm_type) {
  std::cout << "aten::embedding_renorm_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::embedding_renorm_(mlirtens[0], mlirtens[1], max_norm, norm_type);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::embedding_sparse_backward(
    const at::Tensor &grad, const at::Tensor &indices, int64_t num_weights,
    int64_t padding_idx, bool scale_grad_by_freq) {
  std::cout << "aten::embedding_sparse_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::embedding_sparse_backward(
      mlirtens[0], mlirtens[1], num_weights, padding_idx, scale_grad_by_freq);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::embedding_bag(const at::Tensor &weight,
                                   const at::Tensor &indices,
                                   const at::Tensor &offsets,
                                   bool scale_grad_by_freq, int64_t mode,
                                   bool sparse,
                                   const at::Tensor &per_sample_weights) {
  std::cout << "aten::embedding_bag" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {weight, indices, offsets,
                                              per_sample_weights};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::embedding_bag(mlirtens[0], mlirtens[1], mlirtens[2],
                        scale_grad_by_freq, mode, sparse, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(weight)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(weight)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(weight)),
      bridge::CreateMLIRTensor(std::get<3>(x_result),
                               bridge::GetMLIRDevice(weight)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_embedding_bag(const at::Tensor &weight,
                                    const at::Tensor &indices,
                                    const at::Tensor &offsets,
                                    bool scale_grad_by_freq, int64_t mode,
                                    bool sparse,
                                    const at::Tensor &per_sample_weights) {
  std::cout << "aten::_embedding_bag" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {weight, indices, offsets,
                                              per_sample_weights};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_embedding_bag(mlirtens[0], mlirtens[1], mlirtens[2],
                         scale_grad_by_freq, mode, sparse, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(weight)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(weight)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(weight)),
      bridge::CreateMLIRTensor(std::get<3>(x_result),
                               bridge::GetMLIRDevice(weight)));
}

at::Tensor ATenMLIRTypeDefault::_embedding_bag_backward(
    const at::Tensor &grad, const at::Tensor &indices,
    const at::Tensor &offsets, const at::Tensor &offset2bag,
    const at::Tensor &bag_size, const at::Tensor &maximum_indices,
    int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse,
    const at::Tensor &per_sample_weights) {
  std::cout << "aten::_embedding_bag_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad,     indices,         offsets,           offset2bag,
      bag_size, maximum_indices, per_sample_weights};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_embedding_bag_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], num_weights, scale_grad_by_freq, mode, sparse, mlirtens[6]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::_embedding_bag_sparse_backward(
    const at::Tensor &grad, const at::Tensor &indices,
    const at::Tensor &offsets, const at::Tensor &offset2bag,
    const at::Tensor &bag_size, int64_t num_weights, bool scale_grad_by_freq,
    int64_t mode, const at::Tensor &per_sample_weights) {
  std::cout << "aten::_embedding_bag_sparse_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad, indices, offsets, offset2bag, bag_size, per_sample_weights};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_embedding_bag_sparse_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      num_weights, scale_grad_by_freq, mode, mlirtens[5]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::_embedding_bag_dense_backward(
    const at::Tensor &grad, const at::Tensor &indices,
    const at::Tensor &offsets, const at::Tensor &offset2bag,
    const at::Tensor &bag_size, const at::Tensor &maximum_indices,
    int64_t num_weights, bool scale_grad_by_freq, int64_t mode,
    const at::Tensor &per_sample_weights) {
  std::cout << "aten::_embedding_bag_dense_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad,     indices,         offsets,           offset2bag,
      bag_size, maximum_indices, per_sample_weights};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_embedding_bag_dense_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], num_weights, scale_grad_by_freq, mode, mlirtens[6]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::_embedding_bag_per_sample_weights_backward(
    const at::Tensor &grad, const at::Tensor &weight, const at::Tensor &indices,
    const at::Tensor &offsets, const at::Tensor &offset2bag, int64_t mode) {
  std::cout << "aten::_embedding_bag_per_sample_weights_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, weight, indices, offsets,
                                              offset2bag};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_embedding_bag_per_sample_weights_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4], mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor
ATenMLIRTypeDefault::empty(at::IntArrayRef size,
                           const at::TensorOptions &options,
                           c10::optional<at::MemoryFormat> memory_format) {
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::empty(size, o_options, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::new_empty(const at::Tensor &self,
                                          at::IntArrayRef size,
                                          const at::TensorOptions &options) {
  std::cout << "aten::new_empty" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].new_empty(size, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::new_full(const at::Tensor &self,
                                         at::IntArrayRef size,
                                         at::Scalar fill_value,
                                         const at::TensorOptions &options) {
  std::cout << "aten::new_full" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].new_full(size, fill_value, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_empty_affine_quantized(
    at::IntArrayRef size, const at::TensorOptions &options, double scale,
    int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
  std::cout << "aten::_empty_affine_quantized" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_empty_affine_quantized(size, options, scale,
                                                zero_point, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::_empty_per_channel_affine_quantized_like(
    const at::Tensor &self, const at::Tensor &zero_points, at::IntArrayRef size,
    at::IntArrayRef axis, const at::TensorOptions &options,
    c10::optional<at::MemoryFormat> memory_format) {
  std::cout << "aten::_empty_per_channel_affine_quantized_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, zero_points};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_empty_per_channel_affine_quantized_like(
      mlirtens[0], mlirtens[1], size, axis, options, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::resize_(at::Tensor &self,
                                         at::IntArrayRef size) {
  std::cout << "aten::resize_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].resize_(size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &
ATenMLIRTypeDefault::empty_out(at::Tensor &out, at::IntArrayRef size,
                               c10::optional<at::MemoryFormat> memory_format) {
  std::cout << "aten::empty_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::empty_out(mlirtens[0], size, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::empty_like(const at::Tensor &self) {
  std::cout << "aten::empty_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::empty_like(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor
ATenMLIRTypeDefault::empty_like(const at::Tensor &self,
                                const at::TensorOptions &options,
                                c10::optional<at::MemoryFormat> memory_format) {
  std::cout << "aten::empty_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::empty_like(mlirtens[0], options, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor
ATenMLIRTypeDefault::empty_strided(at::IntArrayRef size, at::IntArrayRef stride,
                                   const at::TensorOptions &options) {
  std::cout << "aten::empty_strided" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::empty_strided(size, stride, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::erf(const at::Tensor &self) {
  std::cout << "aten::erf" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erf(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::erf_(at::Tensor &self) {
  std::cout << "aten::erf_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erf_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::erf_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::erf_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erf_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::erfc(const at::Tensor &self) {
  std::cout << "aten::erfc" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erfc(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::erfc_(at::Tensor &self) {
  std::cout << "aten::erfc_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erfc_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::erfc_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::erfc_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erfc_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::exp(const at::Tensor &self) {
  std::cout << "aten::exp" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::exp(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::exp_(at::Tensor &self) {
  std::cout << "aten::exp_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::exp_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::exp_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::exp_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::exp_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::expm1(const at::Tensor &self) {
  std::cout << "aten::expm1" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::expm1(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::expm1_(at::Tensor &self) {
  std::cout << "aten::expm1_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::expm1_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::expm1_out(at::Tensor &out,
                                           const at::Tensor &self) {
  std::cout << "aten::expm1_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::expm1_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::expand(const at::Tensor &self,
                                       at::IntArrayRef size, bool implicit) {
  std::cout << "aten::expand" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].expand(size, implicit);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::expand_as(const at::Tensor &self,
                                          const at::Tensor &other) {
  std::cout << "aten::expand_as" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].expand_as(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::eye(int64_t n,
                                    const at::TensorOptions &options) {
  std::cout << "aten::eye" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eye(n, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::eye(int64_t n, int64_t m,
                                    const at::TensorOptions &options) {
  std::cout << "aten::eye" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eye(n, m, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::eye_out(at::Tensor &out, int64_t n) {
  std::cout << "aten::eye_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eye_out(mlirtens[0], n);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::eye_out(at::Tensor &out, int64_t n,
                                         int64_t m) {
  std::cout << "aten::eye_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eye_out(mlirtens[0], n, m);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::flatten(const at::Tensor &self,
                                        int64_t start_dim, int64_t end_dim) {
  std::cout << "aten::flatten" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::flatten(mlirtens[0], start_dim, end_dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::fill_(at::Tensor &self, at::Scalar value) {
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fill_(mlirtens[0], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::fill_(at::Tensor &self,
                                       const at::Tensor &value) {
  std::vector<at::Tensor> mlirtens_tensors = {self, value};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fill_(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::floor(const at::Tensor &self) {
  std::cout << "aten::floor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::floor(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::floor_(at::Tensor &self) {
  std::cout << "aten::floor_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::floor_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::floor_out(at::Tensor &out,
                                           const at::Tensor &self) {
  std::cout << "aten::floor_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::floor_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::frac(const at::Tensor &self) {
  std::cout << "aten::frac" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::frac(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::frac_(at::Tensor &self) {
  std::cout << "aten::frac_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::frac_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::frac_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::frac_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::frac_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::full(at::IntArrayRef size,
                                     at::Scalar fill_value,
                                     const at::TensorOptions &options) {
  std::cout << "aten::full" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::full(size, fill_value, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::full_out(at::Tensor &out, at::IntArrayRef size,
                                          at::Scalar fill_value) {
  std::cout << "aten::full_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::full_out(mlirtens[0], size, fill_value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::full_like(const at::Tensor &self,
                                          at::Scalar fill_value) {
  std::cout << "aten::full_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::full_like(mlirtens[0], fill_value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::full_like(const at::Tensor &self,
                                          at::Scalar fill_value,
                                          const at::TensorOptions &options) {
  std::cout << "aten::full_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::full_like(mlirtens[0], fill_value, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::from_file(std::string filename,
                                          c10::optional<bool> shared,
                                          c10::optional<int64_t> size,
                                          const at::TensorOptions &options) {
  std::cout << "aten::from_file" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::from_file(filename, shared, size, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::grid_sampler(const at::Tensor &input,
                                             const at::Tensor &grid,
                                             int64_t interpolation_mode,
                                             int64_t padding_mode,
                                             bool align_corners) {
  std::cout << "aten::grid_sampler" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, grid};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::grid_sampler(mlirtens[0], mlirtens[1], interpolation_mode,
                       padding_mode, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::grid_sampler_2d(const at::Tensor &input,
                                                const at::Tensor &grid,
                                                int64_t interpolation_mode,
                                                int64_t padding_mode,
                                                bool align_corners) {
  std::cout << "aten::grid_sampler_2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, grid};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::grid_sampler_2d(mlirtens[0], mlirtens[1], interpolation_mode,
                          padding_mode, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::grid_sampler_2d_backward(const at::Tensor &grad_output,
                                              const at::Tensor &input,
                                              const at::Tensor &grid,
                                              int64_t interpolation_mode,
                                              int64_t padding_mode,
                                              bool align_corners) {
  std::cout << "aten::grid_sampler_2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, input, grid};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::grid_sampler_2d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], interpolation_mode, padding_mode,
      align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor ATenMLIRTypeDefault::grid_sampler_3d(const at::Tensor &input,
                                                const at::Tensor &grid,
                                                int64_t interpolation_mode,
                                                int64_t padding_mode,
                                                bool align_corners) {
  std::cout << "aten::grid_sampler_3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, grid};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::grid_sampler_3d(mlirtens[0], mlirtens[1], interpolation_mode,
                          padding_mode, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::grid_sampler_3d_backward(const at::Tensor &grad_output,
                                              const at::Tensor &input,
                                              const at::Tensor &grid,
                                              int64_t interpolation_mode,
                                              int64_t padding_mode,
                                              bool align_corners) {
  std::cout << "aten::grid_sampler_3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, input, grid};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::grid_sampler_3d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], interpolation_mode, padding_mode,
      align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor ATenMLIRTypeDefault::hann_window(int64_t window_length,
                                            const at::TensorOptions &options) {
  std::cout << "aten::hann_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hann_window(window_length, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::hann_window(int64_t window_length,
                                            bool periodic,
                                            const at::TensorOptions &options) {
  std::cout << "aten::hann_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hann_window(window_length, periodic, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor
ATenMLIRTypeDefault::hamming_window(int64_t window_length,
                                    const at::TensorOptions &options) {
  std::cout << "aten::hamming_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hamming_window(window_length, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor
ATenMLIRTypeDefault::hamming_window(int64_t window_length, bool periodic,
                                    const at::TensorOptions &options) {
  std::cout << "aten::hamming_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hamming_window(window_length, periodic, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor
ATenMLIRTypeDefault::hamming_window(int64_t window_length, bool periodic,
                                    double alpha,
                                    const at::TensorOptions &options) {
  std::cout << "aten::hamming_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hamming_window(window_length, periodic, alpha, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor
ATenMLIRTypeDefault::hamming_window(int64_t window_length, bool periodic,
                                    double alpha, double beta,
                                    const at::TensorOptions &options) {
  std::cout << "aten::hamming_window" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::hamming_window(window_length, periodic, alpha, beta, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::hinge_embedding_loss(const at::Tensor &self,
                                                     const at::Tensor &target,
                                                     double margin,
                                                     int64_t reduction) {
  std::cout << "aten::hinge_embedding_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::hinge_embedding_loss(mlirtens[0], mlirtens[1], margin, reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::ger(const at::Tensor &self,
                                    const at::Tensor &vec2) {
  std::cout << "aten::ger" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ger(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::ger_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &vec2) {
  std::cout << "aten::ger_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ger_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::group_norm(const at::Tensor &input,
                                           int64_t num_groups,
                                           const at::Tensor &weight,
                                           const at::Tensor &bias, double eps,
                                           bool cudnn_enabled) {
  std::cout << "aten::group_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::group_norm(mlirtens[0], num_groups, mlirtens[1],
                                   mlirtens[2], eps, cudnn_enabled);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::fft(const at::Tensor &self, int64_t signal_ndim,
                                    bool normalized) {
  std::cout << "aten::fft" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fft(mlirtens[0], signal_ndim, normalized);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::ifft(const at::Tensor &self,
                                     int64_t signal_ndim, bool normalized) {
  std::cout << "aten::ifft" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ifft(mlirtens[0], signal_ndim, normalized);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::rfft(const at::Tensor &self,
                                     int64_t signal_ndim, bool normalized,
                                     bool onesided) {
  std::cout << "aten::rfft" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rfft(mlirtens[0], signal_ndim, normalized, onesided);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::irfft(const at::Tensor &self,
                                      int64_t signal_ndim, bool normalized,
                                      bool onesided,
                                      at::IntArrayRef signal_sizes) {
  std::cout << "aten::irfft" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::irfft(mlirtens[0], signal_ndim, normalized, onesided, signal_sizes);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_fft_with_size(
    const at::Tensor &self, int64_t signal_ndim, bool complex_input,
    bool complex_output, bool inverse, at::IntArrayRef checked_signal_sizes,
    bool normalized, bool onesided, at::IntArrayRef output_sizes) {
  std::cout << "aten::_fft_with_size" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_fft_with_size(
      mlirtens[0], signal_ndim, complex_input, complex_output, inverse,
      checked_signal_sizes, normalized, onesided, output_sizes);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

int64_t ATenMLIRTypeDefault::_cufft_get_plan_cache_size(int64_t device_index) {
  std::cout << "aten::_cufft_get_plan_cache_size" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cufft_get_plan_cache_size(device_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t
ATenMLIRTypeDefault::_cufft_get_plan_cache_max_size(int64_t device_index) {
  std::cout << "aten::_cufft_get_plan_cache_max_size" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cufft_get_plan_cache_max_size(device_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

void ATenMLIRTypeDefault::_cufft_set_plan_cache_max_size(int64_t device_index,
                                                         int64_t max_size) {
  std::cout << "aten::_cufft_set_plan_cache_max_size" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  at::_cufft_set_plan_cache_max_size(device_index, max_size);
}

void ATenMLIRTypeDefault::_cufft_clear_plan_cache(int64_t device_index) {
  std::cout << "aten::_cufft_clear_plan_cache" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  at::_cufft_clear_plan_cache(device_index);
}

at::Tensor ATenMLIRTypeDefault::index(const at::Tensor &self,
                                      at::TensorList indices) {
  std::cout << "aten::index" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::index(mlirtens[0], indices);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::index_copy_(at::Tensor &self, int64_t dim,
                                             const at::Tensor &index,
                                             const at::Tensor &source) {
  std::cout << "aten::index_copy_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].index_copy_(dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::index_copy(const at::Tensor &self, int64_t dim,
                                           const at::Tensor &index,
                                           const at::Tensor &source) {
  std::cout << "aten::index_copy" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::index_copy(mlirtens[0], dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::index_put_(at::Tensor &self,
                                            at::TensorList indices,
                                            const at::Tensor &values,
                                            bool accumulate) {
  std::cout << "aten::index_put_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, values};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::index_put_(mlirtens[0], indices, mlirtens[1], accumulate);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::index_put(const at::Tensor &self,
                                          at::TensorList indices,
                                          const at::Tensor &values,
                                          bool accumulate) {
  std::cout << "aten::index_put" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, values};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::index_put(mlirtens[0], indices, mlirtens[1], accumulate);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::_index_put_impl_(at::Tensor &self,
                                                  at::TensorList indices,
                                                  const at::Tensor &values,
                                                  bool accumulate,
                                                  bool unsafe) {
  std::cout << "aten::_index_put_impl_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, values};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_index_put_impl_(mlirtens[0], indices, mlirtens[1],
                                         accumulate, unsafe);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::instance_norm(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    const at::Tensor &running_mean, const at::Tensor &running_var,
    bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
  std::cout << "aten::instance_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias, running_mean,
                                              running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::instance_norm(mlirtens[0], mlirtens[1], mlirtens[2],
                                      mlirtens[3], mlirtens[4], use_input_stats,
                                      momentum, eps, cudnn_enabled);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::inverse(const at::Tensor &self) {
  std::cout << "aten::inverse" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::inverse(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::inverse_out(at::Tensor &out,
                                             const at::Tensor &self) {
  std::cout << "aten::inverse_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::inverse_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::_inverse_helper(const at::Tensor &self) {
  std::cout << "aten::_inverse_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_inverse_helper(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::isclose(const at::Tensor &self,
                                        const at::Tensor &other, double rtol,
                                        double atol, bool equal_nan) {
  std::cout << "aten::isclose" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::isclose(mlirtens[0], mlirtens[1], rtol, atol, equal_nan);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::isnan(const at::Tensor &self) {
  std::cout << "aten::isnan" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::isnan(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

bool ATenMLIRTypeDefault::is_distributed(const at::Tensor &self) {
  std::cout << "aten::is_distributed" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::is_distributed(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

bool ATenMLIRTypeDefault::is_floating_point(const at::Tensor &self) {
  std::cout << "aten::is_floating_point" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::is_floating_point(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

bool ATenMLIRTypeDefault::is_complex(const at::Tensor &self) {
  std::cout << "aten::is_complex" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::is_complex(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

bool ATenMLIRTypeDefault::is_nonzero(const at::Tensor &self) {
  std::cout << "aten::is_nonzero" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::is_nonzero(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

bool ATenMLIRTypeDefault::is_same_size(const at::Tensor &self,
                                       const at::Tensor &other) {
  std::cout << "aten::is_same_size" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::is_same_size(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

bool ATenMLIRTypeDefault::is_signed(const at::Tensor &self) {
  std::cout << "aten::is_signed" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::is_signed(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::kl_div(const at::Tensor &self,
                                       const at::Tensor &target,
                                       int64_t reduction) {
  std::cout << "aten::kl_div" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::kl_div(mlirtens[0], mlirtens[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::kl_div_backward(const at::Tensor &grad_output,
                                                const at::Tensor &self,
                                                const at::Tensor &target,
                                                int64_t reduction) {
  std::cout << "aten::kl_div_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::kl_div_backward(mlirtens[0], mlirtens[1], mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::kthvalue(const at::Tensor &self, int64_t k, int64_t dim,
                              bool keepdim) {
  std::cout << "aten::kthvalue" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::kthvalue(mlirtens[0], k, dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::kthvalue_out(at::Tensor &values, at::Tensor &indices,
                                  const at::Tensor &self, int64_t k,
                                  int64_t dim, bool keepdim) {
  std::cout << "aten::kthvalue_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {values, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::kthvalue_out(mlirtens[0], mlirtens[1], mlirtens[2], k, dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(values, indices);
}

at::Tensor ATenMLIRTypeDefault::layer_norm(const at::Tensor &input,
                                           at::IntArrayRef normalized_shape,
                                           const at::Tensor &weight,
                                           const at::Tensor &bias, double eps,
                                           bool cudnn_enable) {
  std::cout << "aten::layer_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::layer_norm(mlirtens[0], normalized_shape, mlirtens[1],
                                   mlirtens[2], eps, cudnn_enable);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::native_layer_norm(const at::Tensor &input,
                                       const at::Tensor &weight,
                                       const at::Tensor &bias, int64_t M,
                                       int64_t N, double eps) {
  std::cout << "aten::native_layer_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::native_layer_norm(mlirtens[0], mlirtens[1], mlirtens[2], M, N, eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::native_layer_norm_backward(
    const at::Tensor &grad_out, const at::Tensor &input, const at::Tensor &mean,
    const at::Tensor &rstd, const at::Tensor &weight, int64_t M, int64_t N,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::native_layer_norm_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_out, input, mean, rstd,
                                              weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::native_layer_norm_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4], M, N,
      output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_out)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_out)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_out)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::native_layer_norm_double_backward(
    const at::Tensor &ggI, const at::Tensor &ggW, const at::Tensor &ggb,
    const at::Tensor &gO, const at::Tensor &input, const at::Tensor &mean,
    const at::Tensor &rstd, const at::Tensor &weight, int64_t M, int64_t N,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::native_layer_norm_double_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {ggI,   ggW,  ggb,  gO,
                                              input, mean, rstd, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::native_layer_norm_double_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6], mlirtens[7], M, N, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(ggI)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(ggI)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(ggI)));
}

at::Tensor ATenMLIRTypeDefault::linear(const at::Tensor &input,
                                       const at::Tensor &weight,
                                       const at::Tensor &bias) {
  std::cout << "aten::linear" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::linear(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::mkldnn_linear(const at::Tensor &input,
                                              const at::Tensor &weight,
                                              const at::Tensor &bias) {
  std::cout << "aten::mkldnn_linear" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_linear(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::fbgemm_linear_int8_weight_fp32_activation(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &packed,
    const at::Tensor &col_offsets, at::Scalar weight_scale,
    at::Scalar weight_zero_point, const at::Tensor &bias) {
  std::cout << "aten::fbgemm_linear_int8_weight_fp32_activation" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, packed,
                                              col_offsets, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fbgemm_linear_int8_weight_fp32_activation(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], weight_scale,
      weight_zero_point, mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::fbgemm_linear_int8_weight(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &packed,
    const at::Tensor &col_offsets, at::Scalar weight_scale,
    at::Scalar weight_zero_point, const at::Tensor &bias) {
  std::cout << "aten::fbgemm_linear_int8_weight" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, packed,
                                              col_offsets, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fbgemm_linear_int8_weight(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], weight_scale,
      weight_zero_point, mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor, double, int64_t>
ATenMLIRTypeDefault::fbgemm_linear_quantize_weight(const at::Tensor &input) {
  std::cout << "aten::fbgemm_linear_quantize_weight" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fbgemm_linear_quantize_weight(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, double, int64_t>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      std::get<2>(x_result), std::get<3>(x_result));
}

at::Tensor
ATenMLIRTypeDefault::fbgemm_pack_gemm_matrix_fp16(const at::Tensor &input) {
  std::cout << "aten::fbgemm_pack_gemm_matrix_fp16" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fbgemm_pack_gemm_matrix_fp16(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::fbgemm_linear_fp16_weight_fp32_activation(
    const at::Tensor &input, const at::Tensor &packed_weight,
    const at::Tensor &bias) {
  std::cout << "aten::fbgemm_linear_fp16_weight_fp32_activation" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, packed_weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fbgemm_linear_fp16_weight_fp32_activation(
      mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor
ATenMLIRTypeDefault::fbgemm_linear_fp16_weight(const at::Tensor &input,
                                               const at::Tensor &packed_weight,
                                               const at::Tensor &bias) {
  std::cout << "aten::fbgemm_linear_fp16_weight" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, packed_weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::fbgemm_linear_fp16_weight(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor
ATenMLIRTypeDefault::fbgemm_pack_quantized_matrix(const at::Tensor &input) {
  std::cout << "aten::fbgemm_pack_quantized_matrix" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fbgemm_pack_quantized_matrix(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor
ATenMLIRTypeDefault::fbgemm_pack_quantized_matrix(const at::Tensor &input,
                                                  int64_t K, int64_t N) {
  std::cout << "aten::fbgemm_pack_quantized_matrix" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fbgemm_pack_quantized_matrix(mlirtens[0], K, N);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::linspace(at::Scalar start, at::Scalar end,
                                         int64_t steps,
                                         const at::TensorOptions &options) {
  std::cout << "aten::linspace" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::linspace(start, end, steps, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::linspace_out(at::Tensor &out, at::Scalar start,
                                              at::Scalar end, int64_t steps) {
  std::cout << "aten::linspace_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::linspace_out(mlirtens[0], start, end, steps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::log(const at::Tensor &self) {
  std::cout << "aten::log" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::log_(at::Tensor &self) {
  std::cout << "aten::log_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::log_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::log_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::log10(const at::Tensor &self) {
  std::cout << "aten::log10" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log10(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::log10_(at::Tensor &self) {
  std::cout << "aten::log10_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log10_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::log10_out(at::Tensor &out,
                                           const at::Tensor &self) {
  std::cout << "aten::log10_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log10_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::log1p(const at::Tensor &self) {
  std::cout << "aten::log1p" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log1p(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::log1p_(at::Tensor &self) {
  std::cout << "aten::log1p_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log1p_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::log1p_out(at::Tensor &out,
                                           const at::Tensor &self) {
  std::cout << "aten::log1p_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log1p_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::log2(const at::Tensor &self) {
  std::cout << "aten::log2" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log2(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::log2_(at::Tensor &self) {
  std::cout << "aten::log2_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log2_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::log2_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::log2_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log2_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::logdet(const at::Tensor &self) {
  std::cout << "aten::logdet" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logdet(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::logspace(at::Scalar start, at::Scalar end,
                                         int64_t steps, double base,
                                         const at::TensorOptions &options) {
  std::cout << "aten::logspace" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logspace(start, end, steps, base, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::logspace_out(at::Tensor &out, at::Scalar start,
                                              at::Scalar end, int64_t steps,
                                              double base) {
  std::cout << "aten::logspace_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logspace_out(mlirtens[0], start, end, steps, base);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::log_softmax(const at::Tensor &self, int64_t dim,
                                 c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::log_softmax" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log_softmax(mlirtens[0], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_log_softmax(const at::Tensor &self,
                                             int64_t dim, bool half_to_float) {
  std::cout << "aten::_log_softmax" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_log_softmax(mlirtens[0], dim, half_to_float);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_log_softmax_backward_data(
    const at::Tensor &grad_output, const at::Tensor &output, int64_t dim,
    const at::Tensor &self) {
  std::cout << "aten::_log_softmax_backward_data" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_log_softmax_backward_data(mlirtens[0], mlirtens[1],
                                                   dim, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor ATenMLIRTypeDefault::logsumexp(const at::Tensor &self,
                                          at::IntArrayRef dim, bool keepdim) {
  std::cout << "aten::logsumexp" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logsumexp(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::logsumexp_out(at::Tensor &out,
                                               const at::Tensor &self,
                                               at::IntArrayRef dim,
                                               bool keepdim) {
  std::cout << "aten::logsumexp_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::logsumexp_out(mlirtens[0], mlirtens[1], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::margin_ranking_loss(const at::Tensor &input1,
                                                    const at::Tensor &input2,
                                                    const at::Tensor &target,
                                                    double margin,
                                                    int64_t reduction) {
  std::cout << "aten::margin_ranking_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input1, input2, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::margin_ranking_loss(mlirtens[0], mlirtens[1],
                                            mlirtens[2], margin, reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input1));
}

at::Tensor ATenMLIRTypeDefault::matmul(const at::Tensor &self,
                                       const at::Tensor &other) {
  std::cout << "aten::matmul" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::matmul(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::matmul_out(at::Tensor &out,
                                            const at::Tensor &self,
                                            const at::Tensor &other) {
  std::cout << "aten::matmul_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::matmul_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::matrix_rank(const at::Tensor &self, double tol,
                                            bool symmetric) {
  std::cout << "aten::matrix_rank" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::matrix_rank(mlirtens[0], tol, symmetric);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::matrix_rank(const at::Tensor &self,
                                            bool symmetric) {
  std::cout << "aten::matrix_rank" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::matrix_rank(mlirtens[0], symmetric);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::matrix_power(const at::Tensor &self,
                                             int64_t n) {
  std::cout << "aten::matrix_power" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::matrix_power(mlirtens[0], n);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::max(const at::Tensor &self, int64_t dim, bool keepdim) {
  std::cout << "aten::max" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::max_out(at::Tensor &max, at::Tensor &max_values,
                             const at::Tensor &self, int64_t dim,
                             bool keepdim) {
  std::cout << "aten::max_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {max, max_values, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::max_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(max, max_values);
}

at::Tensor ATenMLIRTypeDefault::max_values(const at::Tensor &self,
                                           at::IntArrayRef dim, bool keepdim) {
  std::cout << "aten::max_values" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_values(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::max_pool1d_with_indices(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool1d_with_indices" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool1d_with_indices(
      mlirtens[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::max_pool1d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool1d(mlirtens[0], kernel_size, stride, padding,
                                   dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::max_pool2d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool2d(mlirtens[0], kernel_size, stride, padding,
                                   dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::mkldnn_max_pool2d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::mkldnn_max_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_max_pool2d(mlirtens[0], kernel_size, stride,
                                          padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::quantized_max_pool2d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::quantized_max_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantized_max_pool2d(mlirtens[0], kernel_size, stride,
                                             padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::max_pool3d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool3d(mlirtens[0], kernel_size, stride, padding,
                                   dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::mean(const at::Tensor &self,
                                     c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::mean" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mean(mlirtens[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::mean(const at::Tensor &self,
                                     at::IntArrayRef dim, bool keepdim,
                                     c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::mean" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mean(mlirtens[0], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::mean_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          at::IntArrayRef dim, bool keepdim,
                                          c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::mean_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mean_out(mlirtens[0], mlirtens[1], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::median(const at::Tensor &self, int64_t dim, bool keepdim) {
  std::cout << "aten::median" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::median(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::median_out(at::Tensor &values, at::Tensor &indices,
                                const at::Tensor &self, int64_t dim,
                                bool keepdim) {
  std::cout << "aten::median_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {values, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::median_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(values, indices);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::min(const at::Tensor &self, int64_t dim, bool keepdim) {
  std::cout << "aten::min" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::min(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::min_out(at::Tensor &min, at::Tensor &min_indices,
                             const at::Tensor &self, int64_t dim,
                             bool keepdim) {
  std::cout << "aten::min_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {min, min_indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::min_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(min, min_indices);
}

at::Tensor ATenMLIRTypeDefault::min_values(const at::Tensor &self,
                                           at::IntArrayRef dim, bool keepdim) {
  std::cout << "aten::min_values" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::min_values(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::mkldnn_convolution(
    const at::Tensor &self, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation,
    int64_t groups) {
  std::cout << "aten::mkldnn_convolution" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_convolution(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, stride, dilation, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::mkldnn_convolution_backward_input(
    at::IntArrayRef self_size, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
  std::cout << "aten::mkldnn_convolution_backward_input" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_convolution_backward_input(
      self_size, mlirtens[0], mlirtens[1], padding, stride, dilation, groups,
      bias_defined);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::mkldnn_convolution_backward_weights(
    at::IntArrayRef weight_size, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
  std::cout << "aten::mkldnn_convolution_backward_weights" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_convolution_backward_weights(
      weight_size, mlirtens[0], mlirtens[1], padding, stride, dilation, groups,
      bias_defined);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::mkldnn_convolution_backward(
    const at::Tensor &self, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, std::array<bool, 3> output_mask) {
  std::cout << "aten::mkldnn_convolution_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_convolution_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, stride, dilation, groups,
      output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::miopen_batch_norm(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    const at::Tensor &running_mean, const at::Tensor &running_var,
    bool training, double exponential_average_factor, double epsilon) {
  std::cout << "aten::miopen_batch_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias, running_mean,
                                              running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_batch_norm(mlirtens[0], mlirtens[1], mlirtens[2],
                                          mlirtens[3], mlirtens[4], training,
                                          exponential_average_factor, epsilon);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::miopen_batch_norm_backward(
    const at::Tensor &input, const at::Tensor &grad_output,
    const at::Tensor &weight, const at::Tensor &running_mean,
    const at::Tensor &running_var, const at::Tensor &save_mean,
    const at::Tensor &save_var, double epsilon) {
  std::cout << "aten::miopen_batch_norm_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      input,       grad_output, weight,  running_mean,
      running_var, save_mean,   save_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_batch_norm_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6], epsilon);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor ATenMLIRTypeDefault::miopen_convolution(
    const at::Tensor &self, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation,
    int64_t groups, bool benchmark, bool deterministic) {
  std::cout << "aten::miopen_convolution" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, stride, dilation, groups,
      benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::miopen_convolution_backward_input(
    at::IntArrayRef self_size, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic) {
  std::cout << "aten::miopen_convolution_backward_input" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_backward_input(
      self_size, mlirtens[0], mlirtens[1], padding, stride, dilation, groups,
      benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::miopen_convolution_backward(
    const at::Tensor &self, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic, std::array<bool, 3> output_mask) {
  std::cout << "aten::miopen_convolution_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, stride, dilation, groups,
      benchmark, deterministic, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::miopen_convolution_backward_bias(
    const at::Tensor &grad_output) {
  std::cout << "aten::miopen_convolution_backward_bias" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_backward_bias(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor ATenMLIRTypeDefault::miopen_convolution_backward_weight(
    at::IntArrayRef weight_size, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic) {
  std::cout << "aten::miopen_convolution_backward_weight" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_backward_weight(
      weight_size, mlirtens[0], mlirtens[1], padding, stride, dilation, groups,
      benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor ATenMLIRTypeDefault::miopen_convolution_transpose(
    const at::Tensor &self, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef padding, at::IntArrayRef output_padding,
    at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups,
    bool benchmark, bool deterministic) {
  std::cout << "aten::miopen_convolution_transpose" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_transpose(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, output_padding, stride,
      dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::miopen_convolution_transpose_backward(
    const at::Tensor &self, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding,
    at::IntArrayRef output_padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic, std::array<bool, 3> output_mask) {
  std::cout << "aten::miopen_convolution_transpose_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_transpose_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, output_padding, stride,
      dilation, groups, benchmark, deterministic, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::miopen_convolution_transpose_backward_input(
    const at::Tensor &grad_output, const at::Tensor &weight,
    at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation,
    int64_t groups, bool benchmark, bool deterministic) {
  std::cout << "aten::miopen_convolution_transpose_backward_input" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_transpose_backward_input(
      mlirtens[0], mlirtens[1], padding, stride, dilation, groups, benchmark,
      deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor ATenMLIRTypeDefault::miopen_convolution_transpose_backward_weight(
    at::IntArrayRef weight_size, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic) {
  std::cout << "aten::miopen_convolution_transpose_backward_weight"
            << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_convolution_transpose_backward_weight(
      weight_size, mlirtens[0], mlirtens[1], padding, stride, dilation, groups,
      benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor ATenMLIRTypeDefault::miopen_depthwise_convolution(
    const at::Tensor &self, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation,
    int64_t groups, bool benchmark, bool deterministic) {
  std::cout << "aten::miopen_depthwise_convolution" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_depthwise_convolution(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, stride, dilation, groups,
      benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::miopen_depthwise_convolution_backward_input(
    at::IntArrayRef self_size, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic) {
  std::cout << "aten::miopen_depthwise_convolution_backward_input" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_depthwise_convolution_backward_input(
      self_size, mlirtens[0], mlirtens[1], padding, stride, dilation, groups,
      benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::miopen_depthwise_convolution_backward(
    const at::Tensor &self, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic, std::array<bool, 3> output_mask) {
  std::cout << "aten::miopen_depthwise_convolution_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_depthwise_convolution_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, stride, dilation, groups,
      benchmark, deterministic, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::miopen_depthwise_convolution_backward_weight(
    at::IntArrayRef weight_size, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups, bool benchmark,
    bool deterministic) {
  std::cout << "aten::miopen_depthwise_convolution_backward_weight"
            << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_depthwise_convolution_backward_weight(
      weight_size, mlirtens[0], mlirtens[1], padding, stride, dilation, groups,
      benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::miopen_rnn(const at::Tensor &input, at::TensorList weight,
                                int64_t weight_stride0, const at::Tensor &hx,
                                const at::Tensor &cx, int64_t mode,
                                int64_t hidden_size, int64_t num_layers,
                                bool batch_first, double dropout, bool train,
                                bool bidirectional, at::IntArrayRef batch_sizes,
                                const at::Tensor &dropout_state) {
  std::cout << "aten::miopen_rnn" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx, cx, dropout_state};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::miopen_rnn(mlirtens[0], weight, weight_stride0, mlirtens[1],
                     mlirtens[2], mode, hidden_size, num_layers, batch_first,
                     dropout, train, bidirectional, batch_sizes, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<3>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<4>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, std::vector<at::Tensor>>
ATenMLIRTypeDefault::miopen_rnn_backward(
    const at::Tensor &input, at::TensorList weight, int64_t weight_stride0,
    const at::Tensor &weight_buf, const at::Tensor &hx, const at::Tensor &cx,
    const at::Tensor &output, const at::Tensor &grad_output,
    const at::Tensor &grad_hy, const at::Tensor &grad_cy, int64_t mode,
    int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout,
    bool train, bool bidirectional, at::IntArrayRef batch_sizes,
    const at::Tensor &dropout_state, const at::Tensor &reserve,
    std::array<bool, 4> output_mask) {
  std::cout << "aten::miopen_rnn_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      input,   weight_buf,    hx,     cx, output, grad_output, grad_hy,
      grad_cy, dropout_state, reserve};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::miopen_rnn_backward(
      mlirtens[0], weight, weight_stride0, mlirtens[1], mlirtens[2],
      mlirtens[3], mlirtens[4], mlirtens[5], mlirtens[6], mlirtens[7], mode,
      hidden_size, num_layers, batch_first, dropout, train, bidirectional,
      batch_sizes, mlirtens[8], mlirtens[9], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor,
                    std::vector<at::Tensor>>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)),
      std::get<3>(x_result));
}

at::Tensor ATenMLIRTypeDefault::mm(const at::Tensor &self,
                                   const at::Tensor &mat2) {
  std::cout << "aten::mm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mm(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::mm_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &mat2) {
  std::cout << "aten::mm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mm_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::_sparse_mm(const at::Tensor &sparse,
                                           const at::Tensor &dense) {
  std::cout << "aten::_sparse_mm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {sparse, dense};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_mm(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(sparse));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::mode(const at::Tensor &self, int64_t dim, bool keepdim) {
  std::cout << "aten::mode" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mode(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::mode_out(at::Tensor &values, at::Tensor &indices,
                              const at::Tensor &self, int64_t dim,
                              bool keepdim) {
  std::cout << "aten::mode_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {values, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::mode_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(values, indices);
}

at::Tensor ATenMLIRTypeDefault::mul(const at::Tensor &self,
                                    const at::Tensor &other) {
  std::cout << "aten::mul" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mul(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::mul_(at::Tensor &self,
                                      const at::Tensor &other) {
  std::cout << "aten::mul_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].mul_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::mul_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &other) {
  std::cout << "aten::mul_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mul_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::mul(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::mul" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mul(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::mul_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::mul_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].mul_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::mv(const at::Tensor &self,
                                   const at::Tensor &vec) {
  std::cout << "aten::mv" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, vec};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mv(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::mv_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &vec) {
  std::cout << "aten::mv_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, vec};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mv_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::mvlgamma(const at::Tensor &self, int64_t p) {
  std::cout << "aten::mvlgamma" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mvlgamma(mlirtens[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::mvlgamma_(at::Tensor &self, int64_t p) {
  std::cout << "aten::mvlgamma_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].mvlgamma_(p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::narrow_copy(const at::Tensor &self, int64_t dim,
                                            int64_t start, int64_t length) {
  std::cout << "aten::narrow_copy" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].narrow_copy(dim, start, length);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::narrow(const at::Tensor &self, int64_t dim,
                                       int64_t start, int64_t length) {
  std::cout << "aten::narrow" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::narrow(mlirtens[0], dim, start, length);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::native_batch_norm(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    const at::Tensor &running_mean, const at::Tensor &running_var,
    bool training, double momentum, double eps) {
  std::cout << "aten::native_batch_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias, running_mean,
                                              running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::native_batch_norm(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3],
                            mlirtens[4], training, momentum, eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::batch_norm_stats(const at::Tensor &input, double eps) {
  std::cout << "aten::batch_norm_stats" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::batch_norm_stats(mlirtens[0], eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor ATenMLIRTypeDefault::batch_norm_elemt(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    const at::Tensor &mean, const at::Tensor &invstd, double eps) {
  std::cout << "aten::batch_norm_elemt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias, mean,
                                              invstd};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::batch_norm_elemt(mlirtens[0], mlirtens[1], mlirtens[2],
                                         mlirtens[3], mlirtens[4], eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::batch_norm_gather_stats(
    const at::Tensor &input, const at::Tensor &mean, const at::Tensor &invstd,
    const at::Tensor &running_mean, const at::Tensor &running_var,
    double momentum, double eps, int64_t count) {
  std::cout << "aten::batch_norm_gather_stats" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, mean, invstd, running_mean,
                                              running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::batch_norm_gather_stats(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4], momentum,
      eps, count);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::batch_norm_gather_stats_with_counts(
    const at::Tensor &input, const at::Tensor &mean, const at::Tensor &invstd,
    const at::Tensor &running_mean, const at::Tensor &running_var,
    double momentum, double eps, at::IntArrayRef counts) {
  std::cout << "aten::batch_norm_gather_stats_with_counts" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, mean, invstd, running_mean,
                                              running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::batch_norm_gather_stats_with_counts(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4], momentum,
      eps, counts);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::native_batch_norm_backward(
    const at::Tensor &grad_out, const at::Tensor &input,
    const at::Tensor &weight, const at::Tensor &running_mean,
    const at::Tensor &running_var, const at::Tensor &save_mean,
    const at::Tensor &save_invstd, bool train, double eps,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::native_batch_norm_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_out,    input,     weight,     running_mean,
      running_var, save_mean, save_invstd};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::native_batch_norm_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6], train, eps, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_out)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_out)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_out)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::batch_norm_backward_reduce(
    const at::Tensor &grad_out, const at::Tensor &input, const at::Tensor &mean,
    const at::Tensor &invstd, const at::Tensor &weight, bool input_g,
    bool weight_g, bool bias_g) {
  std::cout << "aten::batch_norm_backward_reduce" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_out, input, mean, invstd,
                                              weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::batch_norm_backward_reduce(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4], input_g,
      weight_g, bias_g);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_out)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_out)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_out)),
      bridge::CreateMLIRTensor(std::get<3>(x_result),
                               bridge::GetMLIRDevice(grad_out)));
}

at::Tensor ATenMLIRTypeDefault::batch_norm_backward_elemt(
    const at::Tensor &grad_out, const at::Tensor &input, const at::Tensor &mean,
    const at::Tensor &invstd, const at::Tensor &weight,
    const at::Tensor &mean_dy, const at::Tensor &mean_dy_xmu) {
  std::cout << "aten::batch_norm_backward_elemt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_out, input, mean, invstd, weight, mean_dy, mean_dy_xmu};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::batch_norm_backward_elemt(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_out));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::batch_norm_update_stats(
    const at::Tensor &input, const at::Tensor &running_mean,
    const at::Tensor &running_var, double momentum) {
  std::cout << "aten::batch_norm_update_stats" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, running_mean, running_var};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::batch_norm_update_stats(mlirtens[0], mlirtens[1],
                                                mlirtens[2], momentum);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor ATenMLIRTypeDefault::_nnpack_spatial_convolution(
    const at::Tensor &input, const at::Tensor &weight, const at::Tensor &bias,
    at::IntArrayRef padding) {
  std::cout << "aten::_nnpack_spatial_convolution" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_nnpack_spatial_convolution(mlirtens[0], mlirtens[1],
                                                    mlirtens[2], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_nnpack_spatial_convolution_backward(
    const at::Tensor &input, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::_nnpack_spatial_convolution_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_nnpack_spatial_convolution_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], padding, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor ATenMLIRTypeDefault::_nnpack_spatial_convolution_backward_input(
    const at::Tensor &input, const at::Tensor &grad_output,
    const at::Tensor &weight, at::IntArrayRef padding) {
  std::cout << "aten::_nnpack_spatial_convolution_backward_input" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, grad_output, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_nnpack_spatial_convolution_backward_input(
      mlirtens[0], mlirtens[1], mlirtens[2], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::_nnpack_spatial_convolution_backward_weight(
    const at::Tensor &input, at::IntArrayRef weightsize,
    const at::Tensor &grad_output, at::IntArrayRef padding) {
  std::cout << "aten::_nnpack_spatial_convolution_backward_weight" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_nnpack_spatial_convolution_backward_weight(
      mlirtens[0], weightsize, mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor &ATenMLIRTypeDefault::ones_out(at::Tensor &out,
                                          at::IntArrayRef size) {
  std::cout << "aten::ones_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ones_out(mlirtens[0], size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::pairwise_distance(const at::Tensor &x1,
                                                  const at::Tensor &x2,
                                                  double p, double eps,
                                                  bool keepdim) {
  std::cout << "aten::pairwise_distance" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {x1, x2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::pairwise_distance(mlirtens[0], mlirtens[1], p, eps, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(x1));
}

at::Tensor ATenMLIRTypeDefault::cdist(const at::Tensor &x1,
                                      const at::Tensor &x2, double p) {
  std::cout << "aten::cdist" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {x1, x2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cdist(mlirtens[0], mlirtens[1], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(x1));
}

at::Tensor ATenMLIRTypeDefault::_cdist_backward(const at::Tensor &grad,
                                                const at::Tensor &x1,
                                                const at::Tensor &x2, double p,
                                                const at::Tensor &cdist) {
  std::cout << "aten::_cdist_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, x1, x2, cdist};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cdist_backward(mlirtens[0], mlirtens[1], mlirtens[2],
                                        p, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::pdist(const at::Tensor &self, double p) {
  std::cout << "aten::pdist" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pdist(mlirtens[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_pdist_forward(const at::Tensor &self,
                                               double p) {
  std::cout << "aten::_pdist_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_pdist_forward(mlirtens[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_pdist_backward(const at::Tensor &grad,
                                                const at::Tensor &self,
                                                double p,
                                                const at::Tensor &pdist) {
  std::cout << "aten::_pdist_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, self, pdist};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_pdist_backward(mlirtens[0], mlirtens[1], p, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::cosine_similarity(const at::Tensor &x1,
                                                  const at::Tensor &x2,
                                                  int64_t dim, double eps) {
  std::cout << "aten::cosine_similarity" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {x1, x2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cosine_similarity(mlirtens[0], mlirtens[1], dim, eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(x1));
}

at::Tensor ATenMLIRTypeDefault::permute(const at::Tensor &self,
                                        at::IntArrayRef dims) {
  std::cout << "aten::permute" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].permute(dims);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::numpy_T(const at::Tensor &self) {
  std::cout << "aten::numpy_T" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].numpy_T();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::pixel_shuffle(const at::Tensor &self,
                                              int64_t upscale_factor) {
  std::cout << "aten::pixel_shuffle" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pixel_shuffle(mlirtens[0], upscale_factor);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

bool ATenMLIRTypeDefault::is_pinned(const at::Tensor &self) {
  std::cout << "aten::is_pinned" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].is_pinned();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::pin_memory(const at::Tensor &self) {
  std::cout << "aten::pin_memory" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].pin_memory();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::pinverse(const at::Tensor &self, double rcond) {
  std::cout << "aten::pinverse" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pinverse(mlirtens[0], rcond);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::poisson_nll_loss(const at::Tensor &input,
                                                 const at::Tensor &target,
                                                 bool log_input, bool full,
                                                 double eps,
                                                 int64_t reduction) {
  std::cout << "aten::poisson_nll_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::poisson_nll_loss(mlirtens[0], mlirtens[1], log_input,
                                         full, eps, reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor
ATenMLIRTypeDefault::scalar_tensor(at::Scalar s,
                                   const at::TensorOptions &options) {
  std::cout << "aten::scalar_tensor" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::scalar_tensor(s, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::rand(at::IntArrayRef size,
                                     const at::TensorOptions &options) {
  std::cout << "aten::rand" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rand(size, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::rand(at::IntArrayRef size,
                                     at::Generator *generator,
                                     const at::TensorOptions &options) {
  std::cout << "aten::rand" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rand(size, generator, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::rand_out(at::Tensor &out,
                                          at::IntArrayRef size) {
  std::cout << "aten::rand_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rand_out(mlirtens[0], size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::rand_out(at::Tensor &out, at::IntArrayRef size,
                                          at::Generator *generator) {
  std::cout << "aten::rand_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rand_out(mlirtens[0], size, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::rand_like(const at::Tensor &self) {
  std::cout << "aten::rand_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rand_like(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::rand_like(const at::Tensor &self,
                                          const at::TensorOptions &options) {
  std::cout << "aten::rand_like" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rand_like(mlirtens[0], o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::randint(int64_t high, at::IntArrayRef size,
                                        const at::TensorOptions &options) {
  std::cout << "aten::randint" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint(high, size, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::randint(int64_t high, at::IntArrayRef size,
                                        at::Generator *generator,
                                        const at::TensorOptions &options) {
  std::cout << "aten::randint" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint(high, size, generator, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::randint(int64_t low, int64_t high,
                                        at::IntArrayRef size,
                                        const at::TensorOptions &options) {
  std::cout << "aten::randint" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint(low, high, size, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::randint(int64_t low, int64_t high,
                                        at::IntArrayRef size,
                                        at::Generator *generator,
                                        const at::TensorOptions &options) {
  std::cout << "aten::randint" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint(low, high, size, generator, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::randint_out(at::Tensor &out, int64_t high,
                                             at::IntArrayRef size) {
  std::cout << "aten::randint_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_out(mlirtens[0], high, size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::randint_out(at::Tensor &out, int64_t high,
                                             at::IntArrayRef size,
                                             at::Generator *generator) {
  std::cout << "aten::randint_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_out(mlirtens[0], high, size, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::randint_out(at::Tensor &out, int64_t low,
                                             int64_t high,
                                             at::IntArrayRef size) {
  std::cout << "aten::randint_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_out(mlirtens[0], low, high, size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::randint_out(at::Tensor &out, int64_t low,
                                             int64_t high, at::IntArrayRef size,
                                             at::Generator *generator) {
  std::cout << "aten::randint_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_out(mlirtens[0], low, high, size, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::randint_like(const at::Tensor &self,
                                             int64_t high) {
  std::cout << "aten::randint_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_like(mlirtens[0], high);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::randint_like(const at::Tensor &self,
                                             int64_t low, int64_t high) {
  std::cout << "aten::randint_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_like(mlirtens[0], low, high);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::randint_like(const at::Tensor &self,
                                             int64_t high,
                                             const at::TensorOptions &options) {
  std::cout << "aten::randint_like" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_like(mlirtens[0], high, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::randint_like(const at::Tensor &self,
                                             int64_t low, int64_t high,
                                             const at::TensorOptions &options) {
  std::cout << "aten::randint_like" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randint_like(mlirtens[0], low, high, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::randn(at::IntArrayRef size,
                                      const at::TensorOptions &options) {
  std::cout << "aten::randn" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randn(size, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::randn(at::IntArrayRef size,
                                      at::Generator *generator,
                                      const at::TensorOptions &options) {
  std::cout << "aten::randn" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randn(size, generator, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::randn_out(at::Tensor &out,
                                           at::IntArrayRef size) {
  std::cout << "aten::randn_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randn_out(mlirtens[0], size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::randn_out(at::Tensor &out,
                                           at::IntArrayRef size,
                                           at::Generator *generator) {
  std::cout << "aten::randn_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randn_out(mlirtens[0], size, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::randn_like(const at::Tensor &self) {
  std::cout << "aten::randn_like" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randn_like(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::randn_like(const at::Tensor &self,
                                           const at::TensorOptions &options) {
  std::cout << "aten::randn_like" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randn_like(mlirtens[0], o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::randperm(int64_t n,
                                         const at::TensorOptions &options) {
  std::cout << "aten::randperm" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randperm(n, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::randperm(int64_t n, at::Generator *generator,
                                         const at::TensorOptions &options) {
  std::cout << "aten::randperm" << std::endl;
  at::TensorOptions o_options = options.device(at::DeviceType::CPU);
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randperm(n, generator, o_options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::randperm_out(at::Tensor &out, int64_t n) {
  std::cout << "aten::randperm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randperm_out(mlirtens[0], n);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::randperm_out(at::Tensor &out, int64_t n,
                                              at::Generator *generator) {
  std::cout << "aten::randperm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::randperm_out(mlirtens[0], n, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::range(at::Scalar start, at::Scalar end,
                                      at::Scalar step,
                                      const at::TensorOptions &options) {
  std::cout << "aten::range" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::range(start, end, step, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::range(at::Scalar start, at::Scalar end,
                                      const at::TensorOptions &options) {
  std::cout << "aten::range" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::range(start, end, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::range_out(at::Tensor &out, at::Scalar start,
                                           at::Scalar end, at::Scalar step) {
  std::cout << "aten::range_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::range_out(mlirtens[0], start, end, step);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::reciprocal(const at::Tensor &self) {
  std::cout << "aten::reciprocal" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reciprocal(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::reciprocal_(at::Tensor &self) {
  std::cout << "aten::reciprocal_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reciprocal_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::reciprocal_out(at::Tensor &out,
                                                const at::Tensor &self) {
  std::cout << "aten::reciprocal_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reciprocal_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::neg(const at::Tensor &self) {
  std::cout << "aten::neg" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::neg(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::neg_(at::Tensor &self) {
  std::cout << "aten::neg_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::neg_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::neg_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::neg_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::neg_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::repeat(const at::Tensor &self,
                                       at::IntArrayRef repeats) {
  std::cout << "aten::repeat" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].repeat(repeats);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::repeat_interleave(const at::Tensor &repeats) {
  std::cout << "aten::repeat_interleave" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {repeats};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::repeat_interleave(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(repeats));
}

at::Tensor ATenMLIRTypeDefault::repeat_interleave(const at::Tensor &self,
                                                  const at::Tensor &repeats,
                                                  c10::optional<int64_t> dim) {
  std::cout << "aten::repeat_interleave" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, repeats};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::repeat_interleave(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::repeat_interleave(const at::Tensor &self,
                                                  int64_t repeats,
                                                  c10::optional<int64_t> dim) {
  std::cout << "aten::repeat_interleave" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::repeat_interleave(mlirtens[0], repeats, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::reshape(const at::Tensor &self,
                                        at::IntArrayRef shape) {
  std::cout << "aten::reshape" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reshape(mlirtens[0], shape);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_mkldnn_reshape(const at::Tensor &self,
                                                at::IntArrayRef shape) {
  std::cout << "aten::_mkldnn_reshape" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_mkldnn_reshape(mlirtens[0], shape);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::reshape_as(const at::Tensor &self,
                                           const at::Tensor &other) {
  std::cout << "aten::reshape_as" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].reshape_as(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::round(const at::Tensor &self) {
  std::cout << "aten::round" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::round(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::round_(at::Tensor &self) {
  std::cout << "aten::round_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::round_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::round_out(at::Tensor &out,
                                           const at::Tensor &self) {
  std::cout << "aten::round_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::round_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::rrelu(const at::Tensor &self, at::Scalar lower,
                                      at::Scalar upper, bool training,
                                      at::Generator *generator) {
  std::cout << "aten::rrelu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rrelu(mlirtens[0], lower, upper, training, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::rrelu_(at::Tensor &self, at::Scalar lower,
                                        at::Scalar upper, bool training,
                                        at::Generator *generator) {
  std::cout << "aten::rrelu_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rrelu_(mlirtens[0], lower, upper, training, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::relu(const at::Tensor &self) {
  std::cout << "aten::relu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::relu(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::relu_(at::Tensor &self) {
  std::cout << "aten::relu_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::relu_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::prelu(const at::Tensor &self,
                                      const at::Tensor &weight) {
  std::cout << "aten::prelu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::prelu(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::prelu_backward(const at::Tensor &grad_output,
                                    const at::Tensor &self,
                                    const at::Tensor &weight) {
  std::cout << "aten::prelu_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::prelu_backward(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor ATenMLIRTypeDefault::gelu(const at::Tensor &self) {
  std::cout << "aten::gelu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gelu(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::gelu_backward(const at::Tensor &grad,
                                              const at::Tensor &self) {
  std::cout << "aten::gelu_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gelu_backward(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::hardshrink(const at::Tensor &self,
                                           at::Scalar lambd) {
  std::cout << "aten::hardshrink" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hardshrink(mlirtens[0], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::hardshrink_backward(const at::Tensor &grad_out,
                                                    const at::Tensor &self,
                                                    at::Scalar lambd) {
  std::cout << "aten::hardshrink_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hardshrink_backward(mlirtens[0], mlirtens[1], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_out));
}

at::Tensor ATenMLIRTypeDefault::rsqrt(const at::Tensor &self) {
  std::cout << "aten::rsqrt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rsqrt(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::rsqrt_(at::Tensor &self) {
  std::cout << "aten::rsqrt_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rsqrt_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::rsqrt_out(at::Tensor &out,
                                           const at::Tensor &self) {
  std::cout << "aten::rsqrt_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rsqrt_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::select(const at::Tensor &self, int64_t dim,
                                       int64_t index) {
  std::cout << "aten::select" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::select(mlirtens[0], dim, index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::selu(const at::Tensor &self) {
  std::cout << "aten::selu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::selu(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::selu_(at::Tensor &self) {
  std::cout << "aten::selu_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::selu_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::celu(const at::Tensor &self, at::Scalar alpha) {
  std::cout << "aten::celu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::celu(mlirtens[0], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::celu_(at::Tensor &self, at::Scalar alpha) {
  std::cout << "aten::celu_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::celu_(mlirtens[0], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::sigmoid(const at::Tensor &self) {
  std::cout << "aten::sigmoid" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sigmoid(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sigmoid_(at::Tensor &self) {
  std::cout << "aten::sigmoid_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sigmoid_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::sigmoid_out(at::Tensor &out,
                                             const at::Tensor &self) {
  std::cout << "aten::sigmoid_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sigmoid_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::sin(const at::Tensor &self) {
  std::cout << "aten::sin" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sin(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sin_(at::Tensor &self) {
  std::cout << "aten::sin_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sin_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::sin_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::sin_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sin_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::sinh(const at::Tensor &self) {
  std::cout << "aten::sinh" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sinh(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sinh_(at::Tensor &self) {
  std::cout << "aten::sinh_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sinh_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::sinh_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::sinh_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sinh_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::detach(const at::Tensor &self) {
  std::cout << "aten::detach" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::detach(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::detach_(at::Tensor &self) {
  std::cout << "aten::detach_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::detach_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

int64_t ATenMLIRTypeDefault::size(const at::Tensor &self, int64_t dim) {
  std::cout << "aten::size" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::size(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::slice(const at::Tensor &self, int64_t dim,
                                      int64_t start, int64_t end,
                                      int64_t step) {
  std::cout << "aten::slice" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slice(mlirtens[0], dim, start, end, step);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::slogdet(const at::Tensor &self) {
  std::cout << "aten::slogdet" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slogdet(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::smm(const at::Tensor &self,
                                    const at::Tensor &mat2) {
  std::cout << "aten::smm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::smm(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::softmax(const at::Tensor &self, int64_t dim,
                                        c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::softmax" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softmax(mlirtens[0], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_softmax(const at::Tensor &self, int64_t dim,
                                         bool half_to_float) {
  std::cout << "aten::_softmax" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_softmax(mlirtens[0], dim, half_to_float);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_softmax_backward_data(
    const at::Tensor &grad_output, const at::Tensor &output, int64_t dim,
    const at::Tensor &self) {
  std::cout << "aten::_softmax_backward_data" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_softmax_backward_data(mlirtens[0], mlirtens[1], dim, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::_sparse_add_out(at::Tensor &out,
                                                 const at::Tensor &self,
                                                 const at::Tensor &other,
                                                 at::Scalar alpha) {
  std::cout << "aten::_sparse_add_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sparse_add_out(mlirtens[0], mlirtens[1], mlirtens[2], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::_sparse_dense_add_out(at::Tensor &out,
                                                       const at::Tensor &self,
                                                       const at::Tensor &other,
                                                       at::Scalar alpha) {
  std::cout << "aten::_sparse_dense_add_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sparse_dense_add_out(mlirtens[0], mlirtens[1], mlirtens[2], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::_sparse_div_zerodim_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &other) {
  std::cout << "aten::_sparse_div_zerodim_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sparse_div_zerodim_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::_sparse_div_scalar_out(at::Tensor &out,
                                                        const at::Tensor &self,
                                                        at::Scalar other) {
  std::cout << "aten::_sparse_div_scalar_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_div_scalar_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::_sparse_mul_out(at::Tensor &out,
                                                 const at::Tensor &self,
                                                 const at::Tensor &other) {
  std::cout << "aten::_sparse_mul_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_mul_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::_sparse_mul_zerodim_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &other) {
  std::cout << "aten::_sparse_mul_zerodim_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sparse_mul_zerodim_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::_sparse_mul_scalar_out(at::Tensor &out,
                                                        const at::Tensor &self,
                                                        at::Scalar other) {
  std::cout << "aten::_sparse_mul_scalar_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_mul_scalar_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::vector<at::Tensor> ATenMLIRTypeDefault::split(const at::Tensor &self,
                                                   int64_t split_size,
                                                   int64_t dim) {
  std::cout << "aten::split" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::split(mlirtens[0], split_size, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

std::vector<at::Tensor> ATenMLIRTypeDefault::split_with_sizes(
    const at::Tensor &self, at::IntArrayRef split_sizes, int64_t dim) {
  std::cout << "aten::split_with_sizes" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::split_with_sizes(mlirtens[0], split_sizes, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::squeeze(const at::Tensor &self) {
  std::cout << "aten::squeeze" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::squeeze(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::squeeze(const at::Tensor &self, int64_t dim) {
  std::cout << "aten::squeeze" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::squeeze(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::squeeze_(at::Tensor &self) {
  std::cout << "aten::squeeze_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].squeeze_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::squeeze_(at::Tensor &self, int64_t dim) {
  std::cout << "aten::squeeze_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].squeeze_(dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::sspaddmm(const at::Tensor &self,
                                         const at::Tensor &mat1,
                                         const at::Tensor &mat2,
                                         at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::sspaddmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::sspaddmm(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sspaddmm_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &mat1,
    const at::Tensor &mat2, at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::sspaddmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sspaddmm_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                     mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::stack(at::TensorList tensors, int64_t dim) {
  std::cout << "aten::stack" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::stack(tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(tensors));
}

at::Tensor &ATenMLIRTypeDefault::stack_out(at::Tensor &out,
                                           at::TensorList tensors,
                                           int64_t dim) {
  std::cout << "aten::stack_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::stack_out(mlirtens[0], tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::stft(const at::Tensor &self, int64_t n_fft,
                                     c10::optional<int64_t> hop_length,
                                     c10::optional<int64_t> win_length,
                                     const at::Tensor &window, bool normalized,
                                     bool onesided) {
  std::cout << "aten::stft" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, window};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::stft(mlirtens[0], n_fft, hop_length, win_length,
                             mlirtens[1], normalized, onesided);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

int64_t ATenMLIRTypeDefault::stride(const at::Tensor &self, int64_t dim) {
  std::cout << "aten::stride" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::stride(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::sum(const at::Tensor &self,
                                    c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::sum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sum(mlirtens[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::sum(const at::Tensor &self, at::IntArrayRef dim,
                                    bool keepdim,
                                    c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::sum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sum(mlirtens[0], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sum_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         at::IntArrayRef dim, bool keepdim,
                                         c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::sum_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sum_out(mlirtens[0], mlirtens[1], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::sum_to_size(const at::Tensor &self,
                                            at::IntArrayRef size) {
  std::cout << "aten::sum_to_size" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].sum_to_size(size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::sqrt(const at::Tensor &self) {
  std::cout << "aten::sqrt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sqrt(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sqrt_(at::Tensor &self) {
  std::cout << "aten::sqrt_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sqrt_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::sqrt_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::sqrt_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sqrt_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::std(const at::Tensor &self, bool unbiased) {
  std::cout << "aten::std" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::std(mlirtens[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::std(const at::Tensor &self, at::IntArrayRef dim,
                                    bool unbiased, bool keepdim) {
  std::cout << "aten::std" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::std(mlirtens[0], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::std_mean(const at::Tensor &self, bool unbiased) {
  std::cout << "aten::std_mean" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::std_mean(mlirtens[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::std_mean(const at::Tensor &self, at::IntArrayRef dim,
                              bool unbiased, bool keepdim) {
  std::cout << "aten::std_mean" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::std_mean(mlirtens[0], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::std_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         at::IntArrayRef dim, bool unbiased,
                                         bool keepdim) {
  std::cout << "aten::std_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::std_out(mlirtens[0], mlirtens[1], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::prod(const at::Tensor &self,
                                     c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::prod" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::prod(mlirtens[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::prod(const at::Tensor &self, int64_t dim,
                                     bool keepdim,
                                     c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::prod" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::prod(mlirtens[0], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::prod_out(at::Tensor &out,
                                          const at::Tensor &self, int64_t dim,
                                          bool keepdim,
                                          c10::optional<at::ScalarType> dtype) {
  std::cout << "aten::prod_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::prod_out(mlirtens[0], mlirtens[1], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::t(const at::Tensor &self) {
  std::cout << "aten::t" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::t(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::t_(at::Tensor &self) {
  std::cout << "aten::t_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].t_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::tan(const at::Tensor &self) {
  std::cout << "aten::tan" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tan(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::tan_(at::Tensor &self) {
  std::cout << "aten::tan_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tan_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::tan_out(at::Tensor &out,
                                         const at::Tensor &self) {
  std::cout << "aten::tan_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tan_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::tanh(const at::Tensor &self) {
  std::cout << "aten::tanh" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tanh(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::tanh_(at::Tensor &self) {
  std::cout << "aten::tanh_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tanh_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::tanh_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::tanh_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tanh_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::tensordot(const at::Tensor &self,
                                          const at::Tensor &other,
                                          at::IntArrayRef dims_self,
                                          at::IntArrayRef dims_other) {
  std::cout << "aten::tensordot" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::tensordot(mlirtens[0], mlirtens[1], dims_self, dims_other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::threshold(const at::Tensor &self,
                                          at::Scalar threshold,
                                          at::Scalar value) {
  std::cout << "aten::threshold" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::threshold(mlirtens[0], threshold, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::threshold_(at::Tensor &self,
                                            at::Scalar threshold,
                                            at::Scalar value) {
  std::cout << "aten::threshold_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::threshold_(mlirtens[0], threshold, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::threshold_out(at::Tensor &out,
                                               const at::Tensor &self,
                                               at::Scalar threshold,
                                               at::Scalar value) {
  std::cout << "aten::threshold_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::threshold_out(mlirtens[0], mlirtens[1], threshold, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::threshold_backward(const at::Tensor &grad_output,
                                        const at::Tensor &self,
                                        at::Scalar threshold) {
  std::cout << "aten::threshold_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::threshold_backward(mlirtens[0], mlirtens[1], threshold);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor ATenMLIRTypeDefault::transpose(const at::Tensor &self, int64_t dim0,
                                          int64_t dim1) {
  std::cout << "aten::transpose" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::transpose(mlirtens[0], dim0, dim1);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_mkldnn_transpose(const at::Tensor &self,
                                                  int64_t dim0, int64_t dim1) {
  std::cout << "aten::_mkldnn_transpose" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_mkldnn_transpose(mlirtens[0], dim0, dim1);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::transpose_(at::Tensor &self, int64_t dim0,
                                            int64_t dim1) {
  std::cout << "aten::transpose_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].transpose_(dim0, dim1);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::_mkldnn_transpose_(at::Tensor &self,
                                                    int64_t dim0,
                                                    int64_t dim1) {
  std::cout << "aten::_mkldnn_transpose_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_mkldnn_transpose_(mlirtens[0], dim0, dim1);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::one_hot(const at::Tensor &self,
                                        int64_t num_classes) {
  std::cout << "aten::one_hot" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::one_hot(mlirtens[0], num_classes);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::flip(const at::Tensor &self,
                                     at::IntArrayRef dims) {
  std::cout << "aten::flip" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::flip(mlirtens[0], dims);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::roll(const at::Tensor &self,
                                     at::IntArrayRef shifts,
                                     at::IntArrayRef dims) {
  std::cout << "aten::roll" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::roll(mlirtens[0], shifts, dims);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::rot90(const at::Tensor &self, int64_t k,
                                      at::IntArrayRef dims) {
  std::cout << "aten::rot90" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rot90(mlirtens[0], k, dims);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::trapz(const at::Tensor &y, const at::Tensor &x,
                                      int64_t dim) {
  std::cout << "aten::trapz" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {y, x};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::trapz(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(y));
}

at::Tensor ATenMLIRTypeDefault::trapz(const at::Tensor &y, double dx,
                                      int64_t dim) {
  std::cout << "aten::trapz" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {y};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::trapz(mlirtens[0], dx, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(y));
}

at::Tensor ATenMLIRTypeDefault::_trilinear(
    const at::Tensor &i1, const at::Tensor &i2, const at::Tensor &i3,
    at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3,
    at::IntArrayRef sumdim, int64_t unroll_dim) {
  std::cout << "aten::_trilinear" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {i1, i2, i3};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_trilinear(mlirtens[0], mlirtens[1], mlirtens[2], expand1, expand2,
                     expand3, sumdim, unroll_dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(i1));
}

at::Tensor ATenMLIRTypeDefault::triplet_margin_loss(const at::Tensor &anchor,
                                                    const at::Tensor &positive,
                                                    const at::Tensor &negative,
                                                    double margin, double p,
                                                    double eps, bool swap,
                                                    int64_t reduction) {
  std::cout << "aten::triplet_margin_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {anchor, positive, negative};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::triplet_margin_loss(
      mlirtens[0], mlirtens[1], mlirtens[2], margin, p, eps, swap, reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(anchor));
}

at::Tensor ATenMLIRTypeDefault::trunc(const at::Tensor &self) {
  std::cout << "aten::trunc" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::trunc(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::trunc_(at::Tensor &self) {
  std::cout << "aten::trunc_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::trunc_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::trunc_out(at::Tensor &out,
                                           const at::Tensor &self) {
  std::cout << "aten::trunc_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::trunc_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::type_as(const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::type_as" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].type_as(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

bool ATenMLIRTypeDefault::_has_compatible_shallow_copy_type(
    const at::Tensor &self, const at::Tensor &from) {
  std::cout << "aten::_has_compatible_shallow_copy_type" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, from};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_has_compatible_shallow_copy_type(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_unique(const at::Tensor &self, bool sorted,
                             bool return_inverse) {
  std::cout << "aten::_unique" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_unique(mlirtens[0], sorted, return_inverse);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::unique_dim(const at::Tensor &self, int64_t dim,
                                bool sorted, bool return_inverse,
                                bool return_counts) {
  std::cout << "aten::unique_dim" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::unique_dim(mlirtens[0], dim, sorted, return_inverse, return_counts);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::unique_consecutive(const at::Tensor &self,
                                        bool return_inverse, bool return_counts,
                                        c10::optional<int64_t> dim) {
  std::cout << "aten::unique_consecutive" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::unique_consecutive(mlirtens[0], return_inverse, return_counts, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::unique_dim_consecutive(const at::Tensor &self, int64_t dim,
                                            bool return_inverse,
                                            bool return_counts) {
  std::cout << "aten::unique_dim_consecutive" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::unique_dim_consecutive(mlirtens[0], dim, return_inverse,
                                               return_counts);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_unique2(const at::Tensor &self, bool sorted,
                              bool return_inverse, bool return_counts) {
  std::cout << "aten::_unique2" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_unique2(mlirtens[0], sorted, return_inverse, return_counts);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::_unsafe_view(const at::Tensor &self,
                                             at::IntArrayRef size) {
  std::cout << "aten::_unsafe_view" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_unsafe_view(mlirtens[0], size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::unsqueeze(const at::Tensor &self, int64_t dim) {
  std::cout << "aten::unsqueeze" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::unsqueeze(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::unsqueeze_(at::Tensor &self, int64_t dim) {
  std::cout << "aten::unsqueeze_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].unsqueeze_(dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::var(const at::Tensor &self, bool unbiased) {
  std::cout << "aten::var" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::var(mlirtens[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::var(const at::Tensor &self, at::IntArrayRef dim,
                                    bool unbiased, bool keepdim) {
  std::cout << "aten::var" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::var(mlirtens[0], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::var_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         at::IntArrayRef dim, bool unbiased,
                                         bool keepdim) {
  std::cout << "aten::var_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::var_out(mlirtens[0], mlirtens[1], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::var_mean(const at::Tensor &self, bool unbiased) {
  std::cout << "aten::var_mean" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::var_mean(mlirtens[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::var_mean(const at::Tensor &self, at::IntArrayRef dim,
                              bool unbiased, bool keepdim) {
  std::cout << "aten::var_mean" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::var_mean(mlirtens[0], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::view_as(const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::view_as" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].view_as(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::where(const at::Tensor &condition,
                                      const at::Tensor &self,
                                      const at::Tensor &other) {
  std::cout << "aten::where" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {condition, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::where(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(condition));
}

std::vector<at::Tensor>
ATenMLIRTypeDefault::where(const at::Tensor &condition) {
  std::cout << "aten::where" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {condition};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::where(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::_s_where(const at::Tensor &condition,
                                         const at::Tensor &self,
                                         const at::Tensor &other) {
  std::cout << "aten::_s_where" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {condition, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_s_where(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(condition));
}

at::Tensor ATenMLIRTypeDefault::norm_except_dim(const at::Tensor &v,
                                                int64_t pow, int64_t dim) {
  std::cout << "aten::norm_except_dim" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {v};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::norm_except_dim(mlirtens[0], pow, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(v));
}

at::Tensor ATenMLIRTypeDefault::_weight_norm(const at::Tensor &v,
                                             const at::Tensor &g, int64_t dim) {
  std::cout << "aten::_weight_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {v, g};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_weight_norm(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(v));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_weight_norm_cuda_interface(const at::Tensor &v,
                                                 const at::Tensor &g,
                                                 int64_t dim) {
  std::cout << "aten::_weight_norm_cuda_interface" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {v, g};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_weight_norm_cuda_interface(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result), bridge::GetMLIRDevice(v)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(v)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_weight_norm_cuda_interface_backward(
    const at::Tensor &grad_w, const at::Tensor &saved_v,
    const at::Tensor &saved_g, const at::Tensor &saved_norms, int64_t dim) {
  std::cout << "aten::_weight_norm_cuda_interface_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_w, saved_v, saved_g,
                                              saved_norms};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_weight_norm_cuda_interface_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_w)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_w)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_weight_norm_differentiable_backward(
    const at::Tensor &grad_w, const at::Tensor &saved_v,
    const at::Tensor &saved_g, const at::Tensor &saved_norms, int64_t dim) {
  std::cout << "aten::_weight_norm_differentiable_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_w, saved_v, saved_g,
                                              saved_norms};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_weight_norm_differentiable_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_w)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_w)));
}

at::Tensor &ATenMLIRTypeDefault::zeros_out(at::Tensor &out,
                                           at::IntArrayRef size) {
  std::cout << "aten::zeros_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::zeros_out(mlirtens[0], size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::_standard_gamma_grad(const at::Tensor &self,
                                                     const at::Tensor &output) {
  std::cout << "aten::_standard_gamma_grad" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_standard_gamma_grad(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_standard_gamma(const at::Tensor &self,
                                                at::Generator *generator) {
  std::cout << "aten::_standard_gamma" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_standard_gamma(mlirtens[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_dirichlet_grad(const at::Tensor &x,
                                                const at::Tensor &alpha,
                                                const at::Tensor &total) {
  std::cout << "aten::_dirichlet_grad" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {x, alpha, total};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_dirichlet_grad(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(x));
}

at::Tensor ATenMLIRTypeDefault::_sample_dirichlet(const at::Tensor &self,
                                                  at::Generator *generator) {
  std::cout << "aten::_sample_dirichlet" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sample_dirichlet(mlirtens[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::poisson(const at::Tensor &self,
                                        at::Generator *generator) {
  std::cout << "aten::poisson" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::poisson(mlirtens[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::native_norm(const at::Tensor &self,
                                            at::Scalar p) {
  std::cout << "aten::native_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::native_norm(mlirtens[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_sparse_sum(const at::Tensor &self) {
  std::cout << "aten::_sparse_sum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_sum(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_sparse_sum(const at::Tensor &self,
                                            at::ScalarType dtype) {
  std::cout << "aten::_sparse_sum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_sum(mlirtens[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_sparse_sum(const at::Tensor &self,
                                            at::IntArrayRef dim) {
  std::cout << "aten::_sparse_sum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_sum(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_sparse_sum(const at::Tensor &self,
                                            at::IntArrayRef dim,
                                            at::ScalarType dtype) {
  std::cout << "aten::_sparse_sum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_sum(mlirtens[0], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_sparse_sum_backward(const at::Tensor &grad,
                                                     const at::Tensor &self,
                                                     at::IntArrayRef dim) {
  std::cout << "aten::_sparse_sum_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_sum_backward(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::norm(const at::Tensor &self,
                                     c10::optional<at::Scalar> p,
                                     at::ScalarType dtype) {
  std::cout << "aten::norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::norm(mlirtens[0], p, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::norm(const at::Tensor &self, at::Scalar p) {
  std::cout << "aten::norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::norm(mlirtens[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::norm(const at::Tensor &self,
                                     c10::optional<at::Scalar> p,
                                     at::IntArrayRef dim, bool keepdim,
                                     at::ScalarType dtype) {
  std::cout << "aten::norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::norm(mlirtens[0], p, dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::norm(const at::Tensor &self,
                                     c10::optional<at::Scalar> p,
                                     at::IntArrayRef dim, bool keepdim) {
  std::cout << "aten::norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::norm(mlirtens[0], p, dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::norm_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          c10::optional<at::Scalar> p,
                                          at::IntArrayRef dim, bool keepdim,
                                          at::ScalarType dtype) {
  std::cout << "aten::norm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::norm_out(mlirtens[0], mlirtens[1], p, dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::norm_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          c10::optional<at::Scalar> p,
                                          at::IntArrayRef dim, bool keepdim) {
  std::cout << "aten::norm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::norm_out(mlirtens[0], mlirtens[1], p, dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::frobenius_norm(const at::Tensor &self) {
  std::cout << "aten::frobenius_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::frobenius_norm(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::frobenius_norm(const at::Tensor &self,
                                               at::IntArrayRef dim,
                                               bool keepdim) {
  std::cout << "aten::frobenius_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::frobenius_norm(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::frobenius_norm_out(at::Tensor &out,
                                                    const at::Tensor &self,
                                                    at::IntArrayRef dim,
                                                    bool keepdim) {
  std::cout << "aten::frobenius_norm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::frobenius_norm_out(mlirtens[0], mlirtens[1], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::nuclear_norm(const at::Tensor &self,
                                             bool keepdim) {
  std::cout << "aten::nuclear_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nuclear_norm(mlirtens[0], keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::nuclear_norm_out(at::Tensor &out,
                                                  const at::Tensor &self,
                                                  bool keepdim) {
  std::cout << "aten::nuclear_norm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nuclear_norm_out(mlirtens[0], mlirtens[1], keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::nuclear_norm(const at::Tensor &self,
                                             at::IntArrayRef dim,
                                             bool keepdim) {
  std::cout << "aten::nuclear_norm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nuclear_norm(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::nuclear_norm_out(at::Tensor &out,
                                                  const at::Tensor &self,
                                                  at::IntArrayRef dim,
                                                  bool keepdim) {
  std::cout << "aten::nuclear_norm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::nuclear_norm_out(mlirtens[0], mlirtens[1], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::clone(const at::Tensor &self) {
  std::cout << "aten::clone" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::clone(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::resize_as_(at::Tensor &self,
                                            const at::Tensor &the_template) {
  std::cout << "aten::resize_as_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, the_template};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::resize_as_(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::pow_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         at::Scalar exponent) {
  std::cout << "aten::pow_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pow_out(mlirtens[0], mlirtens[1], exponent);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::pow(const at::Tensor &self,
                                    at::Scalar exponent) {
  std::cout << "aten::pow" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pow(mlirtens[0], exponent);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::zero_(at::Tensor &self) {
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::zero_(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::sub_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &other,
                                         at::Scalar alpha) {
  std::cout << "aten::sub_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sub_out(mlirtens[0], mlirtens[1], mlirtens[2], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::sub(const at::Tensor &self,
                                    const at::Tensor &other, at::Scalar alpha) {
  std::cout << "aten::sub" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sub(mlirtens[0], mlirtens[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sub_(at::Tensor &self, const at::Tensor &other,
                                      at::Scalar alpha) {
  std::cout << "aten::sub_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].sub_(mlirtens[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::sub(const at::Tensor &self, at::Scalar other,
                                    at::Scalar alpha) {
  std::cout << "aten::sub" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sub(mlirtens[0], other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sub_(at::Tensor &self, at::Scalar other,
                                      at::Scalar alpha) {
  std::cout << "aten::sub_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].sub_(other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::rsub(const at::Tensor &self,
                                     const at::Tensor &other,
                                     at::Scalar alpha) {
  std::cout << "aten::rsub" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rsub(mlirtens[0], mlirtens[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::rsub(const at::Tensor &self, at::Scalar other,
                                     at::Scalar alpha) {
  std::cout << "aten::rsub" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rsub(mlirtens[0], other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::s_native_addmm_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &mat1,
    const at::Tensor &mat2, at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::s_native_addmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::s_native_addmm_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::s_native_addmm(const at::Tensor &self,
                                               const at::Tensor &mat1,
                                               const at::Tensor &mat2,
                                               at::Scalar beta,
                                               at::Scalar alpha) {
  std::cout << "aten::s_native_addmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::s_native_addmm(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::s_native_addmm_(at::Tensor &self,
                                                 const at::Tensor &mat1,
                                                 const at::Tensor &mat2,
                                                 at::Scalar beta,
                                                 at::Scalar alpha) {
  std::cout << "aten::s_native_addmm_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::s_native_addmm_(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::_sparse_addmm(const at::Tensor &self,
                                              const at::Tensor &sparse,
                                              const at::Tensor &dense,
                                              at::Scalar beta,
                                              at::Scalar alpha) {
  std::cout << "aten::_sparse_addmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, sparse, dense};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sparse_addmm(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::addmm_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           const at::Tensor &mat1,
                                           const at::Tensor &mat2,
                                           at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::addmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addmm_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                  mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::addmm(const at::Tensor &self,
                                      const at::Tensor &mat1,
                                      const at::Tensor &mat2, at::Scalar beta,
                                      at::Scalar alpha) {
  std::cout << "aten::addmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::addmm(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::addmm_(at::Tensor &self,
                                        const at::Tensor &mat1,
                                        const at::Tensor &mat2, at::Scalar beta,
                                        at::Scalar alpha) {
  std::cout << "aten::addmm_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].addmm_(mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor
ATenMLIRTypeDefault::sparse_coo_tensor(at::IntArrayRef size,
                                       const at::TensorOptions &options) {
  std::cout << "aten::sparse_coo_tensor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sparse_coo_tensor(size, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor
ATenMLIRTypeDefault::sparse_coo_tensor(const at::Tensor &indices,
                                       const at::Tensor &values,
                                       const at::TensorOptions &options) {
  std::cout << "aten::sparse_coo_tensor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {indices, values};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sparse_coo_tensor(mlirtens[0], mlirtens[1], options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(indices));
}

at::Tensor ATenMLIRTypeDefault::sparse_coo_tensor(
    const at::Tensor &indices, const at::Tensor &values, at::IntArrayRef size,
    const at::TensorOptions &options) {
  std::cout << "aten::sparse_coo_tensor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {indices, values};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::sparse_coo_tensor(mlirtens[0], mlirtens[1], size, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(indices));
}

at::Tensor ATenMLIRTypeDefault::_sparse_coo_tensor_unsafe(
    const at::Tensor &indices, const at::Tensor &values, at::IntArrayRef size,
    const at::TensorOptions &options) {
  std::cout << "aten::_sparse_coo_tensor_unsafe" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {indices, values};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sparse_coo_tensor_unsafe(mlirtens[0], mlirtens[1], size, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(indices));
}

at::Tensor ATenMLIRTypeDefault::_sparse_coo_tensor_with_dims(
    int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size,
    const at::TensorOptions &options) {
  std::cout << "aten::_sparse_coo_tensor_with_dims" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::_sparse_coo_tensor_with_dims_and_tensors(
    int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size,
    const at::Tensor &indices, const at::Tensor &values,
    const at::TensorOptions &options) {
  std::cout << "aten::_sparse_coo_tensor_with_dims_and_tensors" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {indices, values};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_sparse_coo_tensor_with_dims_and_tensors(
      sparse_dim, dense_dim, size, mlirtens[0], mlirtens[1], options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(indices));
}

at::Tensor &ATenMLIRTypeDefault::sparse_resize_(at::Tensor &self,
                                                at::IntArrayRef size,
                                                int64_t sparse_dim,
                                                int64_t dense_dim) {
  std::cout << "aten::sparse_resize_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].sparse_resize_(size, sparse_dim, dense_dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::sparse_resize_and_clear_(at::Tensor &self,
                                                          at::IntArrayRef size,
                                                          int64_t sparse_dim,
                                                          int64_t dense_dim) {
  std::cout << "aten::sparse_resize_and_clear_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      mlirtens[0].sparse_resize_and_clear_(size, sparse_dim, dense_dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::sparse_mask(const at::Tensor &self,
                                            const at::Tensor &mask) {
  std::cout << "aten::sparse_mask" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].sparse_mask(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to_dense(const at::Tensor &self) {
  std::cout << "aten::to_dense" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to_dense();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to_dense_backward(const at::Tensor &grad,
                                                  const at::Tensor &input) {
  std::cout << "aten::to_dense_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::to_dense_backward(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

int64_t ATenMLIRTypeDefault::sparse_dim(const at::Tensor &self) {
  std::cout << "aten::sparse_dim" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].sparse_dim();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t ATenMLIRTypeDefault::_dimI(const at::Tensor &self) {
  std::cout << "aten::_dimI" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0]._dimI();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t ATenMLIRTypeDefault::dense_dim(const at::Tensor &self) {
  std::cout << "aten::dense_dim" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].dense_dim();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t ATenMLIRTypeDefault::_dimV(const at::Tensor &self) {
  std::cout << "aten::_dimV" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0]._dimV();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t ATenMLIRTypeDefault::_nnz(const at::Tensor &self) {
  std::cout << "aten::_nnz" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0]._nnz();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::coalesce(const at::Tensor &self) {
  std::cout << "aten::coalesce" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].coalesce();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

bool ATenMLIRTypeDefault::is_coalesced(const at::Tensor &self) {
  std::cout << "aten::is_coalesced" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].is_coalesced();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::_indices(const at::Tensor &self) {
  std::cout << "aten::_indices" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0]._indices();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_values(const at::Tensor &self) {
  std::cout << "aten::_values" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0]._values();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::_coalesced_(at::Tensor &self, bool coalesced) {
  std::cout << "aten::_coalesced_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0]._coalesced_(coalesced);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::indices(const at::Tensor &self) {
  std::cout << "aten::indices" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].indices();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::values(const at::Tensor &self) {
  std::cout << "aten::values" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].values();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::hspmm_out(at::Tensor &out,
                                           const at::Tensor &mat1,
                                           const at::Tensor &mat2) {
  std::cout << "aten::hspmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hspmm_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::hspmm(const at::Tensor &mat1,
                                      const at::Tensor &mat2) {
  std::cout << "aten::hspmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hspmm(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(mat1));
}

at::Tensor &ATenMLIRTypeDefault::copy_sparse_to_sparse_(at::Tensor &self,
                                                        const at::Tensor &src,
                                                        bool non_blocking) {
  std::cout << "aten::copy_sparse_to_sparse_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, src};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::copy_sparse_to_sparse_(mlirtens[0], mlirtens[1], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

std::vector<at::Tensor> ATenMLIRTypeDefault::unbind(const at::Tensor &self,
                                                    int64_t dim) {
  std::cout << "aten::unbind" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::unbind(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::to_sparse(const at::Tensor &self,
                                          int64_t sparse_dim) {
  std::cout << "aten::to_sparse" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to_sparse(sparse_dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to_sparse(const at::Tensor &self) {
  std::cout << "aten::to_sparse" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to_sparse();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to_mkldnn(const at::Tensor &self) {
  std::cout << "aten::to_mkldnn" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to_mkldnn();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::mkldnn_reorder_conv2d_weight(
    const at::Tensor &self, at::IntArrayRef padding, at::IntArrayRef stride,
    at::IntArrayRef dilation, int64_t groups) {
  std::cout << "aten::mkldnn_reorder_conv2d_weight" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_reorder_conv2d_weight(mlirtens[0], padding,
                                                     stride, dilation, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to_mkldnn_backward(const at::Tensor &grad,
                                                   const at::Tensor &input) {
  std::cout << "aten::to_mkldnn_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::to_mkldnn_backward(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::quantize_linear(const at::Tensor &self,
                                                double scale,
                                                int64_t zero_point,
                                                at::ScalarType dtype) {
  std::cout << "aten::quantize_linear" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantize_linear(mlirtens[0], scale, zero_point, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::quantize_linear_per_channel(
    const at::Tensor &self, const at::Tensor &scales,
    const at::Tensor &zero_points, at::IntArrayRef axis, at::ScalarType dtype) {
  std::cout << "aten::quantize_linear_per_channel" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, scales, zero_points};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantize_linear_per_channel(mlirtens[0], mlirtens[1],
                                                    mlirtens[2], axis, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::dequantize(const at::Tensor &self) {
  std::cout << "aten::dequantize" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::dequantize(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_dequantize_linear(const at::Tensor &self,
                                                   double scale,
                                                   int64_t zero_point,
                                                   at::ScalarType dtype) {
  std::cout << "aten::_dequantize_linear" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_dequantize_linear(mlirtens[0], scale, zero_point, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

double ATenMLIRTypeDefault::q_scale(const at::Tensor &self) {
  std::cout << "aten::q_scale" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::q_scale(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t ATenMLIRTypeDefault::q_zero_point(const at::Tensor &self) {
  std::cout << "aten::q_zero_point" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::q_zero_point(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::q_per_channel_scales(const at::Tensor &self) {
  std::cout << "aten::q_per_channel_scales" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::q_per_channel_scales(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor
ATenMLIRTypeDefault::q_per_channel_zero_points(const at::Tensor &self) {
  std::cout << "aten::q_per_channel_zero_points" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::q_per_channel_zero_points(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::int_repr(const at::Tensor &self) {
  std::cout << "aten::int_repr" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::int_repr(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_per_tensor_affine_qtensor(
    const at::Tensor &self, double scale, int64_t zero_point) {
  std::cout << "aten::_per_tensor_affine_qtensor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_per_tensor_affine_qtensor(mlirtens[0], scale, zero_point);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_per_channel_affine_qtensor(
    const at::Tensor &self, const at::Tensor &scale,
    const at::Tensor &zero_point, at::IntArrayRef axis) {
  std::cout << "aten::_per_channel_affine_qtensor" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, scale, zero_point};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_per_channel_affine_qtensor(mlirtens[0], mlirtens[1],
                                                    mlirtens[2], axis);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::QScheme ATenMLIRTypeDefault::qscheme(const at::Tensor &self) {
  std::cout << "aten::qscheme" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].qscheme();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::fake_quantize_per_tensor_affine(
    const at::Tensor &self, double scale, int64_t zero_point, int64_t quant_min,
    int64_t quant_max) {
  std::cout << "aten::fake_quantize_per_tensor_affine" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fake_quantize_per_tensor_affine(
      mlirtens[0], scale, zero_point, quant_min, quant_max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::fake_quantize_per_tensor_affine_backward(
    const at::Tensor &grad, const at::Tensor &self, double scale,
    int64_t zero_point, int64_t quant_min, int64_t quant_max) {
  std::cout << "aten::fake_quantize_per_tensor_affine_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fake_quantize_per_tensor_affine_backward(
      mlirtens[0], mlirtens[1], scale, zero_point, quant_min, quant_max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

at::Tensor ATenMLIRTypeDefault::to(const at::Tensor &self,
                                   const at::TensorOptions &options,
                                   bool non_blocking, bool copy) {
  std::cout << "aten::to" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to(options, non_blocking, copy);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to(const at::Tensor &self, c10::Device device,
                                   at::ScalarType dtype, bool non_blocking,
                                   bool copy) {
  std::cout << "aten::to" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to(device, dtype, non_blocking, copy);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to(const at::Tensor &self, at::ScalarType dtype,
                                   bool non_blocking, bool copy) {
  std::cout << "aten::to" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to(dtype, non_blocking, copy);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::to(const at::Tensor &self,
                                   const at::Tensor &other, bool non_blocking,
                                   bool copy) {
  std::cout << "aten::to" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].to(mlirtens[1], non_blocking, copy);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::vector<at::Tensor> ATenMLIRTypeDefault::meshgrid(at::TensorList tensors) {
  std::cout << "aten::meshgrid" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::meshgrid(tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor ATenMLIRTypeDefault::cartesian_prod(at::TensorList tensors) {
  std::cout << "aten::cartesian_prod" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cartesian_prod(tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(tensors));
}

at::Tensor ATenMLIRTypeDefault::combinations(const at::Tensor &self, int64_t r,
                                             bool with_replacement) {
  std::cout << "aten::combinations" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::combinations(mlirtens[0], r, with_replacement);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Scalar ATenMLIRTypeDefault::item(const at::Tensor &self) {
  std::cout << "aten::item" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].item();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Scalar ATenMLIRTypeDefault::_local_scalar_dense(const at::Tensor &self) {
  std::cout << "aten::_local_scalar_dense" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_local_scalar_dense(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_thnn_fused_lstm_cell(const at::Tensor &input_gates,
                                           const at::Tensor &hidden_gates,
                                           const at::Tensor &cx,
                                           const at::Tensor &input_bias,
                                           const at::Tensor &hidden_bias) {
  std::cout << "aten::_thnn_fused_lstm_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input_gates, hidden_gates, cx,
                                              input_bias, hidden_bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_thnn_fused_lstm_cell(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input_gates)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input_gates)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input_gates)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_thnn_fused_lstm_cell_backward(
    const at::Tensor &grad_hy, const at::Tensor &grad_cy, const at::Tensor &cx,
    const at::Tensor &cy, const at::Tensor &workspace, bool has_bias) {
  std::cout << "aten::_thnn_fused_lstm_cell_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_hy, grad_cy, cx, cy,
                                              workspace};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_thnn_fused_lstm_cell_backward(mlirtens[0], mlirtens[1], mlirtens[2],
                                         mlirtens[3], mlirtens[4], has_bias);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<3>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<4>(x_result),
                               bridge::GetMLIRDevice(grad_hy)));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::_thnn_fused_gru_cell(
    const at::Tensor &input_gates, const at::Tensor &hidden_gates,
    const at::Tensor &hx, const at::Tensor &input_bias,
    const at::Tensor &hidden_bias) {
  std::cout << "aten::_thnn_fused_gru_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input_gates, hidden_gates, hx,
                                              input_bias, hidden_bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_thnn_fused_gru_cell(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input_gates)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input_gates)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_thnn_fused_gru_cell_backward(const at::Tensor &grad_hy,
                                                   const at::Tensor &workspace,
                                                   bool has_bias) {
  std::cout << "aten::_thnn_fused_gru_cell_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_hy, workspace};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_thnn_fused_gru_cell_backward(mlirtens[0], mlirtens[1], has_bias);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<3>(x_result),
                               bridge::GetMLIRDevice(grad_hy)),
      bridge::CreateMLIRTensor(std::get<4>(x_result),
                               bridge::GetMLIRDevice(grad_hy)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::lstm(const at::Tensor &input, at::TensorList hx,
                          at::TensorList params, bool has_biases,
                          int64_t num_layers, double dropout, bool train,
                          bool bidirectional, bool batch_first) {
  std::cout << "aten::lstm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lstm(mlirtens[0], hx, params, has_biases, num_layers,
                             dropout, train, bidirectional, batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::lstm(const at::Tensor &data, const at::Tensor &batch_sizes,
                          at::TensorList hx, at::TensorList params,
                          bool has_biases, int64_t num_layers, double dropout,
                          bool train, bool bidirectional) {
  std::cout << "aten::lstm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {data, batch_sizes};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lstm(mlirtens[0], mlirtens[1], hx, params, has_biases,
                             num_layers, dropout, train, bidirectional);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(data)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(data)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(data)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::gru(const at::Tensor &input, const at::Tensor &hx,
                         at::TensorList params, bool has_biases,
                         int64_t num_layers, double dropout, bool train,
                         bool bidirectional, bool batch_first) {
  std::cout << "aten::gru" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::gru(mlirtens[0], mlirtens[1], params, has_biases, num_layers, dropout,
              train, bidirectional, batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::gru(const at::Tensor &data, const at::Tensor &batch_sizes,
                         const at::Tensor &hx, at::TensorList params,
                         bool has_biases, int64_t num_layers, double dropout,
                         bool train, bool bidirectional) {
  std::cout << "aten::gru" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {data, batch_sizes, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::gru(mlirtens[0], mlirtens[1], mlirtens[2], params, has_biases,
              num_layers, dropout, train, bidirectional);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(data)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(data)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::rnn_tanh(const at::Tensor &input, const at::Tensor &hx,
                              at::TensorList params, bool has_biases,
                              int64_t num_layers, double dropout, bool train,
                              bool bidirectional, bool batch_first) {
  std::cout << "aten::rnn_tanh" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::rnn_tanh(mlirtens[0], mlirtens[1], params, has_biases, num_layers,
                   dropout, train, bidirectional, batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::rnn_tanh(
    const at::Tensor &data, const at::Tensor &batch_sizes, const at::Tensor &hx,
    at::TensorList params, bool has_biases, int64_t num_layers, double dropout,
    bool train, bool bidirectional) {
  std::cout << "aten::rnn_tanh" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {data, batch_sizes, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::rnn_tanh(mlirtens[0], mlirtens[1], mlirtens[2], params, has_biases,
                   num_layers, dropout, train, bidirectional);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(data)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(data)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::rnn_relu(const at::Tensor &input, const at::Tensor &hx,
                              at::TensorList params, bool has_biases,
                              int64_t num_layers, double dropout, bool train,
                              bool bidirectional, bool batch_first) {
  std::cout << "aten::rnn_relu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::rnn_relu(mlirtens[0], mlirtens[1], params, has_biases, num_layers,
                   dropout, train, bidirectional, batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::rnn_relu(
    const at::Tensor &data, const at::Tensor &batch_sizes, const at::Tensor &hx,
    at::TensorList params, bool has_biases, int64_t num_layers, double dropout,
    bool train, bool bidirectional) {
  std::cout << "aten::rnn_relu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {data, batch_sizes, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::rnn_relu(mlirtens[0], mlirtens[1], mlirtens[2], params, has_biases,
                   num_layers, dropout, train, bidirectional);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(data)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(data)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::lstm_cell(const at::Tensor &input, at::TensorList hx,
                               const at::Tensor &w_ih, const at::Tensor &w_hh,
                               const at::Tensor &b_ih, const at::Tensor &b_hh) {
  std::cout << "aten::lstm_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, w_ih, w_hh, b_ih, b_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lstm_cell(mlirtens[0], hx, mlirtens[1], mlirtens[2],
                                  mlirtens[3], mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor
ATenMLIRTypeDefault::gru_cell(const at::Tensor &input, const at::Tensor &hx,
                              const at::Tensor &w_ih, const at::Tensor &w_hh,
                              const at::Tensor &b_ih, const at::Tensor &b_hh) {
  std::cout << "aten::gru_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx,   w_ih,
                                              w_hh,  b_ih, b_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gru_cell(mlirtens[0], mlirtens[1], mlirtens[2],
                                 mlirtens[3], mlirtens[4], mlirtens[5]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::rnn_tanh_cell(
    const at::Tensor &input, const at::Tensor &hx, const at::Tensor &w_ih,
    const at::Tensor &w_hh, const at::Tensor &b_ih, const at::Tensor &b_hh) {
  std::cout << "aten::rnn_tanh_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx,   w_ih,
                                              w_hh,  b_ih, b_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rnn_tanh_cell(mlirtens[0], mlirtens[1], mlirtens[2],
                                      mlirtens[3], mlirtens[4], mlirtens[5]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::rnn_relu_cell(
    const at::Tensor &input, const at::Tensor &hx, const at::Tensor &w_ih,
    const at::Tensor &w_hh, const at::Tensor &b_ih, const at::Tensor &b_hh) {
  std::cout << "aten::rnn_relu_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx,   w_ih,
                                              w_hh,  b_ih, b_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rnn_relu_cell(mlirtens[0], mlirtens[1], mlirtens[2],
                                      mlirtens[3], mlirtens[4], mlirtens[5]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::quantized_lstm(const at::Tensor &input, at::TensorList hx,
                                    at::TensorList params, bool has_biases,
                                    int64_t num_layers, double dropout,
                                    bool train, bool bidirectional,
                                    bool batch_first,
                                    c10::optional<at::ScalarType> dtype,
                                    bool use_dynamic) {
  std::cout << "aten::quantized_lstm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantized_lstm(
      mlirtens[0], hx, params, has_biases, num_layers, dropout, train,
      bidirectional, batch_first, dtype, use_dynamic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::quantized_gru(
    const at::Tensor &input, const at::Tensor &hx, at::TensorList params,
    bool has_biases, int64_t num_layers, double dropout, bool train,
    bool bidirectional, bool batch_first) {
  std::cout << "aten::quantized_gru" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::quantized_gru(mlirtens[0], mlirtens[1], params, has_biases,
                        num_layers, dropout, train, bidirectional, batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::quantized_gru(
    const at::Tensor &data, const at::Tensor &batch_sizes, const at::Tensor &hx,
    at::TensorList params, bool has_biases, int64_t num_layers, double dropout,
    bool train, bool bidirectional) {
  std::cout << "aten::quantized_gru" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {data, batch_sizes, hx};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::quantized_gru(mlirtens[0], mlirtens[1], mlirtens[2], params,
                        has_biases, num_layers, dropout, train, bidirectional);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(data)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(data)));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::quantized_lstm_cell(
    const at::Tensor &input, at::TensorList hx, const at::Tensor &w_ih,
    const at::Tensor &w_hh, const at::Tensor &b_ih, const at::Tensor &b_hh,
    const at::Tensor &packed_ih, const at::Tensor &packed_hh,
    const at::Tensor &col_offsets_ih, const at::Tensor &col_offsets_hh,
    at::Scalar scale_ih, at::Scalar scale_hh, at::Scalar zero_point_ih,
    at::Scalar zero_point_hh) {
  std::cout << "aten::quantized_lstm_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      input,     w_ih,      w_hh,           b_ih,          b_hh,
      packed_ih, packed_hh, col_offsets_ih, col_offsets_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantized_lstm_cell(
      mlirtens[0], hx, mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6], mlirtens[7], mlirtens[8], scale_ih, scale_hh,
      zero_point_ih, zero_point_hh);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor ATenMLIRTypeDefault::quantized_gru_cell(
    const at::Tensor &input, const at::Tensor &hx, const at::Tensor &w_ih,
    const at::Tensor &w_hh, const at::Tensor &b_ih, const at::Tensor &b_hh,
    const at::Tensor &packed_ih, const at::Tensor &packed_hh,
    const at::Tensor &col_offsets_ih, const at::Tensor &col_offsets_hh,
    at::Scalar scale_ih, at::Scalar scale_hh, at::Scalar zero_point_ih,
    at::Scalar zero_point_hh) {
  std::cout << "aten::quantized_gru_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      input, hx,        w_ih,      w_hh,           b_ih,
      b_hh,  packed_ih, packed_hh, col_offsets_ih, col_offsets_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantized_gru_cell(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6], mlirtens[7], mlirtens[8], mlirtens[9], scale_ih,
      scale_hh, zero_point_ih, zero_point_hh);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::quantized_rnn_relu_cell(
    const at::Tensor &input, const at::Tensor &hx, const at::Tensor &w_ih,
    const at::Tensor &w_hh, const at::Tensor &b_ih, const at::Tensor &b_hh,
    const at::Tensor &packed_ih, const at::Tensor &packed_hh,
    const at::Tensor &col_offsets_ih, const at::Tensor &col_offsets_hh,
    at::Scalar scale_ih, at::Scalar scale_hh, at::Scalar zero_point_ih,
    at::Scalar zero_point_hh) {
  std::cout << "aten::quantized_rnn_relu_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      input, hx,        w_ih,      w_hh,           b_ih,
      b_hh,  packed_ih, packed_hh, col_offsets_ih, col_offsets_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantized_rnn_relu_cell(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6], mlirtens[7], mlirtens[8], mlirtens[9], scale_ih,
      scale_hh, zero_point_ih, zero_point_hh);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

at::Tensor ATenMLIRTypeDefault::quantized_rnn_tanh_cell(
    const at::Tensor &input, const at::Tensor &hx, const at::Tensor &w_ih,
    const at::Tensor &w_hh, const at::Tensor &b_ih, const at::Tensor &b_hh,
    const at::Tensor &packed_ih, const at::Tensor &packed_hh,
    const at::Tensor &col_offsets_ih, const at::Tensor &col_offsets_hh,
    at::Scalar scale_ih, at::Scalar scale_hh, at::Scalar zero_point_ih,
    at::Scalar zero_point_hh) {
  std::cout << "aten::quantized_rnn_tanh_cell" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      input, hx,        w_ih,      w_hh,           b_ih,
      b_hh,  packed_ih, packed_hh, col_offsets_ih, col_offsets_hh};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::quantized_rnn_tanh_cell(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], mlirtens[6], mlirtens[7], mlirtens[8], mlirtens[9], scale_ih,
      scale_hh, zero_point_ih, zero_point_hh);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(input));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::_pack_padded_sequence(
    const at::Tensor &input, const at::Tensor &lengths, bool batch_first) {
  std::cout << "aten::_pack_padded_sequence" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {input, lengths};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_pack_padded_sequence(mlirtens[0], mlirtens[1], batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(input)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(input)));
}

at::Tensor ATenMLIRTypeDefault::_pack_padded_sequence_backward(
    const at::Tensor &grad, at::IntArrayRef input_size,
    const at::Tensor &batch_sizes, bool batch_first) {
  std::cout << "aten::_pack_padded_sequence_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad, batch_sizes};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_pack_padded_sequence_backward(
      mlirtens[0], input_size, mlirtens[1], batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad));
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::_pad_packed_sequence(
    const at::Tensor &data, const at::Tensor &batch_sizes, bool batch_first,
    at::Scalar padding_value, int64_t total_length) {
  std::cout << "aten::_pad_packed_sequence" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {data, batch_sizes};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_pad_packed_sequence(
      mlirtens[0], mlirtens[1], batch_first, padding_value, total_length);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(data)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(data)));
}

at::Tensor &ATenMLIRTypeDefault::set_(at::Tensor &self, at::Storage source) {
  std::cout << "aten::set_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].set_(source);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::set_(at::Tensor &self, at::Storage source,
                                      int64_t storage_offset,
                                      at::IntArrayRef size,
                                      at::IntArrayRef stride) {
  std::cout << "aten::set_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].set_(source, storage_offset, size, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::set_(at::Tensor &self,
                                      const at::Tensor &source) {
  std::cout << "aten::set_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].set_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::set_(at::Tensor &self) {
  std::cout << "aten::set_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].set_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &
ATenMLIRTypeDefault::set_quantizer_(at::Tensor &self,
                                    at::ConstQuantizerPtr quantizer) {
  std::cout << "aten::set_quantizer_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].set_quantizer_(quantizer);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

bool ATenMLIRTypeDefault::is_set_to(const at::Tensor &self,
                                    const at::Tensor &tensor) {
  std::cout << "aten::is_set_to" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, tensor};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].is_set_to(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor &ATenMLIRTypeDefault::masked_fill_(at::Tensor &self,
                                              const at::Tensor &mask,
                                              at::Scalar value) {
  std::cout << "aten::masked_fill_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].masked_fill_(mlirtens[1], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::masked_fill(const at::Tensor &self,
                                            const at::Tensor &mask,
                                            at::Scalar value) {
  std::cout << "aten::masked_fill" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::masked_fill(mlirtens[0], mlirtens[1], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::masked_fill_(at::Tensor &self,
                                              const at::Tensor &mask,
                                              const at::Tensor &value) {
  std::cout << "aten::masked_fill_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask, value};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].masked_fill_(mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::masked_fill(const at::Tensor &self,
                                            const at::Tensor &mask,
                                            const at::Tensor &value) {
  std::cout << "aten::masked_fill" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask, value};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::masked_fill(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::masked_scatter_(at::Tensor &self,
                                                 const at::Tensor &mask,
                                                 const at::Tensor &source) {
  std::cout << "aten::masked_scatter_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].masked_scatter_(mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::masked_scatter(const at::Tensor &self,
                                               const at::Tensor &mask,
                                               const at::Tensor &source) {
  std::cout << "aten::masked_scatter" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::masked_scatter(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::view(const at::Tensor &self,
                                     at::IntArrayRef size) {
  std::cout << "aten::view" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].view(size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::put_(at::Tensor &self, const at::Tensor &index,
                                      const at::Tensor &source,
                                      bool accumulate) {
  std::cout << "aten::put_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].put_(mlirtens[1], mlirtens[2], accumulate);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::index_add_(at::Tensor &self, int64_t dim,
                                            const at::Tensor &index,
                                            const at::Tensor &source) {
  std::cout << "aten::index_add_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].index_add_(dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::index_add(const at::Tensor &self, int64_t dim,
                                          const at::Tensor &index,
                                          const at::Tensor &source) {
  std::cout << "aten::index_add" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::index_add(mlirtens[0], dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::index_fill_(at::Tensor &self, int64_t dim,
                                             const at::Tensor &index,
                                             at::Scalar value) {
  std::cout << "aten::index_fill_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].index_fill_(dim, mlirtens[1], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::index_fill(const at::Tensor &self, int64_t dim,
                                           const at::Tensor &index,
                                           at::Scalar value) {
  std::cout << "aten::index_fill" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::index_fill(mlirtens[0], dim, mlirtens[1], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::index_fill_(at::Tensor &self, int64_t dim,
                                             const at::Tensor &index,
                                             const at::Tensor &value) {
  std::cout << "aten::index_fill_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, value};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].index_fill_(dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::index_fill(const at::Tensor &self, int64_t dim,
                                           const at::Tensor &index,
                                           const at::Tensor &value) {
  std::cout << "aten::index_fill" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, value};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::index_fill(mlirtens[0], dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::scatter_(at::Tensor &self, int64_t dim,
                                          const at::Tensor &index,
                                          const at::Tensor &src) {
  std::cout << "aten::scatter_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, src};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].scatter_(dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::scatter(const at::Tensor &self, int64_t dim,
                                        const at::Tensor &index,
                                        const at::Tensor &src) {
  std::cout << "aten::scatter" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, src};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::scatter(mlirtens[0], dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::scatter_(at::Tensor &self, int64_t dim,
                                          const at::Tensor &index,
                                          at::Scalar value) {
  std::cout << "aten::scatter_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].scatter_(dim, mlirtens[1], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::scatter(const at::Tensor &self, int64_t dim,
                                        const at::Tensor &index,
                                        at::Scalar value) {
  std::cout << "aten::scatter" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::scatter(mlirtens[0], dim, mlirtens[1], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::scatter_add_(at::Tensor &self, int64_t dim,
                                              const at::Tensor &index,
                                              const at::Tensor &src) {
  std::cout << "aten::scatter_add_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, src};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].scatter_add_(dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::scatter_add(const at::Tensor &self, int64_t dim,
                                            const at::Tensor &index,
                                            const at::Tensor &src) {
  std::cout << "aten::scatter_add" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, src};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::scatter_add(mlirtens[0], dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::lt_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::lt_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].lt_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::lt_(at::Tensor &self,
                                     const at::Tensor &other) {
  std::cout << "aten::lt_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].lt_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::gt_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::gt_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].gt_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::gt_(at::Tensor &self,
                                     const at::Tensor &other) {
  std::cout << "aten::gt_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].gt_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::le_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::le_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].le_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::le_(at::Tensor &self,
                                     const at::Tensor &other) {
  std::cout << "aten::le_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].le_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::ge_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::ge_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].ge_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::ge_(at::Tensor &self,
                                     const at::Tensor &other) {
  std::cout << "aten::ge_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].ge_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::eq_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::eq_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].eq_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::eq_(at::Tensor &self,
                                     const at::Tensor &other) {
  std::cout << "aten::eq_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].eq_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::ne_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::ne_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].ne_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::ne_(at::Tensor &self,
                                     const at::Tensor &other) {
  std::cout << "aten::ne_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].ne_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::__and__(const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::__and__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__and__(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::__and__(const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::__and__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__and__(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::__iand__(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::__iand__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__iand__(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::__iand__(at::Tensor &self,
                                          const at::Tensor &other) {
  std::cout << "aten::__iand__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__iand__(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::__or__(const at::Tensor &self,
                                       at::Scalar other) {
  std::cout << "aten::__or__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__or__(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::__or__(const at::Tensor &self,
                                       const at::Tensor &other) {
  std::cout << "aten::__or__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__or__(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::__ior__(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::__ior__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__ior__(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::__ior__(at::Tensor &self,
                                         const at::Tensor &other) {
  std::cout << "aten::__ior__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__ior__(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::__xor__(const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::__xor__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__xor__(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::__xor__(const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::__xor__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__xor__(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::__ixor__(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::__ixor__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__ixor__(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::__ixor__(at::Tensor &self,
                                          const at::Tensor &other) {
  std::cout << "aten::__ixor__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__ixor__(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::__lshift__(const at::Tensor &self,
                                           at::Scalar other) {
  std::cout << "aten::__lshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__lshift__(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::__lshift__(const at::Tensor &self,
                                           const at::Tensor &other) {
  std::cout << "aten::__lshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__lshift__(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::__ilshift__(at::Tensor &self,
                                             at::Scalar other) {
  std::cout << "aten::__ilshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__ilshift__(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::__ilshift__(at::Tensor &self,
                                             const at::Tensor &other) {
  std::cout << "aten::__ilshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__ilshift__(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::__rshift__(const at::Tensor &self,
                                           at::Scalar other) {
  std::cout << "aten::__rshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__rshift__(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::__rshift__(const at::Tensor &self,
                                           const at::Tensor &other) {
  std::cout << "aten::__rshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::__rshift__(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::__irshift__(at::Tensor &self,
                                             at::Scalar other) {
  std::cout << "aten::__irshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__irshift__(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::__irshift__(at::Tensor &self,
                                             const at::Tensor &other) {
  std::cout << "aten::__irshift__" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].__irshift__(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::lgamma_(at::Tensor &self) {
  std::cout << "aten::lgamma_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].lgamma_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::atan2_(at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::atan2_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].atan2_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::tril_(at::Tensor &self, int64_t diagonal) {
  std::cout << "aten::tril_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].tril_(diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::triu_(at::Tensor &self, int64_t diagonal) {
  std::cout << "aten::triu_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].triu_(diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::digamma_(at::Tensor &self) {
  std::cout << "aten::digamma_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].digamma_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::polygamma_(at::Tensor &self, int64_t n) {
  std::cout << "aten::polygamma_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].polygamma_(n);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::renorm_(at::Tensor &self, at::Scalar p,
                                         int64_t dim, at::Scalar maxnorm) {
  std::cout << "aten::renorm_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].renorm_(p, dim, maxnorm);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::pow_(at::Tensor &self, at::Scalar exponent) {
  std::cout << "aten::pow_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].pow_(exponent);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::pow_(at::Tensor &self,
                                      const at::Tensor &exponent) {
  std::cout << "aten::pow_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, exponent};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].pow_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::lerp_(at::Tensor &self, const at::Tensor &end,
                                       at::Scalar weight) {
  std::cout << "aten::lerp_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, end};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].lerp_(mlirtens[1], weight);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::lerp_(at::Tensor &self, const at::Tensor &end,
                                       const at::Tensor &weight) {
  std::cout << "aten::lerp_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, end, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].lerp_(mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::fmod_(at::Tensor &self, at::Scalar other) {
  std::cout << "aten::fmod_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].fmod_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::fmod_(at::Tensor &self,
                                       const at::Tensor &other) {
  std::cout << "aten::fmod_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].fmod_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::remainder_(at::Tensor &self,
                                            at::Scalar other) {
  std::cout << "aten::remainder_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].remainder_(other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::remainder_(at::Tensor &self,
                                            const at::Tensor &other) {
  std::cout << "aten::remainder_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].remainder_(mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::addbmm_(at::Tensor &self,
                                         const at::Tensor &batch1,
                                         const at::Tensor &batch2,
                                         at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::addbmm_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, batch1, batch2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].addbmm_(mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::addbmm_out(at::Tensor &out,
                                            const at::Tensor &self,
                                            const at::Tensor &batch1,
                                            const at::Tensor &batch2,
                                            at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::addbmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, batch1, batch2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addbmm_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                   mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::addbmm(const at::Tensor &self,
                                       const at::Tensor &batch1,
                                       const at::Tensor &batch2,
                                       at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::addbmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, batch1, batch2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::addbmm(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::addcdiv_(at::Tensor &self,
                                          const at::Tensor &tensor1,
                                          const at::Tensor &tensor2,
                                          at::Scalar value) {
  std::cout << "aten::addcdiv_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, tensor1, tensor2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].addcdiv_(mlirtens[1], mlirtens[2], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::random_(at::Tensor &self, int64_t from,
                                         int64_t to, at::Generator *generator) {
  std::cout << "aten::random_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].random_(from, to, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::random_(at::Tensor &self, int64_t to,
                                         at::Generator *generator) {
  std::cout << "aten::random_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].random_(to, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::random_(at::Tensor &self,
                                         at::Generator *generator) {
  std::cout << "aten::random_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].random_(generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::uniform_(at::Tensor &self, double from,
                                          double to, at::Generator *generator) {
  std::cout << "aten::uniform_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].uniform_(from, to, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::normal_(at::Tensor &self, double mean,
                                         double std, at::Generator *generator) {
  std::cout << "aten::normal_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].normal_(mean, std, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::cauchy_(at::Tensor &self, double median,
                                         double sigma,
                                         at::Generator *generator) {
  std::cout << "aten::cauchy_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].cauchy_(median, sigma, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::log_normal_(at::Tensor &self, double mean,
                                             double std,
                                             at::Generator *generator) {
  std::cout << "aten::log_normal_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].log_normal_(mean, std, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::exponential_(at::Tensor &self, double lambd,
                                              at::Generator *generator) {
  std::cout << "aten::exponential_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].exponential_(lambd, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::geometric_(at::Tensor &self, double p,
                                            at::Generator *generator) {
  std::cout << "aten::geometric_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].geometric_(p, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::diag_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          int64_t diagonal) {
  std::cout << "aten::diag_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::diag_out(mlirtens[0], mlirtens[1], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::diag(const at::Tensor &self, int64_t diagonal) {
  std::cout << "aten::diag" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::diag(mlirtens[0], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::cross_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           const at::Tensor &other,
                                           c10::optional<int64_t> dim) {
  std::cout << "aten::cross_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cross_out(mlirtens[0], mlirtens[1], mlirtens[2], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::cross(const at::Tensor &self,
                                      const at::Tensor &other,
                                      c10::optional<int64_t> dim) {
  std::cout << "aten::cross" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cross(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::triu_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          int64_t diagonal) {
  std::cout << "aten::triu_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::triu_out(mlirtens[0], mlirtens[1], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::triu(const at::Tensor &self, int64_t diagonal) {
  std::cout << "aten::triu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::triu(mlirtens[0], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::tril_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          int64_t diagonal) {
  std::cout << "aten::tril_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tril_out(mlirtens[0], mlirtens[1], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::tril(const at::Tensor &self, int64_t diagonal) {
  std::cout << "aten::tril" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tril(mlirtens[0], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::tril_indices(int64_t row, int64_t col,
                                             int64_t offset,
                                             const at::TensorOptions &options) {
  std::cout << "aten::tril_indices" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tril_indices(row, col, offset, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::triu_indices(int64_t row, int64_t col,
                                             int64_t offset,
                                             const at::TensorOptions &options) {
  std::cout << "aten::triu_indices" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::triu_indices(row, col, offset, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor ATenMLIRTypeDefault::trace(const at::Tensor &self) {
  std::cout << "aten::trace" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::trace(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::ne_out(at::Tensor &out, const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::ne_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ne_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::ne(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::ne" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ne(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::ne_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::ne_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ne_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::ne(const at::Tensor &self,
                                   const at::Tensor &other) {
  std::cout << "aten::ne" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ne(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::eq_out(at::Tensor &out, const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::eq_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eq_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::eq(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::eq" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eq(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::eq_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::eq_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eq_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::eq(const at::Tensor &self,
                                   const at::Tensor &other) {
  std::cout << "aten::eq" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eq(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::ge_out(at::Tensor &out, const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::ge_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ge_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::ge(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::ge" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ge(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::ge_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::ge_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ge_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::ge(const at::Tensor &self,
                                   const at::Tensor &other) {
  std::cout << "aten::ge" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ge(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::le_out(at::Tensor &out, const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::le_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::le_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::le(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::le" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::le(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::le_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::le_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::le_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::le(const at::Tensor &self,
                                   const at::Tensor &other) {
  std::cout << "aten::le" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::le(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::gt_out(at::Tensor &out, const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::gt_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gt_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::gt(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::gt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gt(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::gt_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::gt_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gt_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::gt(const at::Tensor &self,
                                   const at::Tensor &other) {
  std::cout << "aten::gt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gt(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::lt_out(at::Tensor &out, const at::Tensor &self,
                                        at::Scalar other) {
  std::cout << "aten::lt_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lt_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::lt(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::lt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lt(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::lt_out(at::Tensor &out, const at::Tensor &self,
                                        const at::Tensor &other) {
  std::cout << "aten::lt_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lt_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::lt(const at::Tensor &self,
                                   const at::Tensor &other) {
  std::cout << "aten::lt" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lt(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::take_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          const at::Tensor &index) {
  std::cout << "aten::take_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::take_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::take(const at::Tensor &self,
                                     const at::Tensor &index) {
  std::cout << "aten::take" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::take(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::index_select_out(at::Tensor &out,
                                                  const at::Tensor &self,
                                                  int64_t dim,
                                                  const at::Tensor &index) {
  std::cout << "aten::index_select_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::index_select_out(mlirtens[0], mlirtens[1], dim, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::index_select(const at::Tensor &self,
                                             int64_t dim,
                                             const at::Tensor &index) {
  std::cout << "aten::index_select" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::index_select(mlirtens[0], dim, mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::masked_select_out(at::Tensor &out,
                                                   const at::Tensor &self,
                                                   const at::Tensor &mask) {
  std::cout << "aten::masked_select_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mask};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::masked_select_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::masked_select(const at::Tensor &self,
                                              const at::Tensor &mask) {
  std::cout << "aten::masked_select" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mask};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::masked_select(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::nonzero_out(at::Tensor &out,
                                             const at::Tensor &self) {
  std::cout << "aten::nonzero_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nonzero_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::nonzero(const at::Tensor &self) {
  std::cout << "aten::nonzero" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nonzero(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::vector<at::Tensor>
ATenMLIRTypeDefault::nonzero_numpy(const at::Tensor &self) {
  std::cout << "aten::nonzero_numpy" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nonzero_numpy(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor &ATenMLIRTypeDefault::gather_out(at::Tensor &out,
                                            const at::Tensor &self, int64_t dim,
                                            const at::Tensor &index,
                                            bool sparse_grad) {
  std::cout << "aten::gather_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::gather_out(mlirtens[0], mlirtens[1], dim, mlirtens[2], sparse_grad);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::gather(const at::Tensor &self, int64_t dim,
                                       const at::Tensor &index,
                                       bool sparse_grad) {
  std::cout << "aten::gather" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::gather(mlirtens[0], dim, mlirtens[1], sparse_grad);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_gather_sparse_backward(
    const at::Tensor &self, int64_t dim, const at::Tensor &index,
    const at::Tensor &grad) {
  std::cout << "aten::_gather_sparse_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, grad};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_gather_sparse_backward(mlirtens[0], dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::addcmul_out(at::Tensor &out,
                                             const at::Tensor &self,
                                             const at::Tensor &tensor1,
                                             const at::Tensor &tensor2,
                                             at::Scalar value) {
  std::cout << "aten::addcmul_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, tensor1, tensor2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addcmul_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                    mlirtens[3], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::addcmul(const at::Tensor &self,
                                        const at::Tensor &tensor1,
                                        const at::Tensor &tensor2,
                                        at::Scalar value) {
  std::cout << "aten::addcmul" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, tensor1, tensor2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addcmul(mlirtens[0], mlirtens[1], mlirtens[2], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::addcmul_(at::Tensor &self,
                                          const at::Tensor &tensor1,
                                          const at::Tensor &tensor2,
                                          at::Scalar value) {
  std::cout << "aten::addcmul_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, tensor1, tensor2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].addcmul_(mlirtens[1], mlirtens[2], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::addcdiv_out(at::Tensor &out,
                                             const at::Tensor &self,
                                             const at::Tensor &tensor1,
                                             const at::Tensor &tensor2,
                                             at::Scalar value) {
  std::cout << "aten::addcdiv_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, tensor1, tensor2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addcdiv_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                    mlirtens[3], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::addcdiv(const at::Tensor &self,
                                        const at::Tensor &tensor1,
                                        const at::Tensor &tensor2,
                                        at::Scalar value) {
  std::cout << "aten::addcdiv" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, tensor1, tensor2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::addcdiv(mlirtens[0], mlirtens[1], mlirtens[2], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::lstsq_out(at::Tensor &X, at::Tensor &qr,
                               const at::Tensor &self, const at::Tensor &A) {
  std::cout << "aten::lstsq_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {X, qr, self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::lstsq_out(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(X, qr);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::lstsq(const at::Tensor &self, const at::Tensor &A) {
  std::cout << "aten::lstsq" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lstsq(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::triangular_solve_out(at::Tensor &X, at::Tensor &M,
                                          const at::Tensor &self,
                                          const at::Tensor &A, bool upper,
                                          bool transpose, bool unitriangular) {
  std::cout << "aten::triangular_solve_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {X, M, self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::triangular_solve_out(mlirtens[0], mlirtens[1], mlirtens[2],
                               mlirtens[3], upper, transpose, unitriangular);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(X, M);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::triangular_solve(const at::Tensor &self,
                                      const at::Tensor &A, bool upper,
                                      bool transpose, bool unitriangular) {
  std::cout << "aten::triangular_solve" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::triangular_solve(mlirtens[0], mlirtens[1], upper,
                                         transpose, unitriangular);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_triangular_solve_helper(const at::Tensor &self,
                                              const at::Tensor &A, bool upper,
                                              bool transpose,
                                              bool unitriangular) {
  std::cout << "aten::_triangular_solve_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_triangular_solve_helper(
      mlirtens[0], mlirtens[1], upper, transpose, unitriangular);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::symeig_out(at::Tensor &e, at::Tensor &V,
                                const at::Tensor &self, bool eigenvectors,
                                bool upper) {
  std::cout << "aten::symeig_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {e, V, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::symeig_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                   eigenvectors, upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(e, V);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::symeig(const at::Tensor &self, bool eigenvectors,
                            bool upper) {
  std::cout << "aten::symeig" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::symeig(mlirtens[0], eigenvectors, upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_symeig_helper(const at::Tensor &self, bool eigenvectors,
                                    bool upper) {
  std::cout << "aten::_symeig_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_symeig_helper(mlirtens[0], eigenvectors, upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::eig_out(at::Tensor &e, at::Tensor &v,
                             const at::Tensor &self, bool eigenvectors) {
  std::cout << "aten::eig_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {e, v, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::eig_out(mlirtens[0], mlirtens[1], mlirtens[2], eigenvectors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(e, v);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::eig(const at::Tensor &self, bool eigenvectors) {
  std::cout << "aten::eig" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::eig(mlirtens[0], eigenvectors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::svd_out(at::Tensor &U, at::Tensor &S, at::Tensor &V,
                             const at::Tensor &self, bool some,
                             bool compute_uv) {
  std::cout << "aten::svd_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {U, S, V, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::svd_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                mlirtens[3], some, compute_uv);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(U, S, V);
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::svd(const at::Tensor &self, bool some, bool compute_uv) {
  std::cout << "aten::svd" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::svd(mlirtens[0], some, compute_uv);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_svd_helper(const at::Tensor &self, bool some,
                                 bool compute_uv) {
  std::cout << "aten::_svd_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_svd_helper(mlirtens[0], some, compute_uv);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::cholesky_out(at::Tensor &out,
                                              const at::Tensor &self,
                                              bool upper) {
  std::cout << "aten::cholesky_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cholesky_out(mlirtens[0], mlirtens[1], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::cholesky(const at::Tensor &self, bool upper) {
  std::cout << "aten::cholesky" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cholesky(mlirtens[0], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cholesky_helper(const at::Tensor &self,
                                                 bool upper) {
  std::cout << "aten::_cholesky_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cholesky_helper(mlirtens[0], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::cholesky_solve_out(at::Tensor &out,
                                                    const at::Tensor &self,
                                                    const at::Tensor &input2,
                                                    bool upper) {
  std::cout << "aten::cholesky_solve_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, input2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::cholesky_solve_out(mlirtens[0], mlirtens[1], mlirtens[2], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::cholesky_solve(const at::Tensor &self,
                                               const at::Tensor &input2,
                                               bool upper) {
  std::cout << "aten::cholesky_solve" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, input2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cholesky_solve(mlirtens[0], mlirtens[1], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_cholesky_solve_helper(const at::Tensor &self,
                                                       const at::Tensor &A,
                                                       bool upper) {
  std::cout << "aten::_cholesky_solve_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cholesky_solve_helper(mlirtens[0], mlirtens[1], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::solve(const at::Tensor &self, const at::Tensor &A) {
  std::cout << "aten::solve" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::solve(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::solve_out(at::Tensor &solution, at::Tensor &lu,
                               const at::Tensor &self, const at::Tensor &A) {
  std::cout << "aten::solve_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {solution, lu, self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::solve_out(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(solution, lu);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_solve_helper(const at::Tensor &self,
                                   const at::Tensor &A) {
  std::cout << "aten::_solve_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, A};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_solve_helper(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::cholesky_inverse_out(at::Tensor &out,
                                                      const at::Tensor &self,
                                                      bool upper) {
  std::cout << "aten::cholesky_inverse_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cholesky_inverse_out(mlirtens[0], mlirtens[1], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::cholesky_inverse(const at::Tensor &self,
                                                 bool upper) {
  std::cout << "aten::cholesky_inverse" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::cholesky_inverse(mlirtens[0], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::qr_out(at::Tensor &Q, at::Tensor &R,
                            const at::Tensor &self, bool some) {
  std::cout << "aten::qr_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {Q, R, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::qr_out(mlirtens[0], mlirtens[1], mlirtens[2], some);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(Q, R);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::qr(const at::Tensor &self, bool some) {
  std::cout << "aten::qr" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::qr(mlirtens[0], some);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_qr_helper(const at::Tensor &self, bool some) {
  std::cout << "aten::_qr_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_qr_helper(mlirtens[0], some);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::geqrf_out(at::Tensor &a, at::Tensor &tau,
                               const at::Tensor &self) {
  std::cout << "aten::geqrf_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {a, tau, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::geqrf_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(a, tau);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::geqrf(const at::Tensor &self) {
  std::cout << "aten::geqrf" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::geqrf(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::orgqr_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           const at::Tensor &input2) {
  std::cout << "aten::orgqr_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, input2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::orgqr_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::orgqr(const at::Tensor &self,
                                      const at::Tensor &input2) {
  std::cout << "aten::orgqr" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, input2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::orgqr(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::ormqr_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           const at::Tensor &input2,
                                           const at::Tensor &input3, bool left,
                                           bool transpose) {
  std::cout << "aten::ormqr_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, input2, input3};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::ormqr_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                  mlirtens[3], left, transpose);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::ormqr(const at::Tensor &self,
                                      const at::Tensor &input2,
                                      const at::Tensor &input3, bool left,
                                      bool transpose) {
  std::cout << "aten::ormqr" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, input2, input3};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::ormqr(mlirtens[0], mlirtens[1], mlirtens[2], left, transpose);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_lu_with_info(const at::Tensor &self, bool pivot,
                                   bool check_errors) {
  std::cout << "aten::_lu_with_info" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_lu_with_info(mlirtens[0], pivot, check_errors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::lu_solve_out(at::Tensor &out,
                                              const at::Tensor &self,
                                              const at::Tensor &LU_data,
                                              const at::Tensor &LU_pivots) {
  std::cout << "aten::lu_solve_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, LU_data, LU_pivots};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::lu_solve_out(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::lu_solve(const at::Tensor &self,
                                         const at::Tensor &LU_data,
                                         const at::Tensor &LU_pivots) {
  std::cout << "aten::lu_solve" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, LU_data, LU_pivots};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lu_solve(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_lu_solve_helper(const at::Tensor &self,
                                                 const at::Tensor &LU_data,
                                                 const at::Tensor &LU_pivots) {
  std::cout << "aten::_lu_solve_helper" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, LU_data, LU_pivots};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_lu_solve_helper(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::multinomial_out(at::Tensor &out,
                                                 const at::Tensor &self,
                                                 int64_t num_samples,
                                                 bool replacement,
                                                 at::Generator *generator) {
  std::cout << "aten::multinomial_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multinomial_out(mlirtens[0], mlirtens[1], num_samples,
                                        replacement, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::multinomial(const at::Tensor &self,
                                            int64_t num_samples,
                                            bool replacement,
                                            at::Generator *generator) {
  std::cout << "aten::multinomial" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::multinomial(mlirtens[0], num_samples, replacement, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_multinomial_alias_setup(const at::Tensor &probs) {
  std::cout << "aten::_multinomial_alias_setup" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {probs};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_multinomial_alias_setup(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(probs)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(probs)));
}

at::Tensor ATenMLIRTypeDefault::_multinomial_alias_draw(
    const at::Tensor &J, const at::Tensor &q, int64_t num_samples,
    at::Generator *generator) {
  std::cout << "aten::_multinomial_alias_draw" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {J, q};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_multinomial_alias_draw(mlirtens[0], mlirtens[1],
                                                num_samples, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(J));
}

at::Tensor &ATenMLIRTypeDefault::lgamma_out(at::Tensor &out,
                                            const at::Tensor &self) {
  std::cout << "aten::lgamma_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lgamma_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::lgamma(const at::Tensor &self) {
  std::cout << "aten::lgamma" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lgamma(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::digamma_out(at::Tensor &out,
                                             const at::Tensor &self) {
  std::cout << "aten::digamma_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::digamma_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::digamma(const at::Tensor &self) {
  std::cout << "aten::digamma" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::digamma(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::polygamma_out(at::Tensor &out, int64_t n,
                                               const at::Tensor &self) {
  std::cout << "aten::polygamma_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::polygamma_out(mlirtens[0], n, mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::polygamma(int64_t n, const at::Tensor &self) {
  std::cout << "aten::polygamma" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::polygamma(n, mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::erfinv(const at::Tensor &self) {
  std::cout << "aten::erfinv" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erfinv(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::erfinv_(at::Tensor &self) {
  std::cout << "aten::erfinv_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].erfinv_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::erfinv_out(at::Tensor &out,
                                            const at::Tensor &self) {
  std::cout << "aten::erfinv_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::erfinv_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::sign(const at::Tensor &self) {
  std::cout << "aten::sign" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sign(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::sign_(at::Tensor &self) {
  std::cout << "aten::sign_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].sign_();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::sign_out(at::Tensor &out,
                                          const at::Tensor &self) {
  std::cout << "aten::sign_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sign_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::dist(const at::Tensor &self,
                                     const at::Tensor &other, at::Scalar p) {
  std::cout << "aten::dist" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::dist(mlirtens[0], mlirtens[1], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::atan2_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           const at::Tensor &other) {
  std::cout << "aten::atan2_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::atan2_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::atan2(const at::Tensor &self,
                                      const at::Tensor &other) {
  std::cout << "aten::atan2" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::atan2(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::lerp_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          const at::Tensor &end,
                                          at::Scalar weight) {
  std::cout << "aten::lerp_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, end};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lerp_out(mlirtens[0], mlirtens[1], mlirtens[2], weight);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::lerp_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          const at::Tensor &end,
                                          const at::Tensor &weight) {
  std::cout << "aten::lerp_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, end, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::lerp_out(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::lerp(const at::Tensor &self,
                                     const at::Tensor &end, at::Scalar weight) {
  std::cout << "aten::lerp" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, end};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lerp(mlirtens[0], mlirtens[1], weight);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::lerp(const at::Tensor &self,
                                     const at::Tensor &end,
                                     const at::Tensor &weight) {
  std::cout << "aten::lerp" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, end, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::lerp(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::histc_out(at::Tensor &out,
                                           const at::Tensor &self, int64_t bins,
                                           at::Scalar min, at::Scalar max) {
  std::cout << "aten::histc_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::histc_out(mlirtens[0], mlirtens[1], bins, min, max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::histc(const at::Tensor &self, int64_t bins,
                                      at::Scalar min, at::Scalar max) {
  std::cout << "aten::histc" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::histc(mlirtens[0], bins, min, max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::fmod_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          at::Scalar other) {
  std::cout << "aten::fmod_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fmod_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::fmod(const at::Tensor &self, at::Scalar other) {
  std::cout << "aten::fmod" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fmod(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::fmod_out(at::Tensor &out,
                                          const at::Tensor &self,
                                          const at::Tensor &other) {
  std::cout << "aten::fmod_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fmod_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::fmod(const at::Tensor &self,
                                     const at::Tensor &other) {
  std::cout << "aten::fmod" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fmod(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::remainder_out(at::Tensor &out,
                                               const at::Tensor &self,
                                               at::Scalar other) {
  std::cout << "aten::remainder_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::remainder_out(mlirtens[0], mlirtens[1], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::remainder(const at::Tensor &self,
                                          at::Scalar other) {
  std::cout << "aten::remainder" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::remainder(mlirtens[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::remainder_out(at::Tensor &out,
                                               const at::Tensor &self,
                                               const at::Tensor &other) {
  std::cout << "aten::remainder_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::remainder_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::remainder(const at::Tensor &self,
                                          const at::Tensor &other) {
  std::cout << "aten::remainder" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::remainder(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::min_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &other) {
  std::cout << "aten::min_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::min_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::min(const at::Tensor &self,
                                    const at::Tensor &other) {
  std::cout << "aten::min" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::min(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::min(const at::Tensor &self) {
  std::cout << "aten::min" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::min(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::max_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &other) {
  std::cout << "aten::max_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::max(const at::Tensor &self,
                                    const at::Tensor &other) {
  std::cout << "aten::max" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::max(const at::Tensor &self) {
  std::cout << "aten::max" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::median(const at::Tensor &self) {
  std::cout << "aten::median" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::median(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::sort_out(at::Tensor &values, at::Tensor &indices,
                              const at::Tensor &self, int64_t dim,
                              bool descending) {
  std::cout << "aten::sort_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {values, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::sort_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, descending);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(values, indices);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::sort(const at::Tensor &self, int64_t dim,
                          bool descending) {
  std::cout << "aten::sort" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sort(mlirtens[0], dim, descending);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::argsort(const at::Tensor &self, int64_t dim,
                                        bool descending) {
  std::cout << "aten::argsort" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::argsort(mlirtens[0], dim, descending);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::topk_out(at::Tensor &values, at::Tensor &indices,
                              const at::Tensor &self, int64_t k, int64_t dim,
                              bool largest, bool sorted) {
  std::cout << "aten::topk_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {values, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::topk_out(mlirtens[0], mlirtens[1], mlirtens[2], k, dim,
                                 largest, sorted);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(values, indices);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::topk(const at::Tensor &self, int64_t k, int64_t dim,
                          bool largest, bool sorted) {
  std::cout << "aten::topk" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::topk(mlirtens[0], k, dim, largest, sorted);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor ATenMLIRTypeDefault::all(const at::Tensor &self) {
  std::cout << "aten::all" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::all(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::any(const at::Tensor &self) {
  std::cout << "aten::any" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::any(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::renorm_out(at::Tensor &out,
                                            const at::Tensor &self,
                                            at::Scalar p, int64_t dim,
                                            at::Scalar maxnorm) {
  std::cout << "aten::renorm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::renorm_out(mlirtens[0], mlirtens[1], p, dim, maxnorm);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::renorm(const at::Tensor &self, at::Scalar p,
                                       int64_t dim, at::Scalar maxnorm) {
  std::cout << "aten::renorm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::renorm(mlirtens[0], p, dim, maxnorm);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::unfold(const at::Tensor &self,
                                       int64_t dimension, int64_t size,
                                       int64_t step) {
  std::cout << "aten::unfold" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = mlirtens[0].unfold(dimension, size, step);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

bool ATenMLIRTypeDefault::equal(const at::Tensor &self,
                                const at::Tensor &other) {
  std::cout << "aten::equal" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, other};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::equal(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor &ATenMLIRTypeDefault::pow_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         const at::Tensor &exponent) {
  std::cout << "aten::pow_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, exponent};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pow_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::pow(const at::Tensor &self,
                                    const at::Tensor &exponent) {
  std::cout << "aten::pow" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, exponent};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pow(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::pow_out(at::Tensor &out, at::Scalar self,
                                         const at::Tensor &exponent) {
  std::cout << "aten::pow_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, exponent};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pow_out(mlirtens[0], self, mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::pow(at::Scalar self,
                                    const at::Tensor &exponent) {
  std::cout << "aten::pow" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {exponent};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::pow(self, mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(exponent));
}

at::Tensor &ATenMLIRTypeDefault::normal_out(at::Tensor &out,
                                            const at::Tensor &mean, double std,
                                            at::Generator *generator) {
  std::cout << "aten::normal_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, mean};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::normal_out(mlirtens[0], mlirtens[1], std, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::normal(const at::Tensor &mean, double std,
                                       at::Generator *generator) {
  std::cout << "aten::normal" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {mean};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::normal(mlirtens[0], std, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(mean));
}

at::Tensor &ATenMLIRTypeDefault::normal_out(at::Tensor &out, double mean,
                                            const at::Tensor &std,
                                            at::Generator *generator) {
  std::cout << "aten::normal_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, std};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::normal_out(mlirtens[0], mean, mlirtens[1], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::normal(double mean, const at::Tensor &std,
                                       at::Generator *generator) {
  std::cout << "aten::normal" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {std};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::normal(mean, mlirtens[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(std));
}

at::Tensor &ATenMLIRTypeDefault::normal_out(at::Tensor &out,
                                            const at::Tensor &mean,
                                            const at::Tensor &std,
                                            at::Generator *generator) {
  std::cout << "aten::normal_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, mean, std};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::normal_out(mlirtens[0], mlirtens[1], mlirtens[2], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::normal(const at::Tensor &mean,
                                       const at::Tensor &std,
                                       at::Generator *generator) {
  std::cout << "aten::normal" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {mean, std};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::normal(mlirtens[0], mlirtens[1], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(mean));
}

at::Tensor ATenMLIRTypeDefault::normal(double mean, double std,
                                       at::IntArrayRef size,
                                       at::Generator *generator,
                                       const at::TensorOptions &options) {
  std::cout << "aten::normal" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::normal(mean, std, size, generator, options);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(options));
}

at::Tensor &ATenMLIRTypeDefault::normal_out(at::Tensor &out, double mean,
                                            double std, at::IntArrayRef size,
                                            at::Generator *generator) {
  std::cout << "aten::normal_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::normal_out(mlirtens[0], mean, std, size, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::alias(const at::Tensor &self) {
  std::cout << "aten::alias" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::alias(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_addr(const at::Tensor &self,
                                      const at::Tensor &vec1,
                                      const at::Tensor &vec2, at::Scalar beta,
                                      at::Scalar alpha) {
  std::cout << "aten::_addr" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, vec1, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_addr(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::_addr_(at::Tensor &self,
                                        const at::Tensor &vec1,
                                        const at::Tensor &vec2, at::Scalar beta,
                                        at::Scalar alpha) {
  std::cout << "aten::_addr_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, vec1, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_addr_(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::_addr_out(at::Tensor &out,
                                           const at::Tensor &self,
                                           const at::Tensor &vec1,
                                           const at::Tensor &vec2,
                                           at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::_addr_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, vec1, vec2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_addr_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                  mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor &ATenMLIRTypeDefault::_index_copy_(at::Tensor &self, int64_t dim,
                                              const at::Tensor &index,
                                              const at::Tensor &source) {
  std::cout << "aten::_index_copy_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, index, source};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_index_copy_(mlirtens[0], dim, mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::_cumsum(const at::Tensor &self, int64_t dim) {
  std::cout << "aten::_cumsum" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cumsum(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::_cumsum_out(at::Tensor &out,
                                             const at::Tensor &self,
                                             int64_t dim) {
  std::cout << "aten::_cumsum_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cumsum_out(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::_cumprod(const at::Tensor &self, int64_t dim) {
  std::cout << "aten::_cumprod" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cumprod(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::_cumprod_out(at::Tensor &out,
                                              const at::Tensor &self,
                                              int64_t dim) {
  std::cout << "aten::_cumprod_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cumprod_out(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::_var(const at::Tensor &self, bool unbiased) {
  std::cout << "aten::_var" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_var(mlirtens[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_std(const at::Tensor &self, bool unbiased) {
  std::cout << "aten::_std" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_std(mlirtens[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::_addmm_out(at::Tensor &out,
                                            const at::Tensor &self,
                                            const at::Tensor &mat1,
                                            const at::Tensor &mat2,
                                            at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::_addmm_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_addmm_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                   mlirtens[3], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::_addmm(const at::Tensor &self,
                                       const at::Tensor &mat1,
                                       const at::Tensor &mat2, at::Scalar beta,
                                       at::Scalar alpha) {
  std::cout << "aten::_addmm" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_addmm(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::_addmm_(at::Tensor &self,
                                         const at::Tensor &mat1,
                                         const at::Tensor &mat2,
                                         at::Scalar beta, at::Scalar alpha) {
  std::cout << "aten::_addmm_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, mat1, mat2};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_addmm_(mlirtens[0], mlirtens[1], mlirtens[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor ATenMLIRTypeDefault::_cat(at::TensorList tensors, int64_t dim) {
  std::cout << "aten::_cat" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cat(tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(tensors));
}

at::Tensor &ATenMLIRTypeDefault::_cat_out(at::Tensor &out,
                                          at::TensorList tensors, int64_t dim) {
  std::cout << "aten::_cat_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_cat_out(mlirtens[0], tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_mode(const at::Tensor &self, int64_t dim, bool keepdim) {
  std::cout << "aten::_mode" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_mode(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::_mode_out(at::Tensor &values, at::Tensor &indices,
                               const at::Tensor &self, int64_t dim,
                               bool keepdim) {
  std::cout << "aten::_mode_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {values, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_mode_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(values, indices);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_max(const at::Tensor &self, int64_t dim, bool keepdim) {
  std::cout << "aten::_max" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_max(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::_max_out(at::Tensor &max, at::Tensor &max_indices,
                              const at::Tensor &self, int64_t dim,
                              bool keepdim) {
  std::cout << "aten::_max_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {max, max_indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_max_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(max, max_indices);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::_min(const at::Tensor &self, int64_t dim, bool keepdim) {
  std::cout << "aten::_min" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_min(mlirtens[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::_min_out(at::Tensor &min, at::Tensor &min_indices,
                              const at::Tensor &self, int64_t dim,
                              bool keepdim) {
  std::cout << "aten::_min_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {min, min_indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::_min_out(mlirtens[0], mlirtens[1], mlirtens[2], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(min, min_indices);
}

at::Tensor &ATenMLIRTypeDefault::binary_cross_entropy_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &target,
    const at::Tensor &weight, int64_t reduction) {
  std::cout << "aten::binary_cross_entropy_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::binary_cross_entropy_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::binary_cross_entropy(const at::Tensor &self,
                                                     const at::Tensor &target,
                                                     const at::Tensor &weight,
                                                     int64_t reduction) {
  std::cout << "aten::binary_cross_entropy" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::binary_cross_entropy(mlirtens[0], mlirtens[1],
                                             mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::binary_cross_entropy_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, const at::Tensor &weight,
    int64_t reduction) {
  std::cout << "aten::binary_cross_entropy_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::binary_cross_entropy_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::binary_cross_entropy_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, const at::Tensor &weight, int64_t reduction) {
  std::cout << "aten::binary_cross_entropy_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target,
                                              weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::binary_cross_entropy_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::mse_loss_out(at::Tensor &out,
                                              const at::Tensor &self,
                                              const at::Tensor &target,
                                              int64_t reduction) {
  std::cout << "aten::mse_loss_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::mse_loss_out(mlirtens[0], mlirtens[1], mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::mse_loss(const at::Tensor &self,
                                         const at::Tensor &target,
                                         int64_t reduction) {
  std::cout << "aten::mse_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mse_loss(mlirtens[0], mlirtens[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::mse_loss_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, int64_t reduction) {
  std::cout << "aten::mse_loss_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mse_loss_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::mse_loss_backward(const at::Tensor &grad_output,
                                                  const at::Tensor &self,
                                                  const at::Tensor &target,
                                                  int64_t reduction) {
  std::cout << "aten::mse_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::mse_loss_backward(mlirtens[0], mlirtens[1], mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::l1_loss_out(at::Tensor &out,
                                             const at::Tensor &self,
                                             const at::Tensor &target,
                                             int64_t reduction) {
  std::cout << "aten::l1_loss_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::l1_loss_out(mlirtens[0], mlirtens[1], mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::l1_loss(const at::Tensor &self,
                                        const at::Tensor &target,
                                        int64_t reduction) {
  std::cout << "aten::l1_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::l1_loss(mlirtens[0], mlirtens[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::l1_loss_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, int64_t reduction) {
  std::cout << "aten::l1_loss_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::l1_loss_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::l1_loss_backward(const at::Tensor &grad_output,
                                                 const at::Tensor &self,
                                                 const at::Tensor &target,
                                                 int64_t reduction) {
  std::cout << "aten::l1_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::l1_loss_backward(mlirtens[0], mlirtens[1], mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::multi_margin_loss_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &target,
    at::Scalar p, at::Scalar margin, const at::Tensor &weight,
    int64_t reduction) {
  std::cout << "aten::multi_margin_loss_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multi_margin_loss_out(
      mlirtens[0], mlirtens[1], mlirtens[2], p, margin, mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::multi_margin_loss(
    const at::Tensor &self, const at::Tensor &target, at::Scalar p,
    at::Scalar margin, const at::Tensor &weight, int64_t reduction) {
  std::cout << "aten::multi_margin_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multi_margin_loss(mlirtens[0], mlirtens[1], p, margin,
                                          mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::multi_margin_loss_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, at::Scalar p,
    at::Scalar margin, const at::Tensor &weight, int64_t reduction) {
  std::cout << "aten::multi_margin_loss_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multi_margin_loss_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], p, margin,
      mlirtens[4], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::multi_margin_loss_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, at::Scalar p, at::Scalar margin,
    const at::Tensor &weight, int64_t reduction) {
  std::cout << "aten::multi_margin_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target,
                                              weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multi_margin_loss_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], p, margin, mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::multilabel_margin_loss_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &target,
    int64_t reduction) {
  std::cout << "aten::multilabel_margin_loss_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multilabel_margin_loss_out(mlirtens[0], mlirtens[1],
                                                   mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::multilabel_margin_loss(const at::Tensor &self,
                                                       const at::Tensor &target,
                                                       int64_t reduction) {
  std::cout << "aten::multilabel_margin_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::multilabel_margin_loss(mlirtens[0], mlirtens[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::multilabel_margin_loss_forward_out(
    at::Tensor &output, at::Tensor &is_target, const at::Tensor &self,
    const at::Tensor &target, int64_t reduction) {
  std::cout << "aten::multilabel_margin_loss_forward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, is_target, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multilabel_margin_loss_forward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(output, is_target);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::multilabel_margin_loss_forward(const at::Tensor &self,
                                                    const at::Tensor &target,
                                                    int64_t reduction) {
  std::cout << "aten::multilabel_margin_loss_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::multilabel_margin_loss_forward(mlirtens[0], mlirtens[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::multilabel_margin_loss_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, int64_t reduction,
    const at::Tensor &is_target) {
  std::cout << "aten::multilabel_margin_loss_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              target, is_target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multilabel_margin_loss_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction,
      mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::multilabel_margin_loss_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, int64_t reduction, const at::Tensor &is_target) {
  std::cout << "aten::multilabel_margin_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target,
                                              is_target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::multilabel_margin_loss_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], reduction, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::nll_loss_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &target,
    const at::Tensor &weight, int64_t reduction, int64_t ignore_index) {
  std::cout << "aten::nll_loss_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                     mlirtens[3], reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::nll_loss(const at::Tensor &self,
                                         const at::Tensor &target,
                                         const at::Tensor &weight,
                                         int64_t reduction,
                                         int64_t ignore_index) {
  std::cout << "aten::nll_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss(mlirtens[0], mlirtens[1], mlirtens[2],
                                 reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::nll_loss_forward_out(
    at::Tensor &output, at::Tensor &total_weight, const at::Tensor &self,
    const at::Tensor &target, const at::Tensor &weight, int64_t reduction,
    int64_t ignore_index) {
  std::cout << "aten::nll_loss_forward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, total_weight, self,
                                              target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss_forward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(output, total_weight);
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::nll_loss_forward(
    const at::Tensor &self, const at::Tensor &target, const at::Tensor &weight,
    int64_t reduction, int64_t ignore_index) {
  std::cout << "aten::nll_loss_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss_forward(mlirtens[0], mlirtens[1], mlirtens[2],
                                         reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::nll_loss_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, const at::Tensor &weight,
    int64_t reduction, int64_t ignore_index, const at::Tensor &total_weight) {
  std::cout << "aten::nll_loss_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_input, grad_output, self, target, weight, total_weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      reduction, ignore_index, mlirtens[5]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::nll_loss_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, const at::Tensor &weight, int64_t reduction,
    int64_t ignore_index, const at::Tensor &total_weight) {
  std::cout << "aten::nll_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target, weight,
                                              total_weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::nll_loss_backward(mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3],
                            reduction, ignore_index, mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::nll_loss2d_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &target,
    const at::Tensor &weight, int64_t reduction, int64_t ignore_index) {
  std::cout << "aten::nll_loss2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss2d_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                       mlirtens[3], reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::nll_loss2d(const at::Tensor &self,
                                           const at::Tensor &target,
                                           const at::Tensor &weight,
                                           int64_t reduction,
                                           int64_t ignore_index) {
  std::cout << "aten::nll_loss2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss2d(mlirtens[0], mlirtens[1], mlirtens[2],
                                   reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::nll_loss2d_forward_out(
    at::Tensor &output, at::Tensor &total_weight, const at::Tensor &self,
    const at::Tensor &target, const at::Tensor &weight, int64_t reduction,
    int64_t ignore_index) {
  std::cout << "aten::nll_loss2d_forward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, total_weight, self,
                                              target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss2d_forward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(output, total_weight);
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::nll_loss2d_forward(
    const at::Tensor &self, const at::Tensor &target, const at::Tensor &weight,
    int64_t reduction, int64_t ignore_index) {
  std::cout << "aten::nll_loss2d_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss2d_forward(
      mlirtens[0], mlirtens[1], mlirtens[2], reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::nll_loss2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, const at::Tensor &weight,
    int64_t reduction, int64_t ignore_index, const at::Tensor &total_weight) {
  std::cout << "aten::nll_loss2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_input, grad_output, self, target, weight, total_weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      reduction, ignore_index, mlirtens[5]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::nll_loss2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, const at::Tensor &weight, int64_t reduction,
    int64_t ignore_index, const at::Tensor &total_weight) {
  std::cout << "aten::nll_loss2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target, weight,
                                              total_weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::nll_loss2d_backward(mlirtens[0], mlirtens[1],
                                            mlirtens[2], mlirtens[3], reduction,
                                            ignore_index, mlirtens[4]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::smooth_l1_loss_out(at::Tensor &out,
                                                    const at::Tensor &self,
                                                    const at::Tensor &target,
                                                    int64_t reduction) {
  std::cout << "aten::smooth_l1_loss_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::smooth_l1_loss_out(mlirtens[0], mlirtens[1], mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::smooth_l1_loss(const at::Tensor &self,
                                               const at::Tensor &target,
                                               int64_t reduction) {
  std::cout << "aten::smooth_l1_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::smooth_l1_loss(mlirtens[0], mlirtens[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::smooth_l1_loss_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, int64_t reduction) {
  std::cout << "aten::smooth_l1_loss_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::smooth_l1_loss_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::smooth_l1_loss_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, int64_t reduction) {
  std::cout << "aten::smooth_l1_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::smooth_l1_loss_backward(mlirtens[0], mlirtens[1],
                                                mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::soft_margin_loss_out(at::Tensor &out,
                                                      const at::Tensor &self,
                                                      const at::Tensor &target,
                                                      int64_t reduction) {
  std::cout << "aten::soft_margin_loss_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::soft_margin_loss_out(mlirtens[0], mlirtens[1],
                                             mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::soft_margin_loss(const at::Tensor &self,
                                                 const at::Tensor &target,
                                                 int64_t reduction) {
  std::cout << "aten::soft_margin_loss" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::soft_margin_loss(mlirtens[0], mlirtens[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::soft_margin_loss_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &target, int64_t reduction) {
  std::cout << "aten::soft_margin_loss_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::soft_margin_loss_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::soft_margin_loss_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &target, int64_t reduction) {
  std::cout << "aten::soft_margin_loss_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, target};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::soft_margin_loss_backward(mlirtens[0], mlirtens[1],
                                                  mlirtens[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::elu_out(at::Tensor &out,
                                         const at::Tensor &self,
                                         at::Scalar alpha, at::Scalar scale,
                                         at::Scalar input_scale) {
  std::cout << "aten::elu_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::elu_out(mlirtens[0], mlirtens[1], alpha, scale, input_scale);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::elu(const at::Tensor &self, at::Scalar alpha,
                                    at::Scalar scale, at::Scalar input_scale) {
  std::cout << "aten::elu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::elu(mlirtens[0], alpha, scale, input_scale);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::elu_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output, at::Scalar alpha,
    at::Scalar scale, at::Scalar input_scale, const at::Tensor &output) {
  std::cout << "aten::elu_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::elu_backward_out(mlirtens[0], mlirtens[1], alpha, scale,
                                         input_scale, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::elu_backward(const at::Tensor &grad_output,
                                             at::Scalar alpha, at::Scalar scale,
                                             at::Scalar input_scale,
                                             const at::Tensor &output) {
  std::cout << "aten::elu_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::elu_backward(mlirtens[0], alpha, scale, input_scale, mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::elu_(at::Tensor &self, at::Scalar alpha,
                                      at::Scalar scale,
                                      at::Scalar input_scale) {
  std::cout << "aten::elu_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::elu_(mlirtens[0], alpha, scale, input_scale);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::glu_out(at::Tensor &out,
                                         const at::Tensor &self, int64_t dim) {
  std::cout << "aten::glu_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::glu_out(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::glu(const at::Tensor &self, int64_t dim) {
  std::cout << "aten::glu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::glu(mlirtens[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::glu_backward_out(at::Tensor &grad_input,
                                                  const at::Tensor &grad_output,
                                                  const at::Tensor &self,
                                                  int64_t dim) {
  std::cout << "aten::glu_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::glu_backward_out(mlirtens[0], mlirtens[1], mlirtens[2], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::glu_backward(const at::Tensor &grad_output,
                                             const at::Tensor &self,
                                             int64_t dim) {
  std::cout << "aten::glu_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::glu_backward(mlirtens[0], mlirtens[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::hardtanh_out(at::Tensor &out,
                                              const at::Tensor &self,
                                              at::Scalar min_val,
                                              at::Scalar max_val) {
  std::cout << "aten::hardtanh_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::hardtanh_out(mlirtens[0], mlirtens[1], min_val, max_val);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::hardtanh(const at::Tensor &self,
                                         at::Scalar min_val,
                                         at::Scalar max_val) {
  std::cout << "aten::hardtanh" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hardtanh(mlirtens[0], min_val, max_val);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::hardtanh_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::Scalar min_val, at::Scalar max_val) {
  std::cout << "aten::hardtanh_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hardtanh_backward_out(mlirtens[0], mlirtens[1],
                                              mlirtens[2], min_val, max_val);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::hardtanh_backward(const at::Tensor &grad_output,
                                                  const at::Tensor &self,
                                                  at::Scalar min_val,
                                                  at::Scalar max_val) {
  std::cout << "aten::hardtanh_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::hardtanh_backward(mlirtens[0], mlirtens[1], min_val, max_val);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::hardtanh_(at::Tensor &self, at::Scalar min_val,
                                           at::Scalar max_val) {
  std::cout << "aten::hardtanh_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::hardtanh_(mlirtens[0], min_val, max_val);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::leaky_relu_out(at::Tensor &out,
                                                const at::Tensor &self,
                                                at::Scalar negative_slope) {
  std::cout << "aten::leaky_relu_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::leaky_relu_out(mlirtens[0], mlirtens[1], negative_slope);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::leaky_relu(const at::Tensor &self,
                                           at::Scalar negative_slope) {
  std::cout << "aten::leaky_relu" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::leaky_relu(mlirtens[0], negative_slope);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::leaky_relu_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::Scalar negative_slope) {
  std::cout << "aten::leaky_relu_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::leaky_relu_backward_out(mlirtens[0], mlirtens[1],
                                                mlirtens[2], negative_slope);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::leaky_relu_backward(const at::Tensor &grad_output,
                                         const at::Tensor &self,
                                         at::Scalar negative_slope) {
  std::cout << "aten::leaky_relu_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::leaky_relu_backward(mlirtens[0], mlirtens[1], negative_slope);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::leaky_relu_(at::Tensor &self,
                                             at::Scalar negative_slope) {
  std::cout << "aten::leaky_relu_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::leaky_relu_(mlirtens[0], negative_slope);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::log_sigmoid_out(at::Tensor &out,
                                                 const at::Tensor &self) {
  std::cout << "aten::log_sigmoid_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log_sigmoid_out(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::log_sigmoid(const at::Tensor &self) {
  std::cout << "aten::log_sigmoid" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log_sigmoid(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::log_sigmoid_forward_out(at::Tensor &output,
                                             at::Tensor &buffer,
                                             const at::Tensor &self) {
  std::cout << "aten::log_sigmoid_forward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, buffer, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::log_sigmoid_forward_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(output, buffer);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::log_sigmoid_forward(const at::Tensor &self) {
  std::cout << "aten::log_sigmoid_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log_sigmoid_forward(mlirtens[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::log_sigmoid_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &buffer) {
  std::cout << "aten::log_sigmoid_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              buffer};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::log_sigmoid_backward_out(mlirtens[0], mlirtens[1],
                                                 mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::log_sigmoid_backward(const at::Tensor &grad_output,
                                          const at::Tensor &self,
                                          const at::Tensor &buffer) {
  std::cout << "aten::log_sigmoid_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, buffer};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::log_sigmoid_backward(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::rrelu_with_noise_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &noise,
    at::Scalar lower, at::Scalar upper, bool training,
    at::Generator *generator) {
  std::cout << "aten::rrelu_with_noise_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, noise};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rrelu_with_noise_out(
      mlirtens[0], mlirtens[1], mlirtens[2], lower, upper, training, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::rrelu_with_noise(
    const at::Tensor &self, const at::Tensor &noise, at::Scalar lower,
    at::Scalar upper, bool training, at::Generator *generator) {
  std::cout << "aten::rrelu_with_noise" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, noise};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rrelu_with_noise(mlirtens[0], mlirtens[1], lower, upper,
                                         training, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::rrelu_with_noise_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &noise, at::Scalar lower,
    at::Scalar upper, bool training) {
  std::cout << "aten::rrelu_with_noise_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              noise};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::rrelu_with_noise_backward_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                        mlirtens[3], lower, upper, training);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::rrelu_with_noise_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &noise, at::Scalar lower, at::Scalar upper,
    bool training) {
  std::cout << "aten::rrelu_with_noise_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, noise};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rrelu_with_noise_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], lower, upper, training);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::rrelu_with_noise_(
    at::Tensor &self, const at::Tensor &noise, at::Scalar lower,
    at::Scalar upper, bool training, at::Generator *generator) {
  std::cout << "aten::rrelu_with_noise_" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, noise};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::rrelu_with_noise_(mlirtens[0], mlirtens[1], lower,
                                          upper, training, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor &ATenMLIRTypeDefault::softplus_out(at::Tensor &out,
                                              const at::Tensor &self,
                                              at::Scalar beta,
                                              at::Scalar threshold) {
  std::cout << "aten::softplus_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softplus_out(mlirtens[0], mlirtens[1], beta, threshold);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::softplus(const at::Tensor &self,
                                         at::Scalar beta,
                                         at::Scalar threshold) {
  std::cout << "aten::softplus" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softplus(mlirtens[0], beta, threshold);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::softplus_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::Scalar beta, at::Scalar threshold,
    const at::Tensor &output) {
  std::cout << "aten::softplus_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softplus_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], beta, threshold, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::softplus_backward(const at::Tensor &grad_output,
                                                  const at::Tensor &self,
                                                  at::Scalar beta,
                                                  at::Scalar threshold,
                                                  const at::Tensor &output) {
  std::cout << "aten::softplus_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softplus_backward(mlirtens[0], mlirtens[1], beta,
                                          threshold, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::softshrink_out(at::Tensor &out,
                                                const at::Tensor &self,
                                                at::Scalar lambd) {
  std::cout << "aten::softshrink_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softshrink_out(mlirtens[0], mlirtens[1], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::softshrink(const at::Tensor &self,
                                           at::Scalar lambd) {
  std::cout << "aten::softshrink" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softshrink(mlirtens[0], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::softshrink_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::Scalar lambd) {
  std::cout << "aten::softshrink_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::softshrink_backward_out(mlirtens[0], mlirtens[1], mlirtens[2], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::softshrink_backward(
    const at::Tensor &grad_output, const at::Tensor &self, at::Scalar lambd) {
  std::cout << "aten::softshrink_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::softshrink_backward(mlirtens[0], mlirtens[1], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::adaptive_avg_pool2d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_avg_pool2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::adaptive_avg_pool2d_out(mlirtens[0], mlirtens[1], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::adaptive_avg_pool2d(const at::Tensor &self,
                                         at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_avg_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_avg_pool2d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor
ATenMLIRTypeDefault::mkldnn_adaptive_avg_pool2d(const at::Tensor &self,
                                                at::IntArrayRef output_size) {
  std::cout << "aten::mkldnn_adaptive_avg_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::mkldnn_adaptive_avg_pool2d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor
ATenMLIRTypeDefault::_adaptive_avg_pool2d(const at::Tensor &self,
                                          at::IntArrayRef output_size) {
  std::cout << "aten::_adaptive_avg_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_adaptive_avg_pool2d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor ATenMLIRTypeDefault::_adaptive_avg_pool2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self) {
  std::cout << "aten::_adaptive_avg_pool2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::_adaptive_avg_pool2d_backward(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::adaptive_avg_pool3d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_avg_pool3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::adaptive_avg_pool3d_out(mlirtens[0], mlirtens[1], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::adaptive_avg_pool3d(const at::Tensor &self,
                                         at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_avg_pool3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_avg_pool3d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::adaptive_avg_pool3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self) {
  std::cout << "aten::adaptive_avg_pool3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_avg_pool3d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::adaptive_avg_pool3d_backward(const at::Tensor &grad_output,
                                                  const at::Tensor &self) {
  std::cout << "aten::adaptive_avg_pool3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_avg_pool3d_backward(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::adaptive_max_pool2d_out(at::Tensor &out,
                                             at::Tensor &indices,
                                             const at::Tensor &self,
                                             at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_max_pool2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_max_pool2d_out(mlirtens[0], mlirtens[1],
                                                mlirtens[2], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(out, indices);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::adaptive_max_pool2d(const at::Tensor &self,
                                         at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_max_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_max_pool2d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::adaptive_max_pool2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &indices) {
  std::cout << "aten::adaptive_max_pool2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_max_pool2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::adaptive_max_pool2d_backward(const at::Tensor &grad_output,
                                                  const at::Tensor &self,
                                                  const at::Tensor &indices) {
  std::cout << "aten::adaptive_max_pool2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::adaptive_max_pool2d_backward(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::adaptive_max_pool3d_out(at::Tensor &out,
                                             at::Tensor &indices,
                                             const at::Tensor &self,
                                             at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_max_pool3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_max_pool3d_out(mlirtens[0], mlirtens[1],
                                                mlirtens[2], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(out, indices);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::adaptive_max_pool3d(const at::Tensor &self,
                                         at::IntArrayRef output_size) {
  std::cout << "aten::adaptive_max_pool3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_max_pool3d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::adaptive_max_pool3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &indices) {
  std::cout << "aten::adaptive_max_pool3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::adaptive_max_pool3d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::adaptive_max_pool3d_backward(const at::Tensor &grad_output,
                                                  const at::Tensor &self,
                                                  const at::Tensor &indices) {
  std::cout << "aten::adaptive_max_pool3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::adaptive_max_pool3d_backward(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::avg_pool2d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode,
    bool count_include_pad, c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::avg_pool2d_out(mlirtens[0], mlirtens[1], kernel_size, stride, padding,
                         ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::avg_pool2d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, bool ceil_mode, bool count_include_pad,
    c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::avg_pool2d(mlirtens[0], kernel_size, stride, padding, ceil_mode,
                     count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::avg_pool2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, bool ceil_mode, bool count_include_pad,
    c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::avg_pool2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::avg_pool2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, bool ceil_mode, bool count_include_pad,
    c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::avg_pool2d_backward(
      mlirtens[0], mlirtens[1], kernel_size, stride, padding, ceil_mode,
      count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::avg_pool3d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode,
    bool count_include_pad, c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::avg_pool3d_out(mlirtens[0], mlirtens[1], kernel_size, stride, padding,
                         ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::avg_pool3d(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, bool ceil_mode, bool count_include_pad,
    c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::avg_pool3d(mlirtens[0], kernel_size, stride, padding, ceil_mode,
                     count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::avg_pool3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, bool ceil_mode, bool count_include_pad,
    c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::avg_pool3d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::avg_pool3d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, bool ceil_mode, bool count_include_pad,
    c10::optional<int64_t> divisor_override) {
  std::cout << "aten::avg_pool3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::avg_pool3d_backward(
      mlirtens[0], mlirtens[1], kernel_size, stride, padding, ceil_mode,
      count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::fractional_max_pool2d_out(
    at::Tensor &output, at::Tensor &indices, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef output_size,
    const at::Tensor &random_samples) {
  std::cout << "aten::fractional_max_pool2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, indices, self,
                                              random_samples};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::fractional_max_pool2d_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                    kernel_size, output_size, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(output, indices);
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::fractional_max_pool2d(
    const at::Tensor &self, at::IntArrayRef kernel_size,
    at::IntArrayRef output_size, const at::Tensor &random_samples) {
  std::cout << "aten::fractional_max_pool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, random_samples};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fractional_max_pool2d(mlirtens[0], kernel_size,
                                              output_size, mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::fractional_max_pool2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef kernel_size,
    at::IntArrayRef output_size, const at::Tensor &indices) {
  std::cout << "aten::fractional_max_pool2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fractional_max_pool2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, output_size,
      mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::fractional_max_pool2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef output_size,
    const at::Tensor &indices) {
  std::cout << "aten::fractional_max_pool2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fractional_max_pool2d_backward(
      mlirtens[0], mlirtens[1], kernel_size, output_size, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::fractional_max_pool3d_out(
    at::Tensor &output, at::Tensor &indices, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef output_size,
    const at::Tensor &random_samples) {
  std::cout << "aten::fractional_max_pool3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, indices, self,
                                              random_samples};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::fractional_max_pool3d_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                    kernel_size, output_size, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(output, indices);
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::fractional_max_pool3d(
    const at::Tensor &self, at::IntArrayRef kernel_size,
    at::IntArrayRef output_size, const at::Tensor &random_samples) {
  std::cout << "aten::fractional_max_pool3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, random_samples};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fractional_max_pool3d(mlirtens[0], kernel_size,
                                              output_size, mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::fractional_max_pool3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef kernel_size,
    at::IntArrayRef output_size, const at::Tensor &indices) {
  std::cout << "aten::fractional_max_pool3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fractional_max_pool3d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, output_size,
      mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::fractional_max_pool3d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef output_size,
    const at::Tensor &indices) {
  std::cout << "aten::fractional_max_pool3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::fractional_max_pool3d_backward(
      mlirtens[0], mlirtens[1], kernel_size, output_size, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::max_pool2d_with_indices_out(
    at::Tensor &out, at::Tensor &indices, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool2d_with_indices_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool2d_with_indices_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(out, indices);
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::max_pool2d_with_indices(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool2d_with_indices" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool2d_with_indices(
      mlirtens[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::max_pool2d_with_indices_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode,
    const at::Tensor &indices) {
  std::cout << "aten::max_pool2d_with_indices_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool2d_with_indices_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      dilation, ceil_mode, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::max_pool2d_with_indices_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode,
    const at::Tensor &indices) {
  std::cout << "aten::max_pool2d_with_indices_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool2d_with_indices_backward(
      mlirtens[0], mlirtens[1], kernel_size, stride, padding, dilation,
      ceil_mode, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::max_pool3d_with_indices_out(
    at::Tensor &out, at::Tensor &indices, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool3d_with_indices_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, indices, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool3d_with_indices_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(out, indices);
}

std::tuple<at::Tensor, at::Tensor> ATenMLIRTypeDefault::max_pool3d_with_indices(
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::cout << "aten::max_pool3d_with_indices" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool3d_with_indices(
      mlirtens[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)));
}

at::Tensor &ATenMLIRTypeDefault::max_pool3d_with_indices_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode,
    const at::Tensor &indices) {
  std::cout << "aten::max_pool3d_with_indices_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool3d_with_indices_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      dilation, ceil_mode, mlirtens[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::max_pool3d_with_indices_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    at::IntArrayRef kernel_size, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode,
    const at::Tensor &indices) {
  std::cout << "aten::max_pool3d_with_indices_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_pool3d_with_indices_backward(
      mlirtens[0], mlirtens[1], kernel_size, stride, padding, dilation,
      ceil_mode, mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::max_unpool2d_out(at::Tensor &out,
                                                  const at::Tensor &self,
                                                  const at::Tensor &indices,
                                                  at::IntArrayRef output_size) {
  std::cout << "aten::max_unpool2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::max_unpool2d_out(mlirtens[0], mlirtens[1], mlirtens[2], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::max_unpool2d(const at::Tensor &self,
                                             const at::Tensor &indices,
                                             at::IntArrayRef output_size) {
  std::cout << "aten::max_unpool2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_unpool2d(mlirtens[0], mlirtens[1], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::max_unpool2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &indices,
    at::IntArrayRef output_size) {
  std::cout << "aten::max_unpool2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_unpool2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::max_unpool2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &indices, at::IntArrayRef output_size) {
  std::cout << "aten::max_unpool2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_unpool2d_backward(mlirtens[0], mlirtens[1],
                                              mlirtens[2], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::max_unpool3d_out(at::Tensor &out,
                                                  const at::Tensor &self,
                                                  const at::Tensor &indices,
                                                  at::IntArrayRef output_size,
                                                  at::IntArrayRef stride,
                                                  at::IntArrayRef padding) {
  std::cout << "aten::max_unpool3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_unpool3d_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                         output_size, stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::max_unpool3d(const at::Tensor &self,
                                             const at::Tensor &indices,
                                             at::IntArrayRef output_size,
                                             at::IntArrayRef stride,
                                             at::IntArrayRef padding) {
  std::cout << "aten::max_unpool3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::max_unpool3d(mlirtens[0], mlirtens[1], output_size, stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::max_unpool3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, const at::Tensor &indices,
    at::IntArrayRef output_size, at::IntArrayRef stride,
    at::IntArrayRef padding) {
  std::cout << "aten::max_unpool3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self,
                                              indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::max_unpool3d_backward_out(mlirtens[0], mlirtens[1], mlirtens[2],
                                    mlirtens[3], output_size, stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::max_unpool3d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &indices, at::IntArrayRef output_size,
    at::IntArrayRef stride, at::IntArrayRef padding) {
  std::cout << "aten::max_unpool3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, indices};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::max_unpool3d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], output_size, stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::reflection_pad1d_out(at::Tensor &out,
                                                      const at::Tensor &self,
                                                      at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad1d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reflection_pad1d_out(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::reflection_pad1d(const at::Tensor &self,
                                                 at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reflection_pad1d(mlirtens[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::reflection_pad1d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad1d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reflection_pad1d_backward_out(mlirtens[0], mlirtens[1],
                                                      mlirtens[2], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::reflection_pad1d_backward(const at::Tensor &grad_output,
                                               const at::Tensor &self,
                                               at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad1d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::reflection_pad1d_backward(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::reflection_pad2d_out(at::Tensor &out,
                                                      const at::Tensor &self,
                                                      at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reflection_pad2d_out(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::reflection_pad2d(const at::Tensor &self,
                                                 at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reflection_pad2d(mlirtens[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::reflection_pad2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::reflection_pad2d_backward_out(mlirtens[0], mlirtens[1],
                                                      mlirtens[2], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::reflection_pad2d_backward(const at::Tensor &grad_output,
                                               const at::Tensor &self,
                                               at::IntArrayRef padding) {
  std::cout << "aten::reflection_pad2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::reflection_pad2d_backward(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::replication_pad1d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::replication_pad1d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::replication_pad1d_out(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::replication_pad1d(const at::Tensor &self,
                                                  at::IntArrayRef padding) {
  std::cout << "aten::replication_pad1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::replication_pad1d(mlirtens[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::replication_pad1d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::replication_pad1d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::replication_pad1d_backward_out(mlirtens[0], mlirtens[1],
                                                       mlirtens[2], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::replication_pad1d_backward(const at::Tensor &grad_output,
                                                const at::Tensor &self,
                                                at::IntArrayRef padding) {
  std::cout << "aten::replication_pad1d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::replication_pad1d_backward(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::replication_pad2d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::replication_pad2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::replication_pad2d_out(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::replication_pad2d(const at::Tensor &self,
                                                  at::IntArrayRef padding) {
  std::cout << "aten::replication_pad2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::replication_pad2d(mlirtens[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::replication_pad2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::replication_pad2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::replication_pad2d_backward_out(mlirtens[0], mlirtens[1],
                                                       mlirtens[2], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::replication_pad2d_backward(const at::Tensor &grad_output,
                                                const at::Tensor &self,
                                                at::IntArrayRef padding) {
  std::cout << "aten::replication_pad2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::replication_pad2d_backward(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::replication_pad3d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::replication_pad3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::replication_pad3d_out(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::replication_pad3d(const at::Tensor &self,
                                                  at::IntArrayRef padding) {
  std::cout << "aten::replication_pad3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::replication_pad3d(mlirtens[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::replication_pad3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    const at::Tensor &self, at::IntArrayRef padding) {
  std::cout << "aten::replication_pad3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::replication_pad3d_backward_out(mlirtens[0], mlirtens[1],
                                                       mlirtens[2], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::replication_pad3d_backward(const at::Tensor &grad_output,
                                                const at::Tensor &self,
                                                at::IntArrayRef padding) {
  std::cout << "aten::replication_pad3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::replication_pad3d_backward(mlirtens[0], mlirtens[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::upsample_linear1d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size,
    bool align_corners) {
  std::cout << "aten::upsample_linear1d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_linear1d_out(mlirtens[0], mlirtens[1],
                                              output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::upsample_linear1d(const at::Tensor &self,
                                                  at::IntArrayRef output_size,
                                                  bool align_corners) {
  std::cout << "aten::upsample_linear1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_linear1d(mlirtens[0], output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::upsample_linear1d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef output_size, at::IntArrayRef input_size,
    bool align_corners) {
  std::cout << "aten::upsample_linear1d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_linear1d_backward_out(
      mlirtens[0], mlirtens[1], output_size, input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::upsample_linear1d_backward(
    const at::Tensor &grad_output, at::IntArrayRef output_size,
    at::IntArrayRef input_size, bool align_corners) {
  std::cout << "aten::upsample_linear1d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_linear1d_backward(mlirtens[0], output_size,
                                                   input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::upsample_bilinear2d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size,
    bool align_corners) {
  std::cout << "aten::upsample_bilinear2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_bilinear2d_out(mlirtens[0], mlirtens[1],
                                                output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::upsample_bilinear2d(const at::Tensor &self,
                                                    at::IntArrayRef output_size,
                                                    bool align_corners) {
  std::cout << "aten::upsample_bilinear2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_bilinear2d(mlirtens[0], output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::upsample_bilinear2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef output_size, at::IntArrayRef input_size,
    bool align_corners) {
  std::cout << "aten::upsample_bilinear2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_bilinear2d_backward_out(
      mlirtens[0], mlirtens[1], output_size, input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::upsample_bilinear2d_backward(
    const at::Tensor &grad_output, at::IntArrayRef output_size,
    at::IntArrayRef input_size, bool align_corners) {
  std::cout << "aten::upsample_bilinear2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_bilinear2d_backward(mlirtens[0], output_size,
                                                     input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::upsample_bicubic2d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size,
    bool align_corners) {
  std::cout << "aten::upsample_bicubic2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_bicubic2d_out(mlirtens[0], mlirtens[1],
                                               output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::upsample_bicubic2d(const at::Tensor &self,
                                                   at::IntArrayRef output_size,
                                                   bool align_corners) {
  std::cout << "aten::upsample_bicubic2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_bicubic2d(mlirtens[0], output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::upsample_bicubic2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef output_size, at::IntArrayRef input_size,
    bool align_corners) {
  std::cout << "aten::upsample_bicubic2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_bicubic2d_backward_out(
      mlirtens[0], mlirtens[1], output_size, input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::upsample_bicubic2d_backward(
    const at::Tensor &grad_output, at::IntArrayRef output_size,
    at::IntArrayRef input_size, bool align_corners) {
  std::cout << "aten::upsample_bicubic2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_bicubic2d_backward(mlirtens[0], output_size,
                                                    input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::upsample_trilinear3d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size,
    bool align_corners) {
  std::cout << "aten::upsample_trilinear3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_trilinear3d_out(mlirtens[0], mlirtens[1],
                                                 output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::upsample_trilinear3d(
    const at::Tensor &self, at::IntArrayRef output_size, bool align_corners) {
  std::cout << "aten::upsample_trilinear3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_trilinear3d(mlirtens[0], output_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::upsample_trilinear3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef output_size, at::IntArrayRef input_size,
    bool align_corners) {
  std::cout << "aten::upsample_trilinear3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_trilinear3d_backward_out(
      mlirtens[0], mlirtens[1], output_size, input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::upsample_trilinear3d_backward(
    const at::Tensor &grad_output, at::IntArrayRef output_size,
    at::IntArrayRef input_size, bool align_corners) {
  std::cout << "aten::upsample_trilinear3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_trilinear3d_backward(
      mlirtens[0], output_size, input_size, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::upsample_nearest1d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size) {
  std::cout << "aten::upsample_nearest1d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_nearest1d_out(mlirtens[0], mlirtens[1], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::upsample_nearest1d(const at::Tensor &self,
                                        at::IntArrayRef output_size) {
  std::cout << "aten::upsample_nearest1d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_nearest1d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::upsample_nearest1d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef output_size, at::IntArrayRef input_size) {
  std::cout << "aten::upsample_nearest1d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_nearest1d_backward_out(
      mlirtens[0], mlirtens[1], output_size, input_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::upsample_nearest1d_backward(const at::Tensor &grad_output,
                                                 at::IntArrayRef output_size,
                                                 at::IntArrayRef input_size) {
  std::cout << "aten::upsample_nearest1d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_nearest1d_backward(mlirtens[0], output_size, input_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::upsample_nearest2d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size) {
  std::cout << "aten::upsample_nearest2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_nearest2d_out(mlirtens[0], mlirtens[1], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::upsample_nearest2d(const at::Tensor &self,
                                        at::IntArrayRef output_size) {
  std::cout << "aten::upsample_nearest2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_nearest2d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::upsample_nearest2d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef output_size, at::IntArrayRef input_size) {
  std::cout << "aten::upsample_nearest2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_nearest2d_backward_out(
      mlirtens[0], mlirtens[1], output_size, input_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::upsample_nearest2d_backward(const at::Tensor &grad_output,
                                                 at::IntArrayRef output_size,
                                                 at::IntArrayRef input_size) {
  std::cout << "aten::upsample_nearest2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_nearest2d_backward(mlirtens[0], output_size, input_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::upsample_nearest3d_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size) {
  std::cout << "aten::upsample_nearest3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_nearest3d_out(mlirtens[0], mlirtens[1], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor
ATenMLIRTypeDefault::upsample_nearest3d(const at::Tensor &self,
                                        at::IntArrayRef output_size) {
  std::cout << "aten::upsample_nearest3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_nearest3d(mlirtens[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::upsample_nearest3d_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef output_size, at::IntArrayRef input_size) {
  std::cout << "aten::upsample_nearest3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::upsample_nearest3d_backward_out(
      mlirtens[0], mlirtens[1], output_size, input_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor
ATenMLIRTypeDefault::upsample_nearest3d_backward(const at::Tensor &grad_output,
                                                 at::IntArrayRef output_size,
                                                 at::IntArrayRef input_size) {
  std::cout << "aten::upsample_nearest3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::upsample_nearest3d_backward(mlirtens[0], output_size, input_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &
ATenMLIRTypeDefault::sigmoid_backward_out(at::Tensor &grad_input,
                                          const at::Tensor &grad_output,
                                          const at::Tensor &output) {
  std::cout << "aten::sigmoid_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::sigmoid_backward_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::sigmoid_backward(const at::Tensor &grad_output,
                                                 const at::Tensor &output) {
  std::cout << "aten::sigmoid_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::sigmoid_backward(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &
ATenMLIRTypeDefault::tanh_backward_out(at::Tensor &grad_input,
                                       const at::Tensor &grad_output,
                                       const at::Tensor &output) {
  std::cout << "aten::tanh_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::tanh_backward_out(mlirtens[0], mlirtens[1], mlirtens[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::tanh_backward(const at::Tensor &grad_output,
                                              const at::Tensor &output) {
  std::cout << "aten::tanh_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::tanh_backward(mlirtens[0], mlirtens[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::slow_conv_transpose2d_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef output_padding,
    at::IntArrayRef dilation) {
  std::cout << "aten::slow_conv_transpose2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose2d_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, mlirtens[3], stride,
      padding, output_padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::slow_conv_transpose2d(
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef output_padding,
    at::IntArrayRef dilation) {
  std::cout << "aten::slow_conv_transpose2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose2d(
      mlirtens[0], mlirtens[1], kernel_size, mlirtens[2], stride, padding,
      output_padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::slow_conv_transpose2d_backward_out(
    at::Tensor &grad_input, at::Tensor &grad_weight, at::Tensor &grad_bias,
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef output_padding, at::IntArrayRef dilation,
    const at::Tensor &columns, const at::Tensor &ones) {
  std::cout << "aten::slow_conv_transpose2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_input, grad_weight, grad_bias, grad_output,
      self,       weight,      columns,   ones};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], kernel_size, stride, padding, output_padding, dilation,
      mlirtens[6], mlirtens[7]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
      grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::slow_conv_transpose2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef output_padding, at::IntArrayRef dilation,
    const at::Tensor &columns, const at::Tensor &ones,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::slow_conv_transpose2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight,
                                              columns, ones};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose2d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      output_padding, dilation, mlirtens[3], mlirtens[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor &ATenMLIRTypeDefault::slow_conv_transpose3d_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef output_padding,
    at::IntArrayRef dilation) {
  std::cout << "aten::slow_conv_transpose3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose3d_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, mlirtens[3], stride,
      padding, output_padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::slow_conv_transpose3d(
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef output_padding,
    at::IntArrayRef dilation) {
  std::cout << "aten::slow_conv_transpose3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose3d(
      mlirtens[0], mlirtens[1], kernel_size, mlirtens[2], stride, padding,
      output_padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::slow_conv_transpose3d_backward_out(
    at::Tensor &grad_input, at::Tensor &grad_weight, at::Tensor &grad_bias,
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef output_padding, at::IntArrayRef dilation,
    const at::Tensor &finput, const at::Tensor &fgrad_input) {
  std::cout << "aten::slow_conv_transpose3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_input, grad_weight, grad_bias, grad_output,
      self,       weight,      finput,    fgrad_input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose3d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], kernel_size, stride, padding, output_padding, dilation,
      mlirtens[6], mlirtens[7]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
      grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::slow_conv_transpose3d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding,
    at::IntArrayRef output_padding, at::IntArrayRef dilation,
    const at::Tensor &finput, const at::Tensor &fgrad_input,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::slow_conv_transpose3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight, finput,
                                              fgrad_input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_transpose3d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      output_padding, dilation, mlirtens[3], mlirtens[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor &ATenMLIRTypeDefault::thnn_conv2d_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::thnn_conv2d_out(mlirtens[0], mlirtens[1], mlirtens[2], kernel_size,
                          mlirtens[3], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::thnn_conv2d(const at::Tensor &self,
                                            const at::Tensor &weight,
                                            at::IntArrayRef kernel_size,
                                            const at::Tensor &bias,
                                            at::IntArrayRef stride,
                                            at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv2d(mlirtens[0], mlirtens[1], kernel_size,
                                    mlirtens[2], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::thnn_conv2d_forward_out(
    at::Tensor &output, at::Tensor &finput, at::Tensor &fgrad_input,
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv2d_forward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, finput, fgrad_input,
                                              self,   weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv2d_forward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      kernel_size, mlirtens[5], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(output, finput,
                                                              fgrad_input);
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::thnn_conv2d_forward(const at::Tensor &self,
                                         const at::Tensor &weight,
                                         at::IntArrayRef kernel_size,
                                         const at::Tensor &bias,
                                         at::IntArrayRef stride,
                                         at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv2d_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv2d_forward(
      mlirtens[0], mlirtens[1], kernel_size, mlirtens[2], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::thnn_conv2d_backward_out(
    at::Tensor &grad_input, at::Tensor &grad_weight, at::Tensor &grad_bias,
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor &finput,
    const at::Tensor &fgrad_input) {
  std::cout << "aten::thnn_conv2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_input, grad_weight, grad_bias, grad_output,
      self,       weight,      finput,    fgrad_input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], kernel_size, stride, padding, mlirtens[6], mlirtens[7]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
      grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::thnn_conv2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor &finput,
    const at::Tensor &fgrad_input, std::array<bool, 3> output_mask) {
  std::cout << "aten::thnn_conv2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight, finput,
                                              fgrad_input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv2d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      mlirtens[3], mlirtens[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor &ATenMLIRTypeDefault::thnn_conv_depthwise2d_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::thnn_conv_depthwise2d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv_depthwise2d_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, mlirtens[3], stride,
      padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::thnn_conv_depthwise2d(
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::thnn_conv_depthwise2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::thnn_conv_depthwise2d(mlirtens[0], mlirtens[1], kernel_size,
                                mlirtens[2], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::thnn_conv_depthwise2d_forward_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::thnn_conv_depthwise2d_forward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv_depthwise2d_forward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, mlirtens[3], stride,
      padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::thnn_conv_depthwise2d_forward(
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::thnn_conv_depthwise2d_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::thnn_conv_depthwise2d_forward(mlirtens[0], mlirtens[1], kernel_size,
                                        mlirtens[2], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::thnn_conv_depthwise2d_backward_out(
    at::Tensor &grad_input, at::Tensor &grad_weight,
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::thnn_conv_depthwise2d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_weight,
                                              grad_output, self, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv_depthwise2d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      kernel_size, stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &>(grad_input, grad_weight);
}

std::tuple<at::Tensor, at::Tensor>
ATenMLIRTypeDefault::thnn_conv_depthwise2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    std::array<bool, 2> output_mask) {
  std::cout << "aten::thnn_conv_depthwise2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv_depthwise2d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      dilation, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor &ATenMLIRTypeDefault::thnn_conv3d_out(
    at::Tensor &out, const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv3d_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::thnn_conv3d_out(mlirtens[0], mlirtens[1], mlirtens[2], kernel_size,
                          mlirtens[3], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::thnn_conv3d(const at::Tensor &self,
                                            const at::Tensor &weight,
                                            at::IntArrayRef kernel_size,
                                            const at::Tensor &bias,
                                            at::IntArrayRef stride,
                                            at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv3d(mlirtens[0], mlirtens[1], kernel_size,
                                    mlirtens[2], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::thnn_conv3d_forward_out(
    at::Tensor &output, at::Tensor &finput, at::Tensor &fgrad_input,
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv3d_forward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {output, finput, fgrad_input,
                                              self,   weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv3d_forward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      kernel_size, mlirtens[5], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(output, finput,
                                                              fgrad_input);
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::thnn_conv3d_forward(const at::Tensor &self,
                                         const at::Tensor &weight,
                                         at::IntArrayRef kernel_size,
                                         const at::Tensor &bias,
                                         at::IntArrayRef stride,
                                         at::IntArrayRef padding) {
  std::cout << "aten::thnn_conv3d_forward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv3d_forward(
      mlirtens[0], mlirtens[1], kernel_size, mlirtens[2], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(self)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(self)));
}

std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>
ATenMLIRTypeDefault::thnn_conv3d_backward_out(
    at::Tensor &grad_input, at::Tensor &grad_weight, at::Tensor &grad_bias,
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor &finput,
    const at::Tensor &fgrad_input) {
  std::cout << "aten::thnn_conv3d_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {
      grad_input, grad_weight, grad_bias, grad_output,
      self,       weight,      finput,    fgrad_input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv3d_backward_out(
      mlirtens[0], mlirtens[1], mlirtens[2], mlirtens[3], mlirtens[4],
      mlirtens[5], kernel_size, stride, padding, mlirtens[6], mlirtens[7]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
      grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::thnn_conv3d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor &finput,
    const at::Tensor &fgrad_input, std::array<bool, 3> output_mask) {
  std::cout << "aten::thnn_conv3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight, finput,
                                              fgrad_input};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::thnn_conv3d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      mlirtens[3], mlirtens[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor ATenMLIRTypeDefault::slow_conv_dilated2d(
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::slow_conv_dilated2d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::slow_conv_dilated2d(mlirtens[0], mlirtens[1], kernel_size,
                              mlirtens[2], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::slow_conv_dilated2d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::slow_conv_dilated2d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_dilated2d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      dilation, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor ATenMLIRTypeDefault::slow_conv_dilated3d(
    const at::Tensor &self, const at::Tensor &weight,
    at::IntArrayRef kernel_size, const at::Tensor &bias, at::IntArrayRef stride,
    at::IntArrayRef padding, at::IntArrayRef dilation) {
  std::cout << "aten::slow_conv_dilated3d" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self, weight, bias};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::slow_conv_dilated3d(mlirtens[0], mlirtens[1], kernel_size,
                              mlirtens[2], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
ATenMLIRTypeDefault::slow_conv_dilated3d_backward(
    const at::Tensor &grad_output, const at::Tensor &self,
    const at::Tensor &weight, at::IntArrayRef kernel_size,
    at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation,
    std::array<bool, 3> output_mask) {
  std::cout << "aten::slow_conv_dilated3d_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output, self, weight};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::slow_conv_dilated3d_backward(
      mlirtens[0], mlirtens[1], mlirtens[2], kernel_size, stride, padding,
      dilation, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor, at::Tensor, at::Tensor>(
      bridge::CreateMLIRTensor(std::get<0>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<1>(x_result),
                               bridge::GetMLIRDevice(grad_output)),
      bridge::CreateMLIRTensor(std::get<2>(x_result),
                               bridge::GetMLIRDevice(grad_output)));
}

at::Tensor &ATenMLIRTypeDefault::col2im_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef output_size,
    at::IntArrayRef kernel_size, at::IntArrayRef dilation,
    at::IntArrayRef padding, at::IntArrayRef stride) {
  std::cout << "aten::col2im_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::col2im_out(mlirtens[0], mlirtens[1], output_size,
                                   kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::col2im(const at::Tensor &self,
                                       at::IntArrayRef output_size,
                                       at::IntArrayRef kernel_size,
                                       at::IntArrayRef dilation,
                                       at::IntArrayRef padding,
                                       at::IntArrayRef stride) {
  std::cout << "aten::col2im" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::col2im(mlirtens[0], output_size, kernel_size, dilation,
                               padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::col2im_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef kernel_size, at::IntArrayRef dilation,
    at::IntArrayRef padding, at::IntArrayRef stride) {
  std::cout << "aten::col2im_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::col2im_backward_out(
      mlirtens[0], mlirtens[1], kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::col2im_backward(const at::Tensor &grad_output,
                                                at::IntArrayRef kernel_size,
                                                at::IntArrayRef dilation,
                                                at::IntArrayRef padding,
                                                at::IntArrayRef stride) {
  std::cout << "aten::col2im_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::col2im_backward(mlirtens[0], kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

at::Tensor &ATenMLIRTypeDefault::im2col_out(
    at::Tensor &out, const at::Tensor &self, at::IntArrayRef kernel_size,
    at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  std::cout << "aten::im2col_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {out, self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::im2col_out(mlirtens[0], mlirtens[1], kernel_size,
                                   dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor ATenMLIRTypeDefault::im2col(const at::Tensor &self,
                                       at::IntArrayRef kernel_size,
                                       at::IntArrayRef dilation,
                                       at::IntArrayRef padding,
                                       at::IntArrayRef stride) {
  std::cout << "aten::im2col" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {self};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::im2col(mlirtens[0], kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(self));
}

at::Tensor &ATenMLIRTypeDefault::im2col_backward_out(
    at::Tensor &grad_input, const at::Tensor &grad_output,
    at::IntArrayRef input_size, at::IntArrayRef kernel_size,
    at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  std::cout << "aten::im2col_backward_out" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_input, grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result =
      at::im2col_backward_out(mlirtens[0], mlirtens[1], input_size, kernel_size,
                              dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor ATenMLIRTypeDefault::im2col_backward(const at::Tensor &grad_output,
                                                at::IntArrayRef input_size,
                                                at::IntArrayRef kernel_size,
                                                at::IntArrayRef dilation,
                                                at::IntArrayRef padding,
                                                at::IntArrayRef stride) {
  std::cout << "aten::im2col_backward" << std::endl;
  std::vector<at::Tensor> mlirtens_tensors = {grad_output};
  auto mlirtens = bridge::MLIRCreateTensorList(mlirtens_tensors);
  auto &&x_result = at::im2col_backward(mlirtens[0], input_size, kernel_size,
                                        dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return bridge::CreateMLIRTensor(x_result, bridge::GetMLIRDevice(grad_output));
}

void RegisterAtenTypeFunctions() {
  static auto dispatch =
      torch::RegisterOperators()
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Byte(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Byte>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Char(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Char>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Double(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Double>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Float(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Float>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Int(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Int>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Long(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Long>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Short(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Short>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cast_Half(Tensor self, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_cast_Half>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::backward(Tensor self, Tensor? gradient=None, bool "
                      "keep_graph=False, bool create_graph=False) -> void")
                  .impl_unboxedOnlyKernel<void(const at::Tensor &,
                                               const at::Tensor &, bool, bool),
                                          &ATenMLIRTypeDefault::backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::set_data(Tensor(a!) self, Tensor new_data) -> "
                          "void")
                  .impl_unboxedOnlyKernel<void(const at::Tensor &,
                                               const at::Tensor &),
                                          &ATenMLIRTypeDefault::set_data>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::data(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::data>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_debug_has_internal_overlap(Tensor self) -> int")
                  .impl_unboxedOnlyKernel<
                      int64_t(const at::Tensor &),
                      &ATenMLIRTypeDefault::_debug_has_internal_overlap>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_fused_dropout(Tensor self, float p, "
                          "Generator? generator=None) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, double,
                                              at::Generator *),
                                          &ATenMLIRTypeDefault::_fused_dropout>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_masked_scale(Tensor self, Tensor mask, float "
                          "scale) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     double),
                                          &ATenMLIRTypeDefault::_masked_scale>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_sobol_engine_draw(Tensor quasi, int n, Tensor "
                      "sobolstate, int dimension, int num_generated, "
                      "ScalarType? dtype) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          int64_t, const at::Tensor &, int64_t, int64_t,
                          c10::optional<at::ScalarType>),
                      &ATenMLIRTypeDefault::_sobol_engine_draw>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sobol_engine_ff_(Tensor(a!) self, int n, "
                          "Tensor sobolstate, int dimension, int "
                          "num_generated) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, int64_t, const at::Tensor &,
                                   int64_t, int64_t),
                      &ATenMLIRTypeDefault::_sobol_engine_ff_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sobol_engine_scramble_(Tensor(a!) self, "
                          "Tensor ltm, int dimension) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::_sobol_engine_scramble_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sobol_engine_initialize_state_(Tensor(a!) "
                          "self, int dimension) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::_sobol_engine_initialize_state_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_reshape_from_tensor(Tensor self, Tensor "
                          "shape) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::_reshape_from_tensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_shape_as_tensor(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::_shape_as_tensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::dropout(Tensor input, float p, bool train) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, double,
                                                     bool),
                                          &ATenMLIRTypeDefault::dropout>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::dropout_(Tensor(a!) self, float p, bool "
                          "train) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       bool),
                                          &ATenMLIRTypeDefault::dropout_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::feature_dropout(Tensor input, float p, bool "
                          "train) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, double, bool),
                      &ATenMLIRTypeDefault::feature_dropout>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::feature_dropout_(Tensor(a!) self, float p, "
                          "bool train) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, double, bool),
                      &ATenMLIRTypeDefault::feature_dropout_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::alpha_dropout(Tensor input, float p, bool "
                          "train) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, double,
                                                     bool),
                                          &ATenMLIRTypeDefault::alpha_dropout>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::alpha_dropout_(Tensor(a!) self, float p, bool "
                          "train) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       bool),
                                          &ATenMLIRTypeDefault::alpha_dropout_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::feature_alpha_dropout(Tensor input, float p, "
                          "bool train) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, double, bool),
                      &ATenMLIRTypeDefault::feature_alpha_dropout>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::feature_alpha_dropout_(Tensor(a!) self, float "
                          "p, bool train) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, double, bool),
                      &ATenMLIRTypeDefault::feature_alpha_dropout_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::abs(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::abs>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::abs_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::abs_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::abs.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::abs_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::acos(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::acos>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::acos_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::acos_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::acos.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::acos_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::avg_pool1d(Tensor self, int[1] kernel_size, "
                      "int[1] stride=[], int[1] padding=0, bool "
                      "ceil_mode=False, bool count_include_pad=True) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool, bool),
                      &ATenMLIRTypeDefault::avg_pool1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_avg_pool1d(Tensor self, int[1] "
                          "output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_avg_pool1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool1d(Tensor self, int[1] "
                          "output_size) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_max_pool1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::add.Tensor(Tensor self, Tensor other, *, "
                          "Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRType::add>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::add_.Tensor(Tensor(a!) self, Tensor other, *, "
                          "Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRType::add_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::add.out(Tensor self, Tensor other, *, Scalar "
                          "alpha=1, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::add_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::add.Scalar(Tensor self, Scalar other, Scalar "
                          "alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::add>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::add_.Scalar(Tensor(a!) self, Scalar other, "
                          "Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::add_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addmv(Tensor self, Tensor mat, Tensor vec, *, "
                          "Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addmv>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, "
                      "*, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addmv_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addmv.out(Tensor self, Tensor mat, Tensor "
                          "vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addmv_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addr(Tensor self, Tensor vec1, Tensor vec2, "
                          "*, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addr>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, "
                      "*, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addr_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addr.out(Tensor self, Tensor vec1, Tensor "
                          "vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addr_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::affine_grid_generator(Tensor theta, int[] "
                          "size, bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::affine_grid_generator>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::affine_grid_generator_backward(Tensor grad, "
                          "int[] size, bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::affine_grid_generator_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::all.dim(Tensor self, int dim, bool "
                          "keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, bool),
                      &ATenMLIRTypeDefault::all>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::all.out(Tensor self, int dim, bool "
                          "keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       int64_t, bool),
                                          &ATenMLIRTypeDefault::all_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::allclose(Tensor self, Tensor other, float "
                          "rtol=1e-05, float atol=1e-08, bool equal_nan=False) "
                          "-> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &,
                                               const at::Tensor &, double,
                                               double, bool),
                                          &ATenMLIRTypeDefault::allclose>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::any.dim(Tensor self, int dim, bool "
                          "keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, bool),
                      &ATenMLIRTypeDefault::any>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::any.out(Tensor self, int dim, bool "
                          "keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       int64_t, bool),
                                          &ATenMLIRTypeDefault::any_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::arange(Scalar end, *, ScalarType? dtype=None, "
                          "Layout? layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::arange>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::arange.start(Scalar start, Scalar end, *, "
                      "ScalarType? dtype=None, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar, at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::arange>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::arange.start_step(Scalar start, Scalar end, "
                          "Scalar step, *, ScalarType? dtype=None, Layout? "
                          "layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar, at::Scalar,
                                                     at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::arange>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::arange.out(Scalar end, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::arange_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::arange.start_out(Scalar start, Scalar end, "
                          "Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::arange_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_dim_arange(Tensor like, int dim) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::_dim_arange>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::argmax(Tensor self, int? dim=None, bool "
                          "keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     c10::optional<int64_t>,
                                                     bool),
                                          &ATenMLIRTypeDefault::argmax>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::argmin(Tensor self, int? dim=None, bool "
                          "keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     c10::optional<int64_t>,
                                                     bool),
                                          &ATenMLIRTypeDefault::argmin>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::as_strided(Tensor(a) self, int[] size, int[] "
                          "stride, int? storage_offset=None) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, c10::optional<int64_t>),
                      &ATenMLIRType::as_strided>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::as_strided_(Tensor(a!) self, int[] size, int[] "
                      "stride, int? storage_offset=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::as_strided_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::asin(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::asin>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::asin_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::asin_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::asin.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::asin_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::atan(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::atan>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::atan_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::atan_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::atan.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::atan_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::baddbmm(Tensor self, Tensor batch1, Tensor "
                          "batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::baddbmm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor "
                      "batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::baddbmm_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_baddbmm_mkl_(Tensor(a!) self, Tensor batch1, "
                          "Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_baddbmm_mkl_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::baddbmm.out(Tensor self, Tensor batch1, "
                          "Tensor batch2, *, Scalar beta=1, Scalar alpha=1, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::baddbmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::bartlett_window(int window_length, *, ScalarType? "
                      "dtype=None, Layout? layout=None, Device? device=None, "
                      "bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::bartlett_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bartlett_window.periodic(int window_length, "
                          "bool periodic, *, ScalarType? dtype=None, Layout? "
                          "layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, bool, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::bartlett_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::batch_norm(Tensor input, Tensor? weight, "
                          "Tensor? bias, Tensor? running_mean, Tensor? "
                          "running_var, bool training, float momentum, float "
                          "eps, bool cudnn_enabled) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, bool, double, double,
                                 bool),
                      &ATenMLIRTypeDefault::batch_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_batch_norm_impl_index(Tensor input, Tensor? "
                      "weight, Tensor? bias, Tensor? running_mean, Tensor? "
                      "running_var, bool training, float momentum, float eps, "
                      "bool cudnn_enabled) -> (Tensor, Tensor, Tensor, int)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor, int64_t>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, bool, double, double, bool),
                      &ATenMLIRTypeDefault::_batch_norm_impl_index>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_batch_norm_impl_index_backward(int "
                          "impl_index, Tensor input, Tensor grad_output, "
                          "Tensor? weight, Tensor? running_mean, Tensor? "
                          "running_var, Tensor? save_mean, Tensor? "
                          "save_var_transform, bool train, float eps, bool[3] "
                          "output_mask) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          int64_t, const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, bool, double,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::_batch_norm_impl_index_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bernoulli(Tensor self, *, Generator? "
                          "generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Generator *),
                                          &ATenMLIRTypeDefault::bernoulli>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bernoulli.out(Tensor self, *, Generator? "
                          "generator=None, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::bernoulli_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, "
                          "*, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::bernoulli_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bernoulli_.float(Tensor(a!) self, float "
                          "p=0.5, *, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::bernoulli_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bernoulli.p(Tensor self, float p, *, "
                          "Generator? generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, double,
                                                     at::Generator *),
                                          &ATenMLIRTypeDefault::bernoulli>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bilinear(Tensor input1, Tensor input2, Tensor "
                          "weight, Tensor? bias) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::bilinear>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::binary_cross_entropy_with_logits(Tensor self, "
                          "Tensor target, Tensor? weight=None, Tensor? "
                          "pos_weight=None, int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 int64_t),
                      &ATenMLIRTypeDefault::binary_cross_entropy_with_logits>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::binary_cross_entropy_with_logits_backward("
                          "Tensor grad_output, Tensor self, Tensor target, "
                          "Tensor? weight=None, Tensor? pos_weight=None, int "
                          "reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::
                          binary_cross_entropy_with_logits_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bincount(Tensor self, Tensor? weights=None, "
                          "int minlength=0) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::bincount>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bitwise_not(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::bitwise_not>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::bitwise_not_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bitwise_not.out(Tensor self, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::bitwise_not_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logical_not(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::logical_not>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logical_not_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::logical_not_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logical_not.out(Tensor self, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::logical_not_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::logical_xor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::logical_xor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logical_xor_(Tensor(a!) self, Tensor other) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::logical_xor_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logical_xor.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::logical_xor_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::blackman_window(int window_length, *, ScalarType? "
                      "dtype=None, Layout? layout=None, Device? device=None, "
                      "bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::blackman_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::blackman_window.periodic(int window_length, "
                          "bool periodic, *, ScalarType? dtype=None, Layout? "
                          "layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, bool, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::blackman_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bmm(Tensor self, Tensor mat2) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::bmm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::bmm.out(Tensor self, Tensor mat2, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::bmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]")
                  .impl_unboxedOnlyKernel<
                      std::vector<at::Tensor>(at::TensorList),
                      &ATenMLIRTypeDefault::broadcast_tensors>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cat(Tensor[] tensors, int dim=0) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::TensorList, int64_t),
                                          &ATenMLIRTypeDefault::cat>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cat.out(Tensor[] tensors, int dim=0, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::TensorList, int64_t),
                                          &ATenMLIRTypeDefault::cat_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ceil(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::ceil>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ceil_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::ceil_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ceil.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::ceil_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::chain_matmul(Tensor[] matrices) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::TensorList),
                                          &ATenMLIRTypeDefault::chain_matmul>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::chunk(Tensor(a) self, int chunks, int dim=0) "
                          "-> Tensor(a)[]")
                  .impl_unboxedOnlyKernel<std::vector<at::Tensor>(
                                              const at::Tensor &, int64_t,
                                              int64_t),
                                          &ATenMLIRTypeDefault::chunk>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp(Tensor self, Scalar? min=None, Scalar? "
                          "max=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     c10::optional<at::Scalar>,
                                                     c10::optional<at::Scalar>),
                                          &ATenMLIRTypeDefault::clamp>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp_(Tensor(a!) self, Scalar? min=None, "
                          "Scalar? max=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, c10::optional<at::Scalar>,
                                   c10::optional<at::Scalar>),
                      &ATenMLIRTypeDefault::clamp_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp.out(Tensor self, Scalar? min=None, "
                          "Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              c10::optional<at::Scalar>,
                                              c10::optional<at::Scalar>),
                                          &ATenMLIRTypeDefault::clamp_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp_max(Tensor self, Scalar max) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::clamp_max>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp_max_(Tensor(a!) self, Scalar max) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::clamp_max_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp_max.out(Tensor self, Scalar max, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::clamp_max_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp_min(Tensor self, Scalar min) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::clamp_min>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp_min_(Tensor(a!) self, Scalar min) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::clamp_min_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clamp_min.out(Tensor self, Scalar min, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::clamp_min_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::constant_pad_nd(Tensor self, int[] pad, "
                          "Scalar value=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::Scalar),
                      &ATenMLIRTypeDefault::constant_pad_nd>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::contiguous(Tensor self, *, MemoryFormat "
                          "memory_format=contiguous_format) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::MemoryFormat),
                                          &ATenMLIRTypeDefault::contiguous>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::convolution(Tensor input, Tensor weight, Tensor? "
                      "bias, int[] stride, int[] padding, int[] dilation, bool "
                      "transposed, int[] output_padding, int groups) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool,
                                 at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::convolution>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::convolution_overrideable(Tensor input, Tensor "
                          "weight, Tensor? bias, int[] stride, int[] padding, "
                          "int[] dilation, bool transposed, int[] "
                          "output_padding, int groups) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool,
                                 at::IntArrayRef, int64_t),
                      &ATenMLIRType::convolution_overrideable>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::convolution_backward_overrideable(Tensor "
                          "grad_output, Tensor input, Tensor weight, int[] "
                          "stride, int[] padding, int[] dilation, bool "
                          "transposed, int[] output_padding, int groups, "
                          "bool[3] output_mask) -> (Tensor grad_input, Tensor "
                          "grad_weight, Tensor grad_bias)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, bool, at::IntArrayRef, int64_t,
                          std::array<bool, 3>),
                      &ATenMLIRType::convolution_backward_overrideable>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_convolution(Tensor input, Tensor weight, "
                          "Tensor? bias, int[] stride, int[] padding, int[] "
                          "dilation, bool transposed, int[] output_padding, "
                          "int groups, bool benchmark, bool deterministic, "
                          "bool cudnn_enabled) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool,
                                 at::IntArrayRef, int64_t, bool, bool, bool),
                      &ATenMLIRTypeDefault::_convolution>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_convolution_nogroup(Tensor input, Tensor "
                          "weight, Tensor? bias, int[] stride, int[] padding, "
                          "int[] dilation, bool transposed, int[] "
                          "output_padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::_convolution_nogroup>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_convolution_double_backward(Tensor? ggI, Tensor? "
                      "ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor "
                      "self, int[] stride, int[] padding, int[] dilation, bool "
                      "transposed, int[] output_padding, int groups, bool "
                      "benchmark, bool deterministic, bool cudnn_enabled, "
                      "bool[3] output_mask) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          bool, at::IntArrayRef, int64_t, bool, bool, bool,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::_convolution_double_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv1d(Tensor input, Tensor weight, Tensor? "
                          "bias=None, int[1] stride=1, int[1] padding=0, "
                          "int[1] dilation=1, int groups=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::conv1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv2d(Tensor input, Tensor weight, Tensor? "
                          "bias=None, int[2] stride=1, int[2] padding=0, "
                          "int[2] dilation=1, int groups=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::conv2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv3d(Tensor input, Tensor weight, Tensor? "
                          "bias=None, int[3] stride=1, int[3] padding=0, "
                          "int[3] dilation=1, int groups=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::conv3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv_tbc(Tensor self, Tensor weight, Tensor "
                          "bias, int pad=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::conv_tbc>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv_tbc_backward(Tensor self, Tensor input, "
                          "Tensor weight, Tensor bias, int pad) -> (Tensor, "
                          "Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::conv_tbc_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv_transpose1d(Tensor input, Tensor weight, "
                          "Tensor? bias=None, int[1] stride=1, int[1] "
                          "padding=0, int[1] output_padding=0, int groups=1, "
                          "int[1] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::conv_transpose1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv_transpose2d.input(Tensor input, Tensor "
                          "weight, Tensor? bias=None, int[2] stride=1, int[2] "
                          "padding=0, int[2] output_padding=0, int groups=1, "
                          "int[2] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::conv_transpose2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::conv_transpose3d.input(Tensor input, Tensor "
                          "weight, Tensor? bias=None, int[3] stride=1, int[3] "
                          "padding=0, int[3] output_padding=0, int groups=1, "
                          "int[3] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::conv_transpose3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::copy_(Tensor(a!) self, Tensor src, bool "
                          "non_blocking=False) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRType::copy_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_copy_from(Tensor self, Tensor dst, bool "
                          "non_blocking=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRType::_copy_from>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cos(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::cos>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cos_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::cos_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cos.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::cos_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cosh(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::cosh>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cosh_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::cosh_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cosh.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::cosh_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cosine_embedding_loss(Tensor input1, Tensor "
                          "input2, Tensor target, float margin=0.0, int "
                          "reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, double, int64_t),
                      &ATenMLIRTypeDefault::cosine_embedding_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cumsum(Tensor self, int dim, *, ScalarType? "
                          "dtype=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &, int64_t,
                                              c10::optional<at::ScalarType>),
                                          &ATenMLIRTypeDefault::cumsum>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::cumsum.out(Tensor self, int dim, *, ScalarType? "
                      "dtype=None, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t,
                                   c10::optional<at::ScalarType>),
                      &ATenMLIRTypeDefault::cumsum_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cumprod(Tensor self, int dim, *, ScalarType? "
                          "dtype=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &, int64_t,
                                              c10::optional<at::ScalarType>),
                                          &ATenMLIRTypeDefault::cumprod>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::cumprod.out(Tensor self, int dim, *, ScalarType? "
                      "dtype=None, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t,
                                   c10::optional<at::ScalarType>),
                      &ATenMLIRTypeDefault::cumprod_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ctc_loss.IntList(Tensor log_probs, Tensor "
                          "targets, int[] input_lengths, int[] target_lengths, "
                          "int blank=0, int reduction=Mean, bool "
                          "zero_infinity=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 int64_t, bool),
                      &ATenMLIRTypeDefault::ctc_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ctc_loss.Tensor(Tensor log_probs, Tensor "
                          "targets, Tensor input_lengths, Tensor "
                          "target_lengths, int blank=0, int reduction=Mean, "
                          "bool zero_infinity=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, bool),
                      &ATenMLIRTypeDefault::ctc_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] "
                      "input_lengths, int[] target_lengths, int blank=0, bool "
                      "zero_infinity=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::IntArrayRef, at::IntArrayRef,
                                              int64_t, bool),
                                          &ATenMLIRTypeDefault::_ctc_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_ctc_loss_backward(Tensor grad, Tensor "
                          "log_probs, Tensor targets, int[] input_lengths, "
                          "int[] target_lengths, Tensor neg_log_likelihood, "
                          "Tensor log_alpha, int blank, bool "
                          "zero_infinity=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, const at::Tensor &,
                                 const at::Tensor &, int64_t, bool),
                      &ATenMLIRTypeDefault::_ctc_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::det(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::det>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::diag_embed(Tensor self, int offset=0, int "
                          "dim1=-2, int dim2=-1) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t, int64_t),
                                          &ATenMLIRTypeDefault::diag_embed>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::diagflat(Tensor self, int offset=0) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::diagflat>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::diagonal(Tensor(a) self, int offset=0, int "
                          "dim1=0, int dim2=1) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t, int64_t),
                                          &ATenMLIRTypeDefault::diagonal>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fill_diagonal_(Tensor(a!) self, Scalar "
                          "fill_value, bool wrap=False) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       bool),
                                          &ATenMLIRTypeDefault::fill_diagonal_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::div.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRType::div>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::div_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRType::div_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::div.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::div_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::div.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRType::div>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::div_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::div_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::dot(Tensor self, Tensor tensor) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::dot>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::dot.out(Tensor self, Tensor tensor, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::dot_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::einsum(str equation, Tensor[] tensors) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(std::string,
                                                     at::TensorList),
                                          &ATenMLIRTypeDefault::einsum>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::embedding(Tensor weight, Tensor indices, int "
                          "padding_idx=-1, bool scale_grad_by_freq=False, bool "
                          "sparse=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t, bool, bool),
                                          &ATenMLIRTypeDefault::embedding>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::embedding_backward(Tensor grad, Tensor "
                          "indices, int num_weights, int padding_idx, bool "
                          "scale_grad_by_freq, bool sparse) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, bool, bool),
                      &ATenMLIRTypeDefault::embedding_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::embedding_dense_backward(Tensor grad_output, "
                          "Tensor indices, int num_weights, int padding_idx, "
                          "bool scale_grad_by_freq) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, bool),
                      &ATenMLIRTypeDefault::embedding_dense_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::embedding_renorm_(Tensor(a!) self, Tensor "
                      "indices, float max_norm, float norm_type) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, double,
                                   double),
                      &ATenMLIRTypeDefault::embedding_renorm_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::embedding_sparse_backward(Tensor grad, Tensor "
                          "indices, int num_weights, int padding_idx, bool "
                          "scale_grad_by_freq) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, bool),
                      &ATenMLIRTypeDefault::embedding_sparse_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::embedding_bag(Tensor weight, Tensor indices, "
                          "Tensor offsets, bool scale_grad_by_freq=False, int "
                          "mode=0, bool sparse=False, Tensor? "
                          "per_sample_weights=None) -> (Tensor, Tensor, "
                          "Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor,
                                                     at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &, bool, int64_t,
                                              bool, const at::Tensor &),
                                          &ATenMLIRTypeDefault::embedding_bag>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_embedding_bag(Tensor weight, Tensor indices, "
                          "Tensor offsets, bool scale_grad_by_freq=False, int "
                          "mode=0, bool sparse=False, Tensor? "
                          "per_sample_weights=None) -> (Tensor, Tensor, "
                          "Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor,
                                                     at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &, bool, int64_t,
                                              bool, const at::Tensor &),
                                          &ATenMLIRTypeDefault::_embedding_bag>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_embedding_bag_backward(Tensor grad, Tensor "
                          "indices, Tensor offsets, Tensor offset2bag, Tensor "
                          "bag_size, Tensor maximum_indices, int num_weights, "
                          "bool scale_grad_by_freq, int mode, bool sparse, "
                          "Tensor? per_sample_weights) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 int64_t, bool, int64_t, bool,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::_embedding_bag_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_embedding_bag_sparse_backward(Tensor grad, "
                          "Tensor indices, Tensor offsets, Tensor offset2bag, "
                          "Tensor bag_size, int num_weights, bool "
                          "scale_grad_by_freq, int mode, Tensor? "
                          "per_sample_weights) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t, bool, int64_t,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::_embedding_bag_sparse_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_embedding_bag_dense_backward(Tensor grad, "
                          "Tensor indices, Tensor offsets, Tensor offset2bag, "
                          "Tensor bag_size, Tensor maximum_indices, int "
                          "num_weights, bool scale_grad_by_freq, int mode, "
                          "Tensor? per_sample_weights) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 int64_t, bool, int64_t, const at::Tensor &),
                      &ATenMLIRTypeDefault::_embedding_bag_dense_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_embedding_bag_per_sample_weights_backward("
                          "Tensor grad, Tensor weight, Tensor indices, Tensor "
                          "offsets, Tensor offset2bag, int mode) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::
                          _embedding_bag_per_sample_weights_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::empty.memory_format(int[] size, *, "
                          "ScalarType? dtype=None, Layout? layout=None, "
                          "Device? device=None, bool? pin_memory=None, "
                          "MemoryFormat? memory_format=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::TensorOptions &,
                                 c10::optional<at::MemoryFormat>),
                      &ATenMLIRTypeDefault::empty>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::new_empty(Tensor self, int[] size, *, ScalarType? "
                      "dtype=None, Layout? layout=None, Device? device=None, "
                      "bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::new_empty>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::new_full(Tensor self, int[] size, Scalar "
                          "fill_value, *, ScalarType? dtype=None, Layout? "
                          "layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::Scalar, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::new_full>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_empty_affine_quantized(int[] size, *, "
                          "ScalarType? dtype=None, Layout? layout=None, "
                          "Device? device=None, bool? pin_memory=None, float "
                          "scale=1, int zero_point=0, MemoryFormat? "
                          "memory_format=contiguous_format) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::TensorOptions &,
                                 double, int64_t,
                                 c10::optional<at::MemoryFormat>),
                      &ATenMLIRTypeDefault::_empty_affine_quantized>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_empty_per_channel_affine_quantized_like(Tensor "
                      "self, Tensor zero_points, int[] size, int[] axis, *, "
                      "ScalarType? dtype=None, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None, MemoryFormat? "
                      "memory_format=contiguous_format) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 const at::TensorOptions &,
                                 c10::optional<at::MemoryFormat>),
                      &ATenMLIRTypeDefault::
                          _empty_per_channel_affine_quantized_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::resize_(Tensor(a!) self, int[] size) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef),
                                          &ATenMLIRTypeDefault::resize_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::empty.out(int[] size, *, MemoryFormat? "
                          "memory_format=None, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, at::IntArrayRef,
                                              c10::optional<at::MemoryFormat>),
                                          &ATenMLIRTypeDefault::empty_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::empty_like(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::empty_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::empty_like.dtype(Tensor self, *, ScalarType "
                          "dtype, Layout layout, Device device, bool "
                          "pin_memory=False, MemoryFormat? "
                          "memory_format=contiguous_format) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::TensorOptions &,
                                 c10::optional<at::MemoryFormat>),
                      &ATenMLIRTypeDefault::empty_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::empty_strided(int[] size, int[] stride, *, "
                      "ScalarType? dtype=None, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::IntArrayRef,
                                                     at::IntArrayRef,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::empty_strided>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erf(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::erf>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erf_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::erf_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erf.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::erf_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erfc(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::erfc>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erfc_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::erfc_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erfc.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::erfc_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::exp(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::exp>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::exp_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::exp_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::exp.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::exp_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::expm1(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::expm1>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::expm1_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::expm1_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::expm1.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::expm1_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::expand(Tensor(a) self, int[] size, *, bool "
                          "implicit=False) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool),
                      &ATenMLIRType::expand>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::expand_as(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::expand_as>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eye(int n, *, ScalarType? dtype=None, Layout? "
                          "layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::eye>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eye.m(int n, int m, *, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, int64_t, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::eye>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::eye_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       int64_t),
                                          &ATenMLIRTypeDefault::eye_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::flatten.using_ints(Tensor self, int "
                          "start_dim=0, int end_dim=-1) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t),
                                          &ATenMLIRTypeDefault::flatten>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fill_.Scalar(Tensor(a!) self, Scalar value) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::fill_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fill_.Tensor(Tensor(a!) self, Tensor value) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::fill_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::floor(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::floor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::floor_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::floor_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::floor.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::floor_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::frac(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::frac>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::frac_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::frac_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::frac.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::frac_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::full(int[] size, Scalar fill_value, *, "
                      "ScalarType? dtype=None, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::IntArrayRef,
                                                     at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::full>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::full.out(int[] size, Scalar fill_value, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::IntArrayRef, at::Scalar),
                      &ATenMLIRTypeDefault::full_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::full_like(Tensor self, Scalar fill_value) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::full_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::full_like.dtype(Tensor self, Scalar "
                          "fill_value, *, ScalarType dtype, Layout layout, "
                          "Device device, bool pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::full_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::from_file(str filename, bool? shared=None, int? "
                      "size=0, *, ScalarType? dtype=None, Layout? layout=None, "
                      "Device? device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(std::string,
                                                     c10::optional<bool>,
                                                     c10::optional<int64_t>,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::from_file>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::grid_sampler(Tensor input, Tensor grid, int "
                          "interpolation_mode, int padding_mode, bool "
                          "align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t, int64_t, bool),
                                          &ATenMLIRTypeDefault::grid_sampler>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::grid_sampler_2d(Tensor input, Tensor grid, "
                          "int interpolation_mode, int padding_mode, bool "
                          "align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, bool),
                      &ATenMLIRTypeDefault::grid_sampler_2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::grid_sampler_2d_backward(Tensor grad_output, "
                      "Tensor input, Tensor grid, int interpolation_mode, int "
                      "padding_mode, bool align_corners) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         int64_t, int64_t,
                                                         bool),
                      &ATenMLIRTypeDefault::grid_sampler_2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::grid_sampler_3d(Tensor input, Tensor grid, "
                          "int interpolation_mode, int padding_mode, bool "
                          "align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, bool),
                      &ATenMLIRTypeDefault::grid_sampler_3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::grid_sampler_3d_backward(Tensor grad_output, "
                      "Tensor input, Tensor grid, int interpolation_mode, int "
                      "padding_mode, bool align_corners) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         int64_t, int64_t,
                                                         bool),
                      &ATenMLIRTypeDefault::grid_sampler_3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hann_window(int window_length, *, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::hann_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hann_window.periodic(int window_length, bool "
                          "periodic, *, ScalarType? dtype=None, Layout? "
                          "layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, bool,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::hann_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::hamming_window(int window_length, *, ScalarType? "
                      "dtype=None, Layout? layout=None, Device? device=None, "
                      "bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::hamming_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hamming_window.periodic(int window_length, "
                          "bool periodic, *, ScalarType? dtype=None, Layout? "
                          "layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, bool,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::hamming_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::hamming_window.periodic_alpha(int window_length, "
                      "bool periodic, float alpha, *, ScalarType? dtype=None, "
                      "Layout? layout=None, Device? device=None, bool? "
                      "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, bool, double,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::hamming_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::hamming_window.periodic_alpha_beta(int "
                      "window_length, bool periodic, float alpha, float beta, "
                      "*, ScalarType? dtype=None, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, bool, double,
                                                     double,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::hamming_window>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::hinge_embedding_loss(Tensor self, Tensor target, "
                      "float margin=1.0, int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &, double,
                                 int64_t),
                      &ATenMLIRTypeDefault::hinge_embedding_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ger(Tensor self, Tensor vec2) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::ger>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ger.out(Tensor self, Tensor vec2, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::ger_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::group_norm(Tensor input, int num_groups, "
                          "Tensor? weight=None, Tensor? bias=None, float "
                          "eps=1e-05, bool cudnn_enabled=True) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &, int64_t,
                                              const at::Tensor &,
                                              const at::Tensor &, double, bool),
                                          &ATenMLIRTypeDefault::group_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fft(Tensor self, int signal_ndim, bool "
                          "normalized=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, bool),
                      &ATenMLIRTypeDefault::fft>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ifft(Tensor self, int signal_ndim, bool "
                          "normalized=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, bool),
                      &ATenMLIRTypeDefault::ifft>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rfft(Tensor self, int signal_ndim, bool "
                          "normalized=False, bool onesided=True) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, bool, bool),
                      &ATenMLIRTypeDefault::rfft>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::irfft(Tensor self, int signal_ndim, bool "
                          "normalized=False, bool onesided=True, int[] "
                          "signal_sizes=[]) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, bool, bool,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::irfft>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_fft_with_size(Tensor self, int signal_ndim, bool "
                      "complex_input, bool complex_output, bool inverse, int[] "
                      "checked_signal_sizes, bool normalized, bool onesided, "
                      "int[] output_sizes) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, bool, bool, bool,
                                 at::IntArrayRef, bool, bool, at::IntArrayRef),
                      &ATenMLIRTypeDefault::_fft_with_size>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cufft_get_plan_cache_size(int device_index) "
                          "-> int")
                  .impl_unboxedOnlyKernel<
                      int64_t(int64_t),
                      &ATenMLIRTypeDefault::_cufft_get_plan_cache_size>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cufft_get_plan_cache_max_size(int "
                          "device_index) -> int")
                  .impl_unboxedOnlyKernel<
                      int64_t(int64_t),
                      &ATenMLIRTypeDefault::_cufft_get_plan_cache_max_size>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cufft_set_plan_cache_max_size(int "
                          "device_index, int max_size) -> void")
                  .impl_unboxedOnlyKernel<
                      void(int64_t, int64_t),
                      &ATenMLIRTypeDefault::_cufft_set_plan_cache_max_size>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_cufft_clear_plan_cache(int device_index) -> void")
                  .impl_unboxedOnlyKernel<
                      void(int64_t),
                      &ATenMLIRTypeDefault::_cufft_clear_plan_cache>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index.Tensor(Tensor self, Tensor?[] indices) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::TensorList),
                                          &ATenMLIRTypeDefault::index>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_copy_(Tensor(a!) self, int dim, Tensor "
                          "index, Tensor source) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::index_copy_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_copy(Tensor self, int dim, Tensor "
                          "index, Tensor source) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::index_copy>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::index_put_(Tensor(a!) self, Tensor?[] indices, "
                      "Tensor values, bool accumulate=False) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, at::TensorList,
                                              const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::index_put_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_put(Tensor self, Tensor?[] indices, "
                          "Tensor values, bool accumulate=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::TensorList,
                                                     const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::index_put>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_index_put_impl_(Tensor(a!) self, Tensor?[] "
                          "indices, Tensor values, bool accumulate=False, bool "
                          "unsafe=False) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::TensorList,
                                   const at::Tensor &, bool, bool),
                      &ATenMLIRTypeDefault::_index_put_impl_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::instance_norm(Tensor input, Tensor? weight, "
                          "Tensor? bias, Tensor? running_mean, Tensor? "
                          "running_var, bool use_input_stats, float momentum, "
                          "float eps, bool cudnn_enabled) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, bool, double, double,
                                 bool),
                      &ATenMLIRTypeDefault::instance_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::inverse(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::inverse>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::inverse.out(Tensor self, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::inverse_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_inverse_helper(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::_inverse_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::isclose(Tensor self, Tensor other, float "
                          "rtol=1e-05, float atol=1e-08, bool equal_nan=False) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &, double,
                                                     double, bool),
                                          &ATenMLIRTypeDefault::isclose>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::isnan(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::isnan>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_distributed(Tensor self) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_distributed>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_floating_point(Tensor self) -> bool")
                  .impl_unboxedOnlyKernel<
                      bool(const at::Tensor &),
                      &ATenMLIRTypeDefault::is_floating_point>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_complex(Tensor self) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_complex>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_nonzero(Tensor self) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_nonzero>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::is_same_size(Tensor self, Tensor other) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &,
                                               const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_same_size>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_signed(Tensor self) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_signed>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::kl_div(Tensor self, Tensor target, int "
                          "reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::kl_div>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::kl_div_backward(Tensor grad_output, Tensor "
                          "self, Tensor target, int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::kl_div_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::kthvalue(Tensor self, int k, int dim=-1, bool "
                          "keepdim=False) -> (Tensor values, Tensor indices)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, int64_t,
                                              int64_t, bool),
                                          &ATenMLIRTypeDefault::kthvalue>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::kthvalue.values(Tensor self, int k, int dim=-1, "
                      "bool keepdim=False, *, Tensor(a!) values, Tensor(b!) "
                      "indices) -> (Tensor(a!) values, Tensor(b!) indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, int64_t, bool),
                      &ATenMLIRTypeDefault::kthvalue_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::layer_norm(Tensor input, int[] normalized_shape, "
                      "Tensor? weight=None, Tensor? bias=None, float "
                      "eps=1e-05, bool cudnn_enable=True) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 const at::Tensor &, const at::Tensor &, double,
                                 bool),
                      &ATenMLIRTypeDefault::layer_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::native_layer_norm(Tensor input, Tensor? "
                          "weight, Tensor? bias, int M, int N, float eps) -> "
                          "(Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, int64_t, int64_t, double),
                      &ATenMLIRTypeDefault::native_layer_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::native_layer_norm_backward(Tensor grad_out, "
                          "Tensor input, Tensor mean, Tensor rstd, Tensor? "
                          "weight, int M, int N, bool[3] output_mask) -> "
                          "(Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, int64_t, int64_t,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::native_layer_norm_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::native_layer_norm_double_backward(Tensor? ggI, "
                      "Tensor? ggW, Tensor? ggb, Tensor gO, Tensor input, "
                      "Tensor mean, Tensor rstd, Tensor? weight, int M, int N, "
                      "bool[3] output_mask) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &, int64_t,
                          int64_t, std::array<bool, 3>),
                      &ATenMLIRTypeDefault::native_layer_norm_double_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::linear(Tensor input, Tensor weight, Tensor? "
                          "bias=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::linear>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_linear(Tensor input, Tensor weight, "
                          "Tensor? bias=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::mkldnn_linear>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fbgemm_linear_int8_weight_fp32_activation("
                          "Tensor input, Tensor weight, Tensor packed, Tensor "
                          "col_offsets, Scalar weight_scale, Scalar "
                          "weight_zero_point, Tensor bias) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, const at::Tensor &),
                      &ATenMLIRTypeDefault::
                          fbgemm_linear_int8_weight_fp32_activation>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fbgemm_linear_int8_weight(Tensor input, "
                          "Tensor weight, Tensor packed, Tensor col_offsets, "
                          "Scalar weight_scale, Scalar weight_zero_point, "
                          "Tensor bias) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, const at::Tensor &),
                      &ATenMLIRTypeDefault::fbgemm_linear_int8_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fbgemm_linear_quantize_weight(Tensor input) "
                          "-> (Tensor, Tensor, float, int)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, double, int64_t>(
                          const at::Tensor &),
                      &ATenMLIRTypeDefault::fbgemm_linear_quantize_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::fbgemm_pack_gemm_matrix_fp16>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor "
                      "input, Tensor packed_weight, Tensor bias) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::
                          fbgemm_linear_fp16_weight_fp32_activation>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fbgemm_linear_fp16_weight(Tensor input, "
                          "Tensor packed_weight, Tensor bias) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::fbgemm_linear_fp16_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fbgemm_pack_quantized_matrix(Tensor input) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::fbgemm_pack_quantized_matrix>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fbgemm_pack_quantized_matrix.KN(Tensor input, "
                          "int K, int N) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, int64_t),
                      &ATenMLIRTypeDefault::fbgemm_pack_quantized_matrix>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::linspace(Scalar start, Scalar end, int steps=100, "
                      "*, ScalarType? dtype=None, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar, at::Scalar,
                                                     int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::linspace>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::linspace.out(Scalar start, Scalar end, int "
                          "steps=100, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       at::Scalar, int64_t),
                                          &ATenMLIRTypeDefault::linspace_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::log>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::log_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::log_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log10(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::log10>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log10_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::log10_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log10.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::log10_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log1p(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::log1p>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log1p_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::log1p_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log1p.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::log1p_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log2(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::log2>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log2_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::log2_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log2.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::log2_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logdet(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::logdet>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logspace(Scalar start, Scalar end, int "
                          "steps=100, float base=10.0, *, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar, at::Scalar,
                                                     int64_t, double,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::logspace>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logspace.out(Scalar start, Scalar end, int "
                          "steps=100, float base=10.0, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       at::Scalar, int64_t,
                                                       double),
                                          &ATenMLIRTypeDefault::logspace_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_softmax(Tensor self, int dim, ScalarType? "
                          "dtype=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &, int64_t,
                                              c10::optional<at::ScalarType>),
                                          &ATenMLIRTypeDefault::log_softmax>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_log_softmax(Tensor self, int dim, bool "
                          "half_to_float) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, bool),
                                          &ATenMLIRType::_log_softmax>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_log_softmax_backward_data(Tensor grad_output, "
                      "Tensor output, int dim, Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, const at::Tensor &),
                      &ATenMLIRType::_log_softmax_backward_data>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logsumexp(Tensor self, int[1] dim, bool "
                          "keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::logsumexp>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::logsumexp.out(Tensor self, int[1] dim, bool "
                          "keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::logsumexp_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::margin_ranking_loss(Tensor input1, Tensor "
                          "input2, Tensor target, float margin=0.0, int "
                          "reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, double, int64_t),
                      &ATenMLIRTypeDefault::margin_ranking_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::matmul(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::matmul>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::matmul.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::matmul_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::matrix_rank.tol(Tensor self, float tol, bool "
                          "symmetric=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, double,
                                                     bool),
                                          &ATenMLIRTypeDefault::matrix_rank>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::matrix_rank(Tensor self, bool "
                          "symmetric=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::matrix_rank>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::matrix_power(Tensor self, int n) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::matrix_power>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max.dim(Tensor self, int dim, bool "
                          "keepdim=False) -> (Tensor values, Tensor indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         int64_t, bool),
                      &ATenMLIRTypeDefault::max>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max.dim_max(Tensor self, int dim, bool "
                      "keepdim=False, *, Tensor(a!) max, Tensor(b!) "
                      "max_values) -> (Tensor(a!) values, Tensor(b!) indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::max_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_values(Tensor self, int[1] dim, bool "
                          "keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::max_values>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_pool1d_with_indices(Tensor self, int[1] "
                      "kernel_size, int[1] stride=[], int[1] padding=0, int[1] "
                      "dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::max_pool1d_with_indices>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_pool1d(Tensor self, int[1] kernel_size, "
                          "int[1] stride=[], int[1] padding=0, int[1] "
                          "dilation=1, bool ceil_mode=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::max_pool1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_pool2d(Tensor self, int[2] kernel_size, "
                          "int[2] stride=[], int[2] padding=0, int[2] "
                          "dilation=1, bool ceil_mode=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::max_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_max_pool2d(Tensor self, int[2] "
                          "kernel_size, int[2] stride=[], int[2] padding=0, "
                          "int[2] dilation=1, bool ceil_mode=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::mkldnn_max_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_max_pool2d(Tensor self, int[2] "
                          "kernel_size, int[2] stride=[], int[2] padding=0, "
                          "int[2] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::quantized_max_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_pool3d(Tensor self, int[3] kernel_size, "
                          "int[3] stride=[], int[3] padding=0, int[3] "
                          "dilation=1, bool ceil_mode=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::max_pool3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mean(Tensor self, *, ScalarType? dtype=None) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &,
                                 c10::optional<at::ScalarType>),
                      &ATenMLIRType::mean>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mean.dim(Tensor self, int[1] dim, bool "
                          "keepdim=False, *, ScalarType? dtype=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool,
                                 c10::optional<at::ScalarType>),
                      &ATenMLIRType::mean>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mean.out(Tensor self, int[1] dim, bool "
                          "keepdim=False, *, ScalarType? dtype=None, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, bool,
                                              c10::optional<at::ScalarType>),
                                          &ATenMLIRTypeDefault::mean_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::median.dim(Tensor self, int dim, bool "
                          "keepdim=False) -> (Tensor values, Tensor indices)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, int64_t,
                                              bool),
                                          &ATenMLIRTypeDefault::median>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::median.dim_values(Tensor self, int dim, bool "
                          "keepdim=False, *, Tensor(a!) values, Tensor(b!) "
                          "indices) -> (Tensor(a!) values, Tensor(b!) indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::median_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::min.dim(Tensor self, int dim, bool "
                          "keepdim=False) -> (Tensor values, Tensor indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         int64_t, bool),
                      &ATenMLIRTypeDefault::min>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::min.dim_min(Tensor self, int dim, bool "
                      "keepdim=False, *, Tensor(a!) min, Tensor(b!) "
                      "min_indices) -> (Tensor(a!) values, Tensor(b!) indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::min_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::min_values(Tensor self, int[1] dim, bool "
                          "keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::min_values>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_convolution(Tensor self, Tensor "
                          "weight, Tensor? bias, int[] padding, int[] stride, "
                          "int[] dilation, int groups) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::mkldnn_convolution>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_convolution_backward_input(int[] "
                          "self_size, Tensor grad_output, Tensor weight, int[] "
                          "padding, int[] stride, int[] dilation, int groups, "
                          "bool bias_defined) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool),
                      &ATenMLIRTypeDefault::mkldnn_convolution_backward_input>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_convolution_backward_weights(int[] "
                          "weight_size, Tensor grad_output, Tensor self, int[] "
                          "padding, int[] stride, int[] dilation, int groups, "
                          "bool bias_defined) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          at::IntArrayRef, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, int64_t, bool),
                      &ATenMLIRTypeDefault::
                          mkldnn_convolution_backward_weights>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_convolution_backward(Tensor self, "
                          "Tensor grad_output, Tensor weight, int[] padding, "
                          "int[] stride, int[] dilation, int groups, bool[3] "
                          "output_mask) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, int64_t, std::array<bool, 3>),
                      &ATenMLIRTypeDefault::mkldnn_convolution_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_batch_norm(Tensor input, Tensor "
                          "weight, Tensor? bias, Tensor? running_mean, Tensor? "
                          "running_var, bool training, float "
                          "exponential_average_factor, float epsilon) -> "
                          "(Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, bool, double, double),
                      &ATenMLIRTypeDefault::miopen_batch_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::miopen_batch_norm_backward(Tensor input, Tensor "
                      "grad_output, Tensor weight, Tensor? running_mean, "
                      "Tensor? running_var, Tensor? save_mean, Tensor? "
                      "save_var, float epsilon) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, double),
                      &ATenMLIRTypeDefault::miopen_batch_norm_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_convolution(Tensor self, Tensor "
                          "weight, Tensor? bias, int[] padding, int[] stride, "
                          "int[] dilation, int groups, bool benchmark, bool "
                          "deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool, bool),
                      &ATenMLIRTypeDefault::miopen_convolution>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_convolution_backward_input(int[] "
                          "self_size, Tensor grad_output, Tensor weight, int[] "
                          "padding, int[] stride, int[] dilation, int groups, "
                          "bool benchmark, bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool, bool),
                      &ATenMLIRTypeDefault::miopen_convolution_backward_input>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_convolution_backward(Tensor self, "
                          "Tensor grad_output, Tensor weight, int[] padding, "
                          "int[] stride, int[] dilation, int groups, bool "
                          "benchmark, bool deterministic, bool[3] output_mask) "
                          "-> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, int64_t, bool, bool,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::miopen_convolution_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_convolution_backward_bias(Tensor "
                          "grad_output) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::miopen_convolution_backward_bias>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_convolution_backward_weight(int[] "
                          "weight_size, Tensor grad_output, Tensor self, int[] "
                          "padding, int[] stride, int[] dilation, int groups, "
                          "bool benchmark, bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool, bool),
                      &ATenMLIRTypeDefault::miopen_convolution_backward_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::miopen_convolution_transpose(Tensor self, Tensor "
                      "weight, Tensor? bias, int[] padding, int[] "
                      "output_padding, int[] stride, int[] dilation, int "
                      "groups, bool benchmark, bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, int64_t, bool, bool),
                      &ATenMLIRTypeDefault::miopen_convolution_transpose>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::miopen_convolution_transpose_backward(Tensor "
                      "self, Tensor grad_output, Tensor weight, int[] padding, "
                      "int[] output_padding, int[] stride, int[] dilation, int "
                      "groups, bool benchmark, bool deterministic, bool[3] "
                      "output_mask) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::
                          miopen_convolution_transpose_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_convolution_transpose_backward_input("
                          "Tensor grad_output, Tensor weight, int[] padding, "
                          "int[] stride, int[] dilation, int groups, bool "
                          "benchmark, bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, int64_t, bool, bool),
                      &ATenMLIRTypeDefault::
                          miopen_convolution_transpose_backward_input>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::miopen_convolution_transpose_backward_weight(int[]"
                      " weight_size, Tensor grad_output, Tensor self, int[] "
                      "padding, int[] stride, int[] dilation, int groups, bool "
                      "benchmark, bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool, bool),
                      &ATenMLIRTypeDefault::
                          miopen_convolution_transpose_backward_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_depthwise_convolution(Tensor self, "
                          "Tensor weight, Tensor? bias, int[] padding, int[] "
                          "stride, int[] dilation, int groups, bool benchmark, "
                          "bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool, bool),
                      &ATenMLIRTypeDefault::miopen_depthwise_convolution>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::miopen_depthwise_convolution_backward_input(int[] "
                      "self_size, Tensor grad_output, Tensor weight, int[] "
                      "padding, int[] stride, int[] dilation, int groups, bool "
                      "benchmark, bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool, bool),
                      &ATenMLIRTypeDefault::
                          miopen_depthwise_convolution_backward_input>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_depthwise_convolution_backward(Tensor "
                          "self, Tensor grad_output, Tensor weight, int[] "
                          "padding, int[] stride, int[] dilation, int groups, "
                          "bool benchmark, bool deterministic, bool[3] "
                          "output_mask) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, int64_t, bool, bool,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::
                          miopen_depthwise_convolution_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::miopen_depthwise_convolution_backward_weight(int[]"
                      " weight_size, Tensor grad_output, Tensor self, int[] "
                      "padding, int[] stride, int[] dilation, int groups, bool "
                      "benchmark, bool deterministic) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t,
                                 bool, bool),
                      &ATenMLIRTypeDefault::
                          miopen_depthwise_convolution_backward_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_rnn(Tensor input, Tensor[] weight, int "
                          "weight_stride0, Tensor hx, Tensor? cx, int mode, "
                          "int hidden_size, int num_layers, bool batch_first, "
                          "float dropout, bool train, bool bidirectional, "
                          "int[] batch_sizes, Tensor? dropout_state) -> "
                          "(Tensor, Tensor, Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor,
                                 at::Tensor>(
                          const at::Tensor &, at::TensorList, int64_t,
                          const at::Tensor &, const at::Tensor &, int64_t,
                          int64_t, int64_t, bool, double, bool, bool,
                          at::IntArrayRef, const at::Tensor &),
                      &ATenMLIRTypeDefault::miopen_rnn>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::miopen_rnn_backward(Tensor input, Tensor[] "
                          "weight, int weight_stride0, Tensor weight_buf, "
                          "Tensor hx, Tensor? cx, Tensor output, Tensor? "
                          "grad_output, Tensor? grad_hy, Tensor? grad_cy, int "
                          "mode, int hidden_size, int num_layers, bool "
                          "batch_first, float dropout, bool train, bool "
                          "bidirectional, int[] batch_sizes, Tensor? "
                          "dropout_state, Tensor reserve, bool[4] output_mask) "
                          "-> (Tensor, Tensor, Tensor, Tensor[])")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor,
                                 std::vector<at::Tensor>>(
                          const at::Tensor &, at::TensorList, int64_t,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, int64_t, int64_t, int64_t, bool,
                          double, bool, bool, at::IntArrayRef,
                          const at::Tensor &, const at::Tensor &,
                          std::array<bool, 4>),
                      &ATenMLIRTypeDefault::miopen_rnn_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mm(Tensor self, Tensor mat2) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRType::mm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mm.out(Tensor self, Tensor mat2, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::mm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_sparse_mm(Tensor sparse, Tensor dense) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::_sparse_mm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mode(Tensor self, int dim=-1, bool "
                          "keepdim=False) -> (Tensor values, Tensor indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         int64_t, bool),
                      &ATenMLIRTypeDefault::mode>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mode.values(Tensor self, int dim=-1, bool "
                          "keepdim=False, *, Tensor(a!) values, Tensor(b!) "
                          "indices) -> (Tensor(a!) values, Tensor(b!) indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::mode_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::mul.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRType::mul>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRType::mul_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mul.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::mul_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::mul.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::mul>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::mul_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mv(Tensor self, Tensor vec) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::mv>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::mv_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mvlgamma(Tensor self, int p) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::mvlgamma>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::mvlgamma_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::narrow_copy(Tensor self, int dim, int start, "
                          "int length) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t, int64_t),
                                          &ATenMLIRTypeDefault::narrow_copy>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::narrow(Tensor(a) self, int dim, int start, "
                          "int length) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t, int64_t),
                                          &ATenMLIRTypeDefault::narrow>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::native_batch_norm(Tensor input, Tensor? "
                          "weight, Tensor? bias, Tensor? running_mean, Tensor? "
                          "running_var, bool training, float momentum, float "
                          "eps) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, bool, double, double),
                      &ATenMLIRType::native_batch_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::batch_norm_stats(Tensor input, float eps) -> "
                          "(Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         double),
                      &ATenMLIRTypeDefault::batch_norm_stats>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::batch_norm_elemt(Tensor input, Tensor? "
                          "weight, Tensor? bias, Tensor mean, Tensor invstd, "
                          "float eps) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, double),
                      &ATenMLIRTypeDefault::batch_norm_elemt>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::batch_norm_gather_stats(Tensor input, Tensor "
                          "mean, Tensor invstd, Tensor? running_mean, Tensor? "
                          "running_var, float momentum, float eps, int count) "
                          "-> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         double, double,
                                                         int64_t),
                      &ATenMLIRTypeDefault::batch_norm_gather_stats>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::batch_norm_gather_stats_with_counts(Tensor "
                          "input, Tensor mean, Tensor invstd, Tensor? "
                          "running_mean, Tensor? running_var, float momentum, "
                          "float eps, int[] counts) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         double, double,
                                                         at::IntArrayRef),
                      &ATenMLIRTypeDefault::
                          batch_norm_gather_stats_with_counts>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::native_batch_norm_backward(Tensor grad_out, "
                          "Tensor input, Tensor? weight, Tensor? running_mean, "
                          "Tensor? running_var, Tensor? save_mean, Tensor? "
                          "save_invstd, bool train, float eps, bool[3] "
                          "output_mask) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, bool, double,
                          std::array<bool, 3>),
                      &ATenMLIRType::native_batch_norm_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::batch_norm_backward_reduce(Tensor grad_out, "
                          "Tensor input, Tensor mean, Tensor invstd, Tensor? "
                          "weight, bool input_g, bool weight_g, bool bias_g) "
                          "-> (Tensor, Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::
                          tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>(
                              const at::Tensor &, const at::Tensor &,
                              const at::Tensor &, const at::Tensor &,
                              const at::Tensor &, bool, bool, bool),
                      &ATenMLIRTypeDefault::batch_norm_backward_reduce>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::batch_norm_backward_elemt(Tensor grad_out, Tensor "
                      "input, Tensor mean, Tensor invstd, Tensor? weight, "
                      "Tensor mean_dy, Tensor mean_dy_xmu) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::batch_norm_backward_elemt>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::batch_norm_update_stats(Tensor input, Tensor? "
                          "running_mean, Tensor? running_var, float momentum) "
                          "-> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         double),
                      &ATenMLIRTypeDefault::batch_norm_update_stats>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_nnpack_spatial_convolution(Tensor input, Tensor "
                      "weight, Tensor? bias, int[2] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::_nnpack_spatial_convolution>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_nnpack_spatial_convolution_backward(Tensor "
                          "input, Tensor grad_output, Tensor weight, int[2] "
                          "padding, bool[3] output_mask) -> (Tensor, Tensor, "
                          "Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::
                          _nnpack_spatial_convolution_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_nnpack_spatial_convolution_backward_input("
                          "Tensor input, Tensor grad_output, Tensor weight, "
                          "int[2] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::
                          _nnpack_spatial_convolution_backward_input>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_nnpack_spatial_convolution_backward_weight("
                          "Tensor input, int[] weightsize, Tensor grad_output, "
                          "int[2] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::
                          _nnpack_spatial_convolution_backward_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ones.out(int[] size, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef),
                                          &ATenMLIRTypeDefault::ones_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pairwise_distance(Tensor x1, Tensor x2, float "
                          "p=2, float eps=1e-06, bool keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &, double,
                                 double, bool),
                      &ATenMLIRTypeDefault::pairwise_distance>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::cdist(Tensor x1, Tensor x2, float p=2) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     double),
                                          &ATenMLIRTypeDefault::cdist>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cdist_backward(Tensor grad, Tensor x1, "
                          "Tensor x2, float p, Tensor cdist) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, double,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::_cdist_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pdist(Tensor self, float p=2) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     double),
                                          &ATenMLIRTypeDefault::pdist>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_pdist_forward(Tensor self, float p=2) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     double),
                                          &ATenMLIRTypeDefault::_pdist_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_pdist_backward(Tensor grad, Tensor self, "
                          "float p, Tensor pdist) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &, double,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::_pdist_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cosine_similarity(Tensor x1, Tensor x2, int "
                          "dim=1, float eps=1e-08) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, double),
                      &ATenMLIRTypeDefault::cosine_similarity>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::permute>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::numpy_T(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::numpy_T>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pixel_shuffle(Tensor self, int "
                          "upscale_factor) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::pixel_shuffle>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_pinned(Tensor self) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_pinned>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pin_memory(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::pin_memory>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pinverse(Tensor self, float rcond=1e-15) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     double),
                                          &ATenMLIRTypeDefault::pinverse>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::poisson_nll_loss(Tensor input, Tensor target, "
                          "bool log_input, bool full, float eps, int "
                          "reduction) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &, bool,
                                 bool, double, int64_t),
                      &ATenMLIRTypeDefault::poisson_nll_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::scalar_tensor(Scalar s, *, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::scalar_tensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rand(int[] size, *, ScalarType? dtype=None, "
                          "Layout? layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::rand>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::rand.generator(int[] size, *, Generator? "
                      "generator, ScalarType? dtype=None, Layout? layout=None, "
                      "Device? device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::IntArrayRef,
                                                     at::Generator *,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::rand>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rand.out(int[] size, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef),
                                          &ATenMLIRTypeDefault::rand_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rand.generator_out(int[] size, *, Generator? "
                          "generator, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::rand_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rand_like(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::rand_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rand_like.dtype(Tensor self, *, ScalarType "
                          "dtype, Layout layout, Device device, bool "
                          "pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::rand_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint(int high, int[] size, *, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, at::IntArrayRef,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randint>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint.generator(int high, int[] size, *, "
                          "Generator? generator, ScalarType? dtype=None, "
                          "Layout? layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, at::IntArrayRef,
                                                     at::Generator *,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randint>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::randint.low(int low, int high, int[] size, *, "
                      "ScalarType? dtype=None, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, int64_t,
                                                     at::IntArrayRef,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randint>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint.low_generator(int low, int high, "
                          "int[] size, *, Generator? generator, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, int64_t, at::IntArrayRef,
                                 at::Generator *, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::randint>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint.out(int high, int[] size, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       at::IntArrayRef),
                                          &ATenMLIRTypeDefault::randint_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::randint.generator_out(int high, int[] size, *, "
                      "Generator? generator, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       at::IntArrayRef,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::randint_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint.low_out(int low, int high, int[] "
                          "size, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       int64_t,
                                                       at::IntArrayRef),
                                          &ATenMLIRTypeDefault::randint_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint.low_generator_out(int low, int high, "
                          "int[] size, *, Generator? generator, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       int64_t, at::IntArrayRef,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::randint_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint_like(Tensor self, int high) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::randint_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint_like.low(Tensor self, int low, int "
                          "high) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t),
                                          &ATenMLIRTypeDefault::randint_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint_like.dtype(Tensor self, int high, *, "
                          "ScalarType dtype, Layout layout, Device device, "
                          "bool pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randint_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randint_like.low_dtype(Tensor self, int low, "
                          "int high, *, ScalarType dtype, Layout layout, "
                          "Device device, bool pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randint_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randn(int[] size, *, ScalarType? dtype=None, "
                          "Layout? layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::IntArrayRef,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randn>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::randn.generator(int[] size, *, Generator? "
                      "generator, ScalarType? dtype=None, Layout? layout=None, "
                      "Device? device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::IntArrayRef,
                                                     at::Generator *,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randn>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randn.out(int[] size, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef),
                                          &ATenMLIRTypeDefault::randn_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randn.generator_out(int[] size, *, Generator? "
                          "generator, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::randn_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randn_like(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::randn_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randn_like.dtype(Tensor self, *, ScalarType "
                          "dtype, Layout layout, Device device, bool "
                          "pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randn_like>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randperm(int n, *, ScalarType? dtype=None, "
                          "Layout? layout=None, Device? device=None, bool? "
                          "pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randperm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::randperm.generator(int n, *, Generator? "
                      "generator, ScalarType? dtype=None, Layout? layout=None, "
                      "Device? device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, at::Generator *,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::randperm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randperm.out(int n, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::randperm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::randperm.generator_out(int n, *, Generator? "
                          "generator, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::randperm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::range.step(Scalar start, Scalar end, Scalar "
                      "step=1, *, ScalarType? dtype=None, Layout? layout=None, "
                      "Device? device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar, at::Scalar,
                                                     at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::range>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::range(Scalar start, Scalar end, *, ScalarType? "
                      "dtype=None, Layout? layout=None, Device? device=None, "
                      "bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::Scalar, at::Scalar,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::range>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::range.out(Scalar start, Scalar end, Scalar "
                          "step=1, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::range_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reciprocal(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::reciprocal>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::reciprocal_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reciprocal.out(Tensor self, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::reciprocal_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::neg(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRType::neg>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::neg_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::neg_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::neg.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::neg_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::repeat(Tensor self, int[] repeats) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::repeat>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::repeat_interleave.Tensor(Tensor repeats) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::repeat_interleave>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::repeat_interleave.self_Tensor(Tensor self, "
                          "Tensor repeats, int? dim=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::repeat_interleave>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::repeat_interleave.self_int(Tensor self, int "
                          "repeats, int? dim=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::repeat_interleave>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reshape(Tensor self, int[] shape) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::reshape>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_mkldnn_reshape(Tensor self, int[] shape) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::_mkldnn_reshape>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::reshape_as(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::reshape_as>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::round(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::round>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::round_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::round_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::round.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::round_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rrelu(Tensor self, Scalar lower=0.125, Scalar "
                          "upper=0.3333333333333333, bool training=False, "
                          "Generator? generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar, at::Scalar,
                                                     bool, at::Generator *),
                                          &ATenMLIRTypeDefault::rrelu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, "
                      "Scalar upper=0.3333333333333333, bool training=False, "
                      "Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       at::Scalar, bool,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::rrelu_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::relu(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRType::relu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::relu_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRType::relu_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::prelu(Tensor self, Tensor weight) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::prelu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::prelu_backward(Tensor grad_output, Tensor "
                          "self, Tensor weight) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::prelu_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gelu(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::gelu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::gelu_backward(Tensor grad, Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::gelu_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hardshrink(Tensor self, Scalar lambd=0.5) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::hardshrink>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hardshrink_backward(Tensor grad_out, Tensor "
                          "self, Scalar lambd) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::Scalar),
                      &ATenMLIRTypeDefault::hardshrink_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rsqrt(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::rsqrt>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::rsqrt_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::rsqrt_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::select.int(Tensor(a) self, int dim, int "
                          "index) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t),
                                          &ATenMLIRTypeDefault::select>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::selu(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::selu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::selu_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::selu_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::celu>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::celu_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sigmoid(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::sigmoid>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::sigmoid_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sigmoid.out(Tensor self, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::sigmoid_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sin(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::sin>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sin_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::sin_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sin.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::sin_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sinh(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::sinh>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sinh_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::sinh_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sinh.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::sinh_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::detach(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::detach>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::detach_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::detach_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::size.int(Tensor self, int dim) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &, int64_t),
                                          &ATenMLIRType::size>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slice.Tensor(Tensor(a) self, int dim=0, int "
                          "start=0, int end=9223372036854775807, int step=1) "
                          "-> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t, int64_t,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::slice>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slogdet(Tensor self) -> (Tensor sign, Tensor "
                          "logabsdet)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::slogdet>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::smm(Tensor self, Tensor mat2) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::smm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::softmax(Tensor self, int dim, ScalarType? "
                          "dtype=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &, int64_t,
                                              c10::optional<at::ScalarType>),
                                          &ATenMLIRTypeDefault::softmax>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_softmax(Tensor self, int dim, bool "
                          "half_to_float) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, bool),
                                          &ATenMLIRTypeDefault::_softmax>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_softmax_backward_data(Tensor grad_output, "
                          "Tensor output, int dim, Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, const at::Tensor &),
                      &ATenMLIRTypeDefault::_softmax_backward_data>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_add.out(Tensor self, Tensor other, *, "
                          "Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::_sparse_add_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_sparse_dense_add.out(Tensor self, Tensor other, "
                      "*, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::_sparse_dense_add_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_div_zerodim.out(Tensor self, Tensor "
                          "other, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::_sparse_div_zerodim_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_div_scalar.out(Tensor self, Scalar "
                          "other, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::Scalar),
                      &ATenMLIRTypeDefault::_sparse_div_scalar_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_mul.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::_sparse_mul_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_mul_zerodim.out(Tensor self, Tensor "
                          "other, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::_sparse_mul_zerodim_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_mul_scalar.out(Tensor self, Scalar "
                          "other, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::Scalar),
                      &ATenMLIRTypeDefault::_sparse_mul_scalar_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::split.Tensor(Tensor(a) self, int split_size, "
                          "int dim=0) -> Tensor(a)[]")
                  .impl_unboxedOnlyKernel<std::vector<at::Tensor>(
                                              const at::Tensor &, int64_t,
                                              int64_t),
                                          &ATenMLIRTypeDefault::split>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::split_with_sizes(Tensor self, int[] "
                          "split_sizes, int dim=0) -> Tensor[]")
                  .impl_unboxedOnlyKernel<
                      std::vector<at::Tensor>(
                          const at::Tensor &,
                          at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::split_with_sizes>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::squeeze(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::squeeze>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t),
                      &ATenMLIRType::squeeze>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::squeeze_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::squeeze_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::squeeze_.dim(Tensor(a!) self, int dim) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::squeeze_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sspaddmm(Tensor self, Tensor mat1, Tensor "
                          "mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::sspaddmm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor "
                          "mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::sspaddmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::stack(Tensor[] tensors, int dim=0) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::TensorList, int64_t),
                                          &ATenMLIRTypeDefault::stack>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::stack.out(Tensor[] tensors, int dim=0, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::TensorList, int64_t),
                                          &ATenMLIRTypeDefault::stack_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::stft(Tensor self, int n_fft, int? "
                          "hop_length=None, int? win_length=None, Tensor? "
                          "window=None, bool normalized=False, bool "
                          "onesided=True) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 c10::optional<int64_t>, c10::optional<int64_t>,
                                 const at::Tensor &, bool, bool),
                      &ATenMLIRTypeDefault::stft>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::stride.int(Tensor self, int dim) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::stride>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sum(Tensor self, *, ScalarType? dtype=None) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &,
                                 c10::optional<at::ScalarType>),
                      &ATenMLIRTypeDefault::sum>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sum.dim_IntList(Tensor self, int[1] dim, bool "
                          "keepdim=False, *, ScalarType? dtype=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool,
                                 c10::optional<at::ScalarType>),
                      &ATenMLIRType::sum>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sum.IntList_out(Tensor self, int[1] dim, bool "
                          "keepdim=False, *, ScalarType? dtype=None, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, bool,
                                              c10::optional<at::ScalarType>),
                                          &ATenMLIRTypeDefault::sum_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::sum_to_size(Tensor self, int[] size) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::sum_to_size>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sqrt(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::sqrt>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sqrt_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::sqrt_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::sqrt_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::std(Tensor self, bool unbiased=True) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::std>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::std.dim(Tensor self, int[1] dim, bool "
                          "unbiased=True, bool keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef, bool,
                                                     bool),
                                          &ATenMLIRTypeDefault::std>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::std_mean(Tensor self, bool unbiased=True) -> "
                          "(Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::std_mean>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::std_mean.dim(Tensor self, int[1] dim, bool "
                      "unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              at::IntArrayRef, bool, bool),
                                          &ATenMLIRTypeDefault::std_mean>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::std.out(Tensor self, int[1] dim, bool "
                          "unbiased=True, bool keepdim=False, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, bool, bool),
                                          &ATenMLIRTypeDefault::std_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::prod(Tensor self, *, ScalarType? dtype=None) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &,
                                 c10::optional<at::ScalarType>),
                      &ATenMLIRTypeDefault::prod>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::prod.dim_int(Tensor self, int dim, bool "
                          "keepdim=False, *, ScalarType? dtype=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, bool,
                                 c10::optional<at::ScalarType>),
                      &ATenMLIRTypeDefault::prod>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::prod.int_out(Tensor self, int dim, bool "
                          "keepdim=False, *, ScalarType? dtype=None, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t,
                                   bool, c10::optional<at::ScalarType>),
                      &ATenMLIRTypeDefault::prod_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::t(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRType::t>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::t_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::t_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tan(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::tan>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tan_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::tan_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tan.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::tan_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tanh(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::tanh>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tanh_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::tanh_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tanh.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::tanh_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tensordot(Tensor self, Tensor other, int[] "
                          "dims_self, int[] dims_other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::tensordot>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::threshold(Tensor self, Scalar threshold, "
                          "Scalar value) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::threshold>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::threshold_(Tensor(a!) self, Scalar threshold, "
                          "Scalar value) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::threshold_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::threshold.out(Tensor self, Scalar threshold, "
                          "Scalar value, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::threshold_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::threshold_backward(Tensor grad_output, Tensor "
                          "self, Scalar threshold) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRType::threshold_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::transpose.int(Tensor(a) self, int dim0, int "
                          "dim1) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t),
                                          &ATenMLIRTypeDefault::transpose>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_mkldnn_transpose(Tensor self, int dim0, int "
                          "dim1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, int64_t),
                      &ATenMLIRTypeDefault::_mkldnn_transpose>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::transpose_(Tensor(a!) self, int dim0, int "
                          "dim1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       int64_t),
                                          &ATenMLIRTypeDefault::transpose_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, "
                          "int dim1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, int64_t, int64_t),
                      &ATenMLIRTypeDefault::_mkldnn_transpose_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::one_hot(Tensor self, int num_classes=-1) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::one_hot>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::flip(Tensor self, int[] dims) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::flip>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::roll(Tensor self, int[1] shifts, int[1] "
                          "dims=[]) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::roll>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, at::IntArrayRef),
                                          &ATenMLIRTypeDefault::rot90>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::trapz>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, double,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::trapz>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, "
                          "int[] expand1, int[] expand2, int[] expand3, int[] "
                          "sumdim, int unroll_dim=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::_trilinear>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::triplet_margin_loss(Tensor anchor, Tensor "
                          "positive, Tensor negative, float margin=1.0, float "
                          "p=2, float eps=1e-06, bool swap=False, int "
                          "reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, double, double, double,
                                 bool, int64_t),
                      &ATenMLIRTypeDefault::triplet_margin_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::trunc(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::trunc>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::trunc_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::trunc_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::trunc.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::trunc_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::type_as(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::type_as>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_has_compatible_shallow_copy_type(Tensor "
                          "self, Tensor from) -> bool")
                  .impl_unboxedOnlyKernel<
                      bool(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::_has_compatible_shallow_copy_type>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_unique(Tensor self, bool sorted=True, bool "
                          "return_inverse=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool, bool),
                                          &ATenMLIRTypeDefault::_unique>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::unique_dim(Tensor self, int dim, bool "
                          "sorted=True, bool return_inverse=False, bool "
                          "return_counts=False) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, int64_t, bool, bool, bool),
                      &ATenMLIRTypeDefault::unique_dim>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::unique_consecutive(Tensor self, bool "
                          "return_inverse=False, bool return_counts=False, "
                          "int? dim=None) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, bool, bool,
                          c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::unique_consecutive>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::unique_dim_consecutive(Tensor self, int dim, "
                          "bool return_inverse=False, bool "
                          "return_counts=False) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, int64_t, bool, bool),
                      &ATenMLIRTypeDefault::unique_dim_consecutive>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_unique2(Tensor self, bool sorted=True, bool "
                          "return_inverse=False, bool return_counts=False) -> "
                          "(Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, bool, bool, bool),
                      &ATenMLIRTypeDefault::_unique2>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_unsafe_view(Tensor self, int[] size) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRType::_unsafe_view>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t),
                      &ATenMLIRType::unsqueeze>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::unsqueeze_(Tensor(a!) self, int dim) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::unsqueeze_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::var(Tensor self, bool unbiased=True) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::var>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::var.dim(Tensor self, int[1] dim, bool "
                          "unbiased=True, bool keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef, bool,
                                                     bool),
                                          &ATenMLIRTypeDefault::var>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::var.out(Tensor self, int[1] dim, bool "
                          "unbiased=True, bool keepdim=False, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, bool, bool),
                                          &ATenMLIRTypeDefault::var_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::var_mean(Tensor self, bool unbiased=True) -> "
                          "(Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::var_mean>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::var_mean.dim(Tensor self, int[1] dim, bool "
                      "unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              at::IntArrayRef, bool, bool),
                                          &ATenMLIRTypeDefault::var_mean>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::view_as(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::view_as>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::where.self(Tensor condition, Tensor self, "
                          "Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::where>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::where(Tensor condition) -> Tensor[]")
                  .impl_unboxedOnlyKernel<std::vector<at::Tensor>(
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::where>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_s_where(Tensor condition, Tensor self, "
                          "Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::_s_where>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::norm_except_dim(Tensor v, int pow=2, int "
                          "dim=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t, int64_t),
                      &ATenMLIRTypeDefault::norm_except_dim>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_weight_norm(Tensor v, Tensor g, int dim=0) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::_weight_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_weight_norm_cuda_interface(Tensor v, Tensor "
                          "g, int dim=0) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         int64_t),
                      &ATenMLIRTypeDefault::_weight_norm_cuda_interface>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_weight_norm_cuda_interface_backward(Tensor "
                          "grad_w, Tensor saved_v, Tensor saved_g, Tensor "
                          "saved_norms, int dim) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         int64_t),
                      &ATenMLIRTypeDefault::
                          _weight_norm_cuda_interface_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_weight_norm_differentiable_backward(Tensor "
                          "grad_w, Tensor saved_v, Tensor saved_g, Tensor "
                          "saved_norms, int dim) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         int64_t),
                      &ATenMLIRTypeDefault::
                          _weight_norm_differentiable_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::zeros.out(int[] size, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef),
                                          &ATenMLIRTypeDefault::zeros_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_standard_gamma_grad(Tensor self, Tensor "
                          "output) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::_standard_gamma_grad>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_standard_gamma(Tensor self, Generator? "
                          "generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Generator *),
                      &ATenMLIRTypeDefault::_standard_gamma>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_dirichlet_grad(Tensor x, Tensor alpha, "
                          "Tensor total) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::_dirichlet_grad>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sample_dirichlet(Tensor self, Generator? "
                          "generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Generator *),
                      &ATenMLIRTypeDefault::_sample_dirichlet>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::poisson(Tensor self, Generator? "
                          "generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Generator *),
                                          &ATenMLIRTypeDefault::poisson>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::native_norm(Tensor self, Scalar p=2) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::native_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_sum(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::_sparse_sum>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_sum.dtype(Tensor self, *, ScalarType "
                          "dtype) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::ScalarType),
                                          &ATenMLIRTypeDefault::_sparse_sum>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_sum.dim(Tensor self, int[1] dim) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::_sparse_sum>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_sum.dim_dtype(Tensor self, int[1] "
                          "dim, *, ScalarType dtype) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef,
                                                     at::ScalarType),
                                          &ATenMLIRTypeDefault::_sparse_sum>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_sum_backward(Tensor grad, Tensor "
                          "self, int[] dim) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::_sparse_sum_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, "
                          "*, ScalarType dtype) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     c10::optional<at::Scalar>,
                                                     at::ScalarType),
                                          &ATenMLIRTypeDefault::norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::norm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? "
                          "p, int[1] dim, bool keepdim, *, ScalarType dtype) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, c10::optional<at::Scalar>,
                                 at::IntArrayRef, bool, at::ScalarType),
                      &ATenMLIRTypeDefault::norm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, "
                          "int[1] dim, bool keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     c10::optional<at::Scalar>,
                                                     at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::norm.dtype_out(Tensor self, Scalar? p, int[1] "
                          "dim, bool keepdim, *, ScalarType dtype, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   c10::optional<at::Scalar>, at::IntArrayRef,
                                   bool, at::ScalarType),
                      &ATenMLIRTypeDefault::norm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool "
                      "keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              c10::optional<at::Scalar>,
                                              at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::norm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::frobenius_norm(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::frobenius_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::frobenius_norm.dim(Tensor self, int[1] dim, "
                          "bool keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::frobenius_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::frobenius_norm.out(Tensor self, int[1] dim, bool "
                      "keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::frobenius_norm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nuclear_norm(Tensor self, bool keepdim=False) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::nuclear_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nuclear_norm.out(Tensor self, bool "
                          "keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::nuclear_norm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nuclear_norm.dim(Tensor self, int[2] dim, "
                          "bool keepdim=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef, bool),
                                          &ATenMLIRTypeDefault::nuclear_norm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::nuclear_norm.dim_out(Tensor self, int[2] dim, "
                      "bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::nuclear_norm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::clone(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRType::clone>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::resize_as_(Tensor(a!) self, Tensor "
                          "the_template) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::resize_as_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow.Tensor_Scalar_out(Tensor self, Scalar "
                          "exponent, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::pow_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow.Tensor_Scalar(Tensor self, Scalar "
                          "exponent) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::pow>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::zero_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::zero_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sub.out(Tensor self, Tensor other, *, Scalar "
                          "alpha=1, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::sub_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sub.Tensor(Tensor self, Tensor other, *, "
                          "Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRType::sub>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, "
                          "Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRType::sub_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sub.Scalar(Tensor self, Scalar other, Scalar "
                          "alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::sub>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sub_.Scalar(Tensor(a!) self, Scalar other, "
                          "Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::sub_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rsub.Tensor(Tensor self, Tensor other, *, "
                          "Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::rsub>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rsub.Scalar(Tensor self, Scalar other, Scalar "
                          "alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::rsub>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::s_native_addmm.out(Tensor self, Tensor mat1, "
                          "Tensor mat2, *, Scalar beta=1, Scalar alpha=1, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::s_native_addmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::s_native_addmm(Tensor self, Tensor mat1, Tensor "
                      "mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::s_native_addmm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::s_native_addmm_(Tensor(a!) self, Tensor mat1, "
                          "Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::s_native_addmm_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_sparse_addmm(Tensor self, Tensor sparse, Tensor "
                      "dense, *, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_sparse_addmm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addmm.out(Tensor self, Tensor mat1, Tensor "
                          "mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addmm(Tensor self, Tensor mat1, Tensor mat2, "
                          "*, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRType::addmm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, "
                      "*, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addmm_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sparse_coo_tensor.size(int[] size, *, "
                          "ScalarType dtype, Layout layout, Device device, "
                          "bool pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::IntArrayRef, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::sparse_coo_tensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::sparse_coo_tensor.indices(Tensor indices, Tensor "
                      "values, *, ScalarType? dtype=None, Layout? layout=None, "
                      "Device? device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::TensorOptions &),
                      &ATenMLIRTypeDefault::sparse_coo_tensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sparse_coo_tensor.indices_size(Tensor "
                          "indices, Tensor values, int[] size, *, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::sparse_coo_tensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_sparse_coo_tensor_unsafe(Tensor indices, "
                          "Tensor values, int[] size, *, ScalarType? "
                          "dtype=None, Layout? layout=None, Device? "
                          "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::_sparse_coo_tensor_unsafe>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_sparse_coo_tensor_with_dims(int sparse_dim, int "
                      "dense_dim, int[] size, *, ScalarType dtype, Layout "
                      "layout, Device device, bool pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, int64_t, at::IntArrayRef,
                                 const at::TensorOptions &),
                      &ATenMLIRTypeDefault::_sparse_coo_tensor_with_dims>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_sparse_coo_tensor_with_dims_and_tensors(int "
                      "sparse_dim, int dense_dim, int[] size, Tensor indices, "
                      "Tensor values, *, ScalarType dtype, Layout layout, "
                      "Device device, bool pin_memory=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(int64_t, int64_t, at::IntArrayRef,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::TensorOptions &),
                      &ATenMLIRTypeDefault::
                          _sparse_coo_tensor_with_dims_and_tensors>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sparse_resize_(Tensor(a!) self, int[] size, "
                          "int sparse_dim, int dense_dim) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::IntArrayRef, int64_t,
                                                       int64_t),
                                          &ATenMLIRTypeDefault::sparse_resize_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::sparse_resize_and_clear_(Tensor(a!) self, int[] "
                      "size, int sparse_dim, int dense_dim) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::IntArrayRef, int64_t,
                                   int64_t),
                      &ATenMLIRTypeDefault::sparse_resize_and_clear_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::sparse_mask(Tensor self, Tensor mask) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::sparse_mask>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to_dense(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::to_dense>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to_dense_backward(Tensor grad, Tensor input) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::to_dense_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sparse_dim(Tensor self) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &),
                                          &ATenMLIRTypeDefault::sparse_dim>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_dimI(Tensor self) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &),
                                          &ATenMLIRTypeDefault::_dimI>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::dense_dim(Tensor self) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &),
                                          &ATenMLIRTypeDefault::dense_dim>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_dimV(Tensor self) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &),
                                          &ATenMLIRTypeDefault::_dimV>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_nnz(Tensor self) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &),
                                          &ATenMLIRTypeDefault::_nnz>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::coalesce(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::coalesce>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_coalesced(Tensor self) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_coalesced>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_indices(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::_indices>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_values(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::_values>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_coalesced_(Tensor(a!) self, bool coalesced) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_coalesced_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::indices(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::indices>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::values(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::values>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hspmm.out(Tensor mat1, Tensor mat2, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::hspmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::hspmm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::copy_sparse_to_sparse_(Tensor(a!) self, "
                          "Tensor src, bool non_blocking=False) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::copy_sparse_to_sparse_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::unbind.int(Tensor(a) self, int dim=0) -> "
                          "Tensor(a)[]")
                  .impl_unboxedOnlyKernel<std::vector<at::Tensor>(
                                              const at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::unbind>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to_sparse.sparse_dim(Tensor self, int "
                          "sparse_dim) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::to_sparse>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to_sparse(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::to_sparse>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to_mkldnn(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::to_mkldnn>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_reorder_conv2d_weight(Tensor self, "
                          "int[2] padding=0, int[2] stride=1, int[2] "
                          "dilation=1, int groups=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, int64_t),
                      &ATenMLIRTypeDefault::mkldnn_reorder_conv2d_weight>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to_mkldnn_backward(Tensor grad, Tensor input) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::to_mkldnn_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantize_linear(Tensor self, float scale, int "
                          "zero_point, ScalarType dtype) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, double, int64_t,
                                 at::ScalarType),
                      &ATenMLIRTypeDefault::quantize_linear>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantize_linear_per_channel(Tensor self, "
                          "Tensor scales, Tensor zero_points, int[] axis, "
                          "ScalarType dtype) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::ScalarType),
                      &ATenMLIRTypeDefault::quantize_linear_per_channel>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::dequantize(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::dequantize>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_dequantize_linear(Tensor self, float scale, "
                          "int zero_point, ScalarType dtype) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, double, int64_t,
                                 at::ScalarType),
                      &ATenMLIRTypeDefault::_dequantize_linear>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::q_scale(Tensor self) -> float")
                  .impl_unboxedOnlyKernel<double(const at::Tensor &),
                                          &ATenMLIRTypeDefault::q_scale>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::q_zero_point(Tensor self) -> int")
                  .impl_unboxedOnlyKernel<int64_t(const at::Tensor &),
                                          &ATenMLIRTypeDefault::q_zero_point>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::q_per_channel_scales(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::q_per_channel_scales>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::q_per_channel_zero_points(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &),
                      &ATenMLIRTypeDefault::q_per_channel_zero_points>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::int_repr(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::int_repr>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_per_tensor_affine_qtensor(Tensor self, float "
                          "scale, int zero_point) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, double, int64_t),
                      &ATenMLIRTypeDefault::_per_tensor_affine_qtensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_per_channel_affine_qtensor(Tensor self, Tensor "
                      "scale, Tensor zero_point, int[] axis) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::_per_channel_affine_qtensor>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::qscheme(Tensor self) -> QScheme")
                  .impl_unboxedOnlyKernel<at::QScheme(const at::Tensor &),
                                          &ATenMLIRTypeDefault::qscheme>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fake_quantize_per_tensor_affine(Tensor self, "
                          "float scale, int zero_point, int quant_min, int "
                          "quant_max) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, double, int64_t, int64_t,
                                 int64_t),
                      &ATenMLIRTypeDefault::fake_quantize_per_tensor_affine>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fake_quantize_per_tensor_affine_backward("
                          "Tensor grad, Tensor self, float scale, int "
                          "zero_point, int quant_min, int quant_max) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &, double,
                                 int64_t, int64_t, int64_t),
                      &ATenMLIRTypeDefault::
                          fake_quantize_per_tensor_affine_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::to.dtype_layout(Tensor self, *, ScalarType dtype, "
                      "Layout layout, Device device, bool pin_memory=False, "
                      "bool non_blocking=False, bool copy=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::TensorOptions &,
                                                     bool, bool),
                                          &ATenMLIRType::to>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to.device(Tensor self, Device device, "
                          "ScalarType dtype, bool non_blocking=False, bool "
                          "copy=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, c10::Device,
                                 at::ScalarType, bool, bool),
                      &ATenMLIRType::to>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to.dtype(Tensor self, ScalarType dtype, bool "
                          "non_blocking=False, bool copy=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::ScalarType, bool,
                                                     bool),
                                          &ATenMLIRType::to>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::to.other(Tensor self, Tensor other, bool "
                          "non_blocking=False, bool copy=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &, bool,
                                                     bool),
                                          &ATenMLIRType::to>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::meshgrid(Tensor[] tensors) -> Tensor[]")
                  .impl_unboxedOnlyKernel<std::vector<at::Tensor>(
                                              at::TensorList),
                                          &ATenMLIRTypeDefault::meshgrid>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cartesian_prod(Tensor[] tensors) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::TensorList),
                                          &ATenMLIRTypeDefault::cartesian_prod>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::combinations(Tensor self, int r=2, bool "
                          "with_replacement=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, bool),
                                          &ATenMLIRTypeDefault::combinations>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::item(Tensor self) -> Scalar")
                  .impl_unboxedOnlyKernel<at::Scalar(const at::Tensor &),
                                          &ATenMLIRTypeDefault::item>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_local_scalar_dense(Tensor self) -> Scalar")
                  .impl_unboxedOnlyKernel<
                      at::Scalar(const at::Tensor &),
                      &ATenMLIRTypeDefault::_local_scalar_dense>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor "
                      "hidden_gates, Tensor cx, Tensor? input_bias=None, "
                      "Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &),
                      &ATenMLIRTypeDefault::_thnn_fused_lstm_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_thnn_fused_lstm_cell_backward(Tensor? "
                          "grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, "
                          "Tensor workspace, bool has_bias) -> (Tensor, "
                          "Tensor, Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor,
                                 at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::_thnn_fused_lstm_cell_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor "
                      "hidden_gates, Tensor hx, Tensor? input_bias=None, "
                      "Tensor? hidden_bias=None) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &,
                                                         const at::Tensor &),
                      &ATenMLIRTypeDefault::_thnn_fused_gru_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, "
                          "Tensor workspace, bool has_bias) -> (Tensor, "
                          "Tensor, Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor,
                                 at::Tensor>(const at::Tensor &,
                                             const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::_thnn_fused_gru_cell_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lstm.input(Tensor input, Tensor[] hx, "
                          "Tensor[] params, bool has_biases, int num_layers, "
                          "float dropout, bool train, bool bidirectional, bool "
                          "batch_first) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, at::TensorList, at::TensorList,
                          bool, int64_t, double, bool, bool, bool),
                      &ATenMLIRTypeDefault::lstm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lstm.data(Tensor data, Tensor batch_sizes, "
                          "Tensor[] hx, Tensor[] params, bool has_biases, int "
                          "num_layers, float dropout, bool train, bool "
                          "bidirectional) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          at::TensorList, at::TensorList, bool, int64_t, double,
                          bool, bool),
                      &ATenMLIRTypeDefault::lstm>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gru.input(Tensor input, Tensor hx, Tensor[] "
                          "params, bool has_biases, int num_layers, float "
                          "dropout, bool train, bool bidirectional, bool "
                          "batch_first) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool, bool),
                                          &ATenMLIRTypeDefault::gru>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gru.data(Tensor data, Tensor batch_sizes, "
                          "Tensor hx, Tensor[] params, bool has_biases, int "
                          "num_layers, float dropout, bool train, bool "
                          "bidirectional) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool),
                                          &ATenMLIRTypeDefault::gru>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rnn_tanh.input(Tensor input, Tensor hx, "
                          "Tensor[] params, bool has_biases, int num_layers, "
                          "float dropout, bool train, bool bidirectional, bool "
                          "batch_first) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool, bool),
                                          &ATenMLIRTypeDefault::rnn_tanh>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rnn_tanh.data(Tensor data, Tensor "
                          "batch_sizes, Tensor hx, Tensor[] params, bool "
                          "has_biases, int num_layers, float dropout, bool "
                          "train, bool bidirectional) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool),
                                          &ATenMLIRTypeDefault::rnn_tanh>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rnn_relu.input(Tensor input, Tensor hx, "
                          "Tensor[] params, bool has_biases, int num_layers, "
                          "float dropout, bool train, bool bidirectional, bool "
                          "batch_first) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool, bool),
                                          &ATenMLIRTypeDefault::rnn_relu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rnn_relu.data(Tensor data, Tensor "
                          "batch_sizes, Tensor hx, Tensor[] params, bool "
                          "has_biases, int num_layers, float dropout, bool "
                          "train, bool bidirectional) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool),
                                          &ATenMLIRTypeDefault::rnn_relu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lstm_cell(Tensor input, Tensor[] hx, Tensor "
                          "w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? "
                          "b_hh=None) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          at::TensorList, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &),
                      &ATenMLIRTypeDefault::lstm_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gru_cell(Tensor input, Tensor hx, Tensor "
                          "w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? "
                          "b_hh=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::gru_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor "
                          "w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? "
                          "b_hh=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::rnn_tanh_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor "
                          "w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? "
                          "b_hh=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::rnn_relu_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_lstm(Tensor input, Tensor[] hx, "
                          "Tensor[] params, bool has_biases, int num_layers, "
                          "float dropout, bool train, bool bidirectional, bool "
                          "batch_first, *, ScalarType? dtype=None, bool "
                          "use_dynamic=False) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, at::TensorList, at::TensorList,
                          bool, int64_t, double, bool, bool, bool,
                          c10::optional<at::ScalarType>, bool),
                      &ATenMLIRTypeDefault::quantized_lstm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_gru.input(Tensor input, Tensor hx, "
                          "Tensor[] params, bool has_biases, int num_layers, "
                          "float dropout, bool train, bool bidirectional, bool "
                          "batch_first) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool, bool),
                                          &ATenMLIRTypeDefault::quantized_gru>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_gru.data(Tensor data, Tensor "
                          "batch_sizes, Tensor hx, Tensor[] params, bool "
                          "has_biases, int num_layers, float dropout, bool "
                          "train, bool bidirectional) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              at::TensorList, bool, int64_t,
                                              double, bool, bool),
                                          &ATenMLIRTypeDefault::quantized_gru>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_lstm_cell(Tensor input, Tensor[] "
                          "hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor "
                          "b_hh, Tensor packed_ih, Tensor packed_hh, Tensor "
                          "col_offsets_ih, Tensor col_offsets_hh, Scalar "
                          "scale_ih, Scalar scale_hh, Scalar zero_point_ih, "
                          "Scalar zero_point_hh) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          at::TensorList, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::Scalar, at::Scalar,
                          at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::quantized_lstm_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_gru_cell(Tensor input, Tensor hx, "
                          "Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, "
                          "Tensor packed_ih, Tensor packed_hh, Tensor "
                          "col_offsets_ih, Tensor col_offsets_hh, Scalar "
                          "scale_ih, Scalar scale_hh, Scalar zero_point_ih, "
                          "Scalar zero_point_hh) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, at::Scalar,
                                 at::Scalar),
                      &ATenMLIRTypeDefault::quantized_gru_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_rnn_relu_cell(Tensor input, Tensor "
                          "hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor "
                          "b_hh, Tensor packed_ih, Tensor packed_hh, Tensor "
                          "col_offsets_ih, Tensor col_offsets_hh, Scalar "
                          "scale_ih, Scalar scale_hh, Scalar zero_point_ih, "
                          "Scalar zero_point_hh) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, at::Scalar,
                                 at::Scalar),
                      &ATenMLIRTypeDefault::quantized_rnn_relu_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::quantized_rnn_tanh_cell(Tensor input, Tensor "
                          "hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor "
                          "b_hh, Tensor packed_ih, Tensor packed_hh, Tensor "
                          "col_offsets_ih, Tensor col_offsets_hh, Scalar "
                          "scale_ih, Scalar scale_hh, Scalar zero_point_ih, "
                          "Scalar zero_point_hh) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, at::Scalar,
                                 at::Scalar),
                      &ATenMLIRTypeDefault::quantized_rnn_tanh_cell>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_pack_padded_sequence(Tensor input, Tensor "
                          "lengths, bool batch_first) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         bool),
                      &ATenMLIRTypeDefault::_pack_padded_sequence>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_pack_padded_sequence_backward(Tensor grad, "
                          "int[] input_size, Tensor batch_sizes, bool "
                          "batch_first) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::_pack_padded_sequence_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_pad_packed_sequence(Tensor data, Tensor "
                      "batch_sizes, bool batch_first, Scalar padding_value, "
                      "int total_length) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         bool, at::Scalar,
                                                         int64_t),
                      &ATenMLIRTypeDefault::_pad_packed_sequence>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::set_.source_Storage(Tensor(a!) self, Storage "
                          "source) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Storage),
                      &ATenMLIRTypeDefault::set_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::set_.source_Storage_storage_offset(Tensor(a!) "
                          "self, Storage source, int storage_offset, int[] "
                          "size, int[] stride=[]) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Storage, int64_t,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::set_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::set_.source_Tensor(Tensor(a!) self, Tensor "
                          "source) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::set_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::set_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::set_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::set_quantizer_(Tensor(a!) self, "
                          "ConstQuantizerPtr quantizer) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::ConstQuantizerPtr),
                                          &ATenMLIRTypeDefault::set_quantizer_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::is_set_to(Tensor self, Tensor tensor) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &,
                                               const at::Tensor &),
                                          &ATenMLIRTypeDefault::is_set_to>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::masked_fill_.Scalar(Tensor(a!) self, Tensor "
                          "mask, Scalar value) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::masked_fill_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::masked_fill.Scalar(Tensor self, Tensor mask, "
                          "Scalar value) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::masked_fill>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::masked_fill_.Tensor(Tensor(a!) self, Tensor "
                          "mask, Tensor value) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::masked_fill_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::masked_fill.Tensor(Tensor self, Tensor mask, "
                          "Tensor value) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::masked_fill>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::masked_scatter_(Tensor(a!) self, Tensor mask, "
                          "Tensor source) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::masked_scatter_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::masked_scatter(Tensor self, Tensor mask, "
                          "Tensor source) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::masked_scatter>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::view(Tensor(a) self, int[] size) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRType::view>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::put_(Tensor(a!) self, Tensor index, Tensor "
                          "source, bool accumulate=False) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::put_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_add_(Tensor(a!) self, int dim, Tensor "
                          "index, Tensor source) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::index_add_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_add(Tensor self, int dim, Tensor index, "
                          "Tensor source) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::index_add>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_fill_.Scalar(Tensor(a!) self, int dim, "
                          "Tensor index, Scalar value) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::index_fill_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_fill.Scalar(Tensor self, int dim, "
                          "Tensor index, Scalar value) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &, int64_t,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::index_fill>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_fill_.Tensor(Tensor(a!) self, int dim, "
                          "Tensor index, Tensor value) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::index_fill_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_fill.Tensor(Tensor self, int dim, "
                          "Tensor index, Tensor value) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::index_fill>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::scatter_.src(Tensor(a!) self, int dim, Tensor "
                          "index, Tensor src) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::scatter_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::scatter.src(Tensor self, int dim, Tensor "
                          "index, Tensor src) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::scatter>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::scatter_.value(Tensor(a!) self, int dim, "
                          "Tensor index, Scalar value) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::scatter_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::scatter.value(Tensor self, int dim, Tensor "
                          "index, Scalar value) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &, int64_t,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::scatter>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::scatter_add_(Tensor(a!) self, int dim, Tensor "
                          "index, Tensor src) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::scatter_add_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::scatter_add(Tensor self, int dim, Tensor "
                          "index, Tensor src) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::scatter_add>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lt_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::lt_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lt_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::lt_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gt_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::gt_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gt_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::gt_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::le_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::le_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::le_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::le_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::ge_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::ge_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eq_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::eq_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eq_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::eq_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::ne_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::ne_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__and__.Scalar(Tensor self, Scalar other) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::__and__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__and__.Tensor(Tensor self, Tensor other) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::__and__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__iand__.Scalar(Tensor(a!) self, Scalar "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::__iand__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__iand__.Tensor(Tensor(a!) self, Tensor "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::__iand__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__or__.Scalar(Tensor self, Scalar other) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::__or__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__or__.Tensor(Tensor self, Tensor other) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::__or__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__ior__.Scalar(Tensor(a!) self, Scalar other) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::__ior__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__ior__.Tensor(Tensor(a!) self, Tensor other) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::__ior__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__xor__.Scalar(Tensor self, Scalar other) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::__xor__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__xor__.Tensor(Tensor self, Tensor other) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::__xor__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__ixor__.Scalar(Tensor(a!) self, Scalar "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::__ixor__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__ixor__.Tensor(Tensor(a!) self, Tensor "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::__ixor__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__lshift__.Scalar(Tensor self, Scalar other) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::__lshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__lshift__.Tensor(Tensor self, Tensor other) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::__lshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__ilshift__.Scalar(Tensor(a!) self, Scalar "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::__ilshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__ilshift__.Tensor(Tensor(a!) self, Tensor "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::__ilshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__rshift__.Scalar(Tensor self, Scalar other) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::__rshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__rshift__.Tensor(Tensor self, Tensor other) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::__rshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__irshift__.Scalar(Tensor(a!) self, Scalar "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::__irshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::__irshift__.Tensor(Tensor(a!) self, Tensor "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::__irshift__>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lgamma_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::lgamma_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::atan2_(Tensor(a!) self, Tensor other) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::atan2_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tril_(Tensor(a!) self, int diagonal=0) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::tril_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::triu_(Tensor(a!) self, int diagonal=0) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::triu_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::digamma_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::digamma_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::polygamma_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::renorm_(Tensor(a!) self, Scalar p, int dim, "
                          "Scalar maxnorm) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       int64_t, at::Scalar),
                                          &ATenMLIRTypeDefault::renorm_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::pow_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::pow_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lerp_.Scalar(Tensor(a!) self, Tensor end, "
                          "Scalar weight) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::lerp_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lerp_.Tensor(Tensor(a!) self, Tensor end, "
                          "Tensor weight) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::lerp_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fmod_.Scalar(Tensor(a!) self, Scalar other) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::fmod_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fmod_.Tensor(Tensor(a!) self, Tensor other) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::fmod_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::remainder_.Scalar(Tensor(a!) self, Scalar "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::remainder_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::remainder_.Tensor(Tensor(a!) self, Tensor "
                          "other) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::remainder_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor "
                      "batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addbmm_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addbmm.out(Tensor self, Tensor batch1, Tensor "
                          "batch2, *, Scalar beta=1, Scalar alpha=1, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addbmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addbmm(Tensor self, Tensor batch1, Tensor "
                          "batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::addbmm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addcdiv_(Tensor(a!) self, Tensor tensor1, "
                          "Tensor tensor2, *, Scalar value=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::addcdiv_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::random_.from(Tensor(a!) self, int from, int "
                          "to, *, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       int64_t,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::random_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::random_.to(Tensor(a!) self, int to, *, "
                          "Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::random_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::random_(Tensor(a!) self, *, Generator? "
                          "generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::random_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::uniform_(Tensor(a!) self, float from=0, float "
                          "to=1, *, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       double, at::Generator *),
                                          &ATenMLIRTypeDefault::uniform_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal_(Tensor(a!) self, float mean=0, float "
                          "std=1, *, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       double, at::Generator *),
                                          &ATenMLIRTypeDefault::normal_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::cauchy_(Tensor(a!) self, float median=0, float "
                      "sigma=1, *, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       double, at::Generator *),
                                          &ATenMLIRTypeDefault::cauchy_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::log_normal_(Tensor(a!) self, float mean=1, float "
                      "std=2, *, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       double, at::Generator *),
                                          &ATenMLIRTypeDefault::log_normal_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::exponential_(Tensor(a!) self, float lambd=1, "
                          "*, Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::exponential_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::geometric_(Tensor(a!) self, float p, *, "
                          "Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::geometric_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::diag.out(Tensor self, int diagonal=0, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::diag_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::diag(Tensor self, int diagonal=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::diag>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cross.out(Tensor self, Tensor other, int? "
                          "dim=None, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::cross_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cross(Tensor self, Tensor other, int? "
                          "dim=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     c10::optional<int64_t>),
                                          &ATenMLIRTypeDefault::cross>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::triu.out(Tensor self, int diagonal=0, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::triu_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::triu(Tensor self, int diagonal=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::triu>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tril.out(Tensor self, int diagonal=0, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::tril_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tril(Tensor self, int diagonal=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::tril>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::tril_indices(int row, int col, int offset=0, *, "
                      "ScalarType? dtype=long, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, int64_t, int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::tril_indices>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::triu_indices(int row, int col, int offset=0, *, "
                      "ScalarType? dtype=long, Layout? layout=None, Device? "
                      "device=None, bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t, int64_t, int64_t,
                                                     const at::TensorOptions &),
                                          &ATenMLIRTypeDefault::triu_indices>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::trace(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::trace>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ne.Scalar_out(Tensor self, Scalar other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::ne_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::ne.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::ne>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ne.Tensor_out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::ne_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::ne.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::ne>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eq.Scalar_out(Tensor self, Scalar other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::eq_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::eq.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::eq>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eq.Tensor_out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::eq_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::eq.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::eq>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ge.Scalar_out(Tensor self, Scalar other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::ge_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::ge.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::ge>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ge.Tensor_out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::ge_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::ge.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::ge>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::le.Scalar_out(Tensor self, Scalar other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::le_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::le.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::le>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::le.Tensor_out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::le_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::le.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::le>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gt.Scalar_out(Tensor self, Scalar other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::gt_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::gt.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::gt>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gt.Tensor_out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::gt_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::gt.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::gt>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lt.Scalar_out(Tensor self, Scalar other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::lt_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::lt.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::lt>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lt.Tensor_out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::lt_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::lt.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::lt>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::take.out(Tensor self, Tensor index, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::take_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::take(Tensor self, Tensor index) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::take>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_select.out(Tensor self, int dim, Tensor "
                          "index, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::index_select_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::index_select(Tensor self, int dim, Tensor "
                          "index) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::index_select>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::masked_select.out(Tensor self, Tensor mask, "
                          "*, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::masked_select_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::masked_select(Tensor self, Tensor mask) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::masked_select>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nonzero.out(Tensor self, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::nonzero_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nonzero(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::nonzero>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nonzero_numpy(Tensor self) -> Tensor[]")
                  .impl_unboxedOnlyKernel<std::vector<at::Tensor>(
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::nonzero_numpy>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::gather.out(Tensor self, int dim, Tensor index, *, "
                      "bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t,
                                   const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::gather_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::gather(Tensor self, int dim, Tensor index, *, "
                          "bool sparse_grad=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t,
                                                     const at::Tensor &, bool),
                                          &ATenMLIRType::gather>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_gather_sparse_backward(Tensor self, int dim, "
                          "Tensor index, Tensor grad) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t,
                                 const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::_gather_sparse_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addcmul.out(Tensor self, Tensor tensor1, "
                          "Tensor tensor2, *, Scalar value=1, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::addcmul_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addcmul(Tensor self, Tensor tensor1, Tensor "
                          "tensor2, *, Scalar value=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::addcmul>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addcmul_(Tensor(a!) self, Tensor tensor1, "
                          "Tensor tensor2, *, Scalar value=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::addcmul_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addcdiv.out(Tensor self, Tensor tensor1, "
                          "Tensor tensor2, *, Scalar value=1, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::addcdiv_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::addcdiv(Tensor self, Tensor tensor1, Tensor "
                          "tensor2, *, Scalar value=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::addcdiv>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::lstsq.X(Tensor self, Tensor A, *, Tensor(a!) X, "
                      "Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          const at::Tensor &),
                      &ATenMLIRTypeDefault::lstsq_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lstsq(Tensor self, Tensor A) -> (Tensor "
                          "solution, Tensor QR)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::lstsq>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::triangular_solve.X(Tensor self, Tensor A, bool "
                      "upper=True, bool transpose=False, bool "
                      "unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> "
                      "(Tensor(a!) solution, Tensor(b!) cloned_coefficient)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          const at::Tensor &, bool, bool, bool),
                      &ATenMLIRTypeDefault::triangular_solve_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::triangular_solve(Tensor self, Tensor A, bool "
                          "upper=True, bool transpose=False, bool "
                          "unitriangular=False) -> (Tensor solution, Tensor "
                          "cloned_coefficient)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         bool, bool, bool),
                      &ATenMLIRTypeDefault::triangular_solve>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_triangular_solve_helper(Tensor self, Tensor "
                          "A, bool upper, bool transpose, bool unitriangular) "
                          "-> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         bool, bool, bool),
                      &ATenMLIRTypeDefault::_triangular_solve_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::symeig.e(Tensor self, bool eigenvectors=False, "
                      "bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> "
                      "(Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &, bool,
                          bool),
                      &ATenMLIRTypeDefault::symeig_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::symeig(Tensor self, bool eigenvectors=False, "
                          "bool upper=True) -> (Tensor eigenvalues, Tensor "
                          "eigenvectors)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool, bool),
                                          &ATenMLIRTypeDefault::symeig>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_symeig_helper(Tensor self, bool "
                          "eigenvectors, bool upper) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool, bool),
                                          &ATenMLIRTypeDefault::_symeig_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eig.e(Tensor self, bool eigenvectors=False, "
                          "*, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) "
                          "eigenvalues, Tensor(b!) eigenvectors)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::eig_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::eig(Tensor self, bool eigenvectors=False) -> "
                          "(Tensor eigenvalues, Tensor eigenvectors)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::eig>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::svd.U(Tensor self, bool some=True, bool "
                          "compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, "
                          "Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, "
                          "Tensor(c!) V)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, at::Tensor &,
                          const at::Tensor &, bool, bool),
                      &ATenMLIRTypeDefault::svd_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::svd(Tensor self, bool some=True, bool "
                          "compute_uv=True) -> (Tensor U, Tensor S, Tensor V)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, bool, bool),
                      &ATenMLIRTypeDefault::svd>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_svd_helper(Tensor self, bool some, bool "
                          "compute_uv) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, bool, bool),
                      &ATenMLIRTypeDefault::_svd_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cholesky.out(Tensor self, bool upper=False, "
                          "*, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::cholesky_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::cholesky(Tensor self, bool upper=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::cholesky>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cholesky_helper(Tensor self, bool upper) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::_cholesky_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::cholesky_solve.out(Tensor self, Tensor input2, "
                      "bool upper=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::cholesky_solve_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cholesky_solve(Tensor self, Tensor input2, "
                          "bool upper=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::cholesky_solve>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cholesky_solve_helper(Tensor self, Tensor A, "
                          "bool upper) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::_cholesky_solve_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::solve(Tensor self, Tensor A) -> (Tensor "
                          "solution, Tensor LU)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::solve>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::solve.solution(Tensor self, Tensor A, *, "
                          "Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) "
                          "solution, Tensor(b!) LU)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          const at::Tensor &),
                      &ATenMLIRTypeDefault::solve_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_solve_helper(Tensor self, Tensor A) -> "
                          "(Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::_solve_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cholesky_inverse.out(Tensor self, bool "
                          "upper=False, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::cholesky_inverse_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::cholesky_inverse(Tensor self, bool "
                          "upper=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::cholesky_inverse>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) "
                      "Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &, bool),
                      &ATenMLIRTypeDefault::qr_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::qr(Tensor self, bool some=True) -> (Tensor Q, "
                          "Tensor R)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::qr>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_qr_helper(Tensor self, bool some) -> "
                          "(Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_qr_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::geqrf.a(Tensor self, *, Tensor(a!) a, "
                          "Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::geqrf_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &),
                                          &ATenMLIRTypeDefault::geqrf>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::orgqr.out(Tensor self, Tensor input2, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::orgqr_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::orgqr(Tensor self, Tensor input2) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::orgqr>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::ormqr.out(Tensor self, Tensor input2, Tensor "
                          "input3, bool left=True, bool transpose=False, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &, bool, bool),
                                          &ATenMLIRTypeDefault::ormqr_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::ormqr(Tensor self, Tensor input2, Tensor input3, "
                      "bool left=True, bool transpose=False) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, bool, bool),
                      &ATenMLIRTypeDefault::ormqr>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_lu_with_info(Tensor self, bool pivot=True, "
                          "bool check_errors=True) -> (Tensor, Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, bool, bool),
                      &ATenMLIRTypeDefault::_lu_with_info>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lu_solve.out(Tensor self, Tensor LU_data, "
                          "Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::lu_solve_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lu_solve(Tensor self, Tensor LU_data, Tensor "
                          "LU_pivots) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::lu_solve>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_lu_solve_helper(Tensor self, Tensor LU_data, "
                          "Tensor LU_pivots) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::_lu_solve_helper>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multinomial.out(Tensor self, int num_samples, "
                          "bool replacement=False, *, Generator? "
                          "generator=None, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t,
                                   bool, at::Generator *),
                      &ATenMLIRTypeDefault::multinomial_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multinomial(Tensor self, int num_samples, "
                          "bool replacement=False, *, Generator? "
                          "generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, bool,
                                                     at::Generator *),
                                          &ATenMLIRTypeDefault::multinomial>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_multinomial_alias_setup(Tensor probs) -> "
                          "(Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &),
                      &ATenMLIRTypeDefault::_multinomial_alias_setup>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_multinomial_alias_draw(Tensor J, Tensor q, int "
                      "num_samples, *, Generator? generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t, at::Generator *),
                      &ATenMLIRTypeDefault::_multinomial_alias_draw>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::lgamma_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lgamma(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::lgamma>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::digamma.out(Tensor self, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::digamma_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::digamma(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::digamma>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::polygamma.out(int n, Tensor self, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::polygamma_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::polygamma(int n, Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(int64_t,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::polygamma>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erfinv(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::erfinv>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erfinv_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::erfinv_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::erfinv_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sign(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::sign>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sign_(Tensor(a!) self) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &),
                                          &ATenMLIRTypeDefault::sign_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sign.out(Tensor self, *, Tensor(a!) out) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::sign_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::dist(Tensor self, Tensor other, Scalar p=2) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::dist>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::atan2.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::atan2_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::atan2(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::atan2>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lerp.Scalar_out(Tensor self, Tensor end, "
                          "Scalar weight, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &, at::Scalar),
                                          &ATenMLIRTypeDefault::lerp_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lerp.Tensor_out(Tensor self, Tensor end, "
                          "Tensor weight, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::lerp_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lerp.Scalar(Tensor self, Tensor end, Scalar "
                          "weight) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::lerp>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::lerp.Tensor(Tensor self, Tensor end, Tensor "
                          "weight) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::lerp>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::histc.out(Tensor self, int bins=100, Scalar "
                      "min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              int64_t, at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::histc_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::histc(Tensor self, int bins=100, Scalar "
                          "min=0, Scalar max=0) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, at::Scalar,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::histc>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fmod.Scalar_out(Tensor self, Scalar other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::fmod_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::fmod>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fmod.Tensor_out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::fmod_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::fmod>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::remainder.Scalar_out(Tensor self, Scalar "
                          "other, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::remainder_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::remainder.Scalar(Tensor self, Scalar other) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::remainder>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::remainder.Tensor_out(Tensor self, Tensor "
                          "other, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::remainder_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::remainder.Tensor(Tensor self, Tensor other) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::remainder>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::min.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::min_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::min.other(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::min>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::min(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::min>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max.out(Tensor self, Tensor other, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::max_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max.other(Tensor self, Tensor other) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::max>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::max>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::median(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::median>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sort.values(Tensor self, int dim=-1, bool "
                          "descending=False, *, Tensor(a!) values, Tensor(b!) "
                          "indices) -> (Tensor(a!) values, Tensor(b!) indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::sort_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::sort(Tensor self, int dim=-1, bool "
                      "descending=False) -> (Tensor values, Tensor indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         int64_t, bool),
                      &ATenMLIRTypeDefault::sort>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::argsort(Tensor self, int dim=-1, bool "
                          "descending=False) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, bool),
                                          &ATenMLIRTypeDefault::argsort>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::topk.values(Tensor self, int k, int dim=-1, "
                          "bool largest=True, bool sorted=True, *, Tensor(a!) "
                          "values, Tensor(b!) indices) ->(Tensor(a!) values, "
                          "Tensor(b!) indices)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, int64_t, bool, bool),
                      &ATenMLIRTypeDefault::topk_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::topk(Tensor self, int k, int dim=-1, bool "
                          "largest=True, bool sorted=True) -> (Tensor values, "
                          "Tensor indices)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, int64_t,
                                              int64_t, bool, bool),
                                          &ATenMLIRTypeDefault::topk>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::all(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::all>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::any(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::any>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::renorm.out(Tensor self, Scalar p, int dim, "
                          "Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::Scalar, int64_t, at::Scalar),
                                          &ATenMLIRTypeDefault::renorm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::renorm(Tensor self, Scalar p, int dim, Scalar "
                          "maxnorm) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar, int64_t,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::renorm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::unfold(Tensor(a) self, int dimension, int "
                          "size, int step) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t, int64_t, int64_t),
                                          &ATenMLIRTypeDefault::unfold>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::equal(Tensor self, Tensor other) -> bool")
                  .impl_unboxedOnlyKernel<bool(const at::Tensor &,
                                               const at::Tensor &),
                                          &ATenMLIRTypeDefault::equal>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow.Tensor_Tensor_out(Tensor self, Tensor "
                          "exponent, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::pow_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow.Tensor_Tensor(Tensor self, Tensor "
                          "exponent) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::pow>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow.Scalar_out(Scalar self, Tensor exponent, "
                          "*, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::pow_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::pow.Scalar(Scalar self, Tensor exponent) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(at::Scalar, const at::Tensor &),
                      &ATenMLIRTypeDefault::pow>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal.Tensor_float_out(Tensor mean, float "
                          "std=1, *, Generator? generator=None, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       double, at::Generator *),
                                          &ATenMLIRTypeDefault::normal_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal.Tensor_float(Tensor mean, float std=1, "
                          "*, Generator? generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, double,
                                                     at::Generator *),
                                          &ATenMLIRTypeDefault::normal>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal.float_Tensor_out(float mean, Tensor "
                          "std, *, Generator? generator=None, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       const at::Tensor &,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::normal_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal.float_Tensor(float mean, Tensor std, "
                          "*, Generator? generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(double, const at::Tensor &,
                                                     at::Generator *),
                                          &ATenMLIRTypeDefault::normal>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal.Tensor_Tensor_out(Tensor mean, Tensor "
                          "std, *, Generator? generator=None, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Generator *),
                      &ATenMLIRTypeDefault::normal_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal.Tensor_Tensor(Tensor mean, Tensor std, "
                          "*, Generator? generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Generator *),
                                          &ATenMLIRTypeDefault::normal>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::normal.float_float(float mean, float std, int[] "
                      "size, *, Generator? generator=None, ScalarType? "
                      "dtype=None, Layout? layout=None, Device? device=None, "
                      "bool? pin_memory=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(double, double, at::IntArrayRef,
                                 at::Generator *, const at::TensorOptions &),
                      &ATenMLIRTypeDefault::normal>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::normal.float_float_out(float mean, float std, "
                          "int[] size, *, Generator? generator=None, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, double,
                                                       double, at::IntArrayRef,
                                                       at::Generator *),
                                          &ATenMLIRTypeDefault::normal_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::alias(Tensor(a) self) -> Tensor(a)")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::alias>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_addr(Tensor self, Tensor vec1, Tensor vec2, "
                          "*, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_addr>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, "
                      "*, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_addr_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_addr.out(Tensor self, Tensor vec1, Tensor "
                          "vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_addr_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_index_copy_(Tensor(a!) self, int dim, Tensor "
                          "index, Tensor source) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, int64_t,
                                                       const at::Tensor &,
                                                       const at::Tensor &),
                                          &ATenMLIRTypeDefault::_index_copy_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cumsum(Tensor self, int dim) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::_cumsum>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cumsum.out(Tensor self, int dim, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::_cumsum_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cumprod(Tensor self, int dim) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::_cumprod>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cumprod.out(Tensor self, int dim, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::_cumprod_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_var(Tensor self, bool unbiased=True) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_var>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_std(Tensor self, bool unbiased=True) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &, bool),
                                          &ATenMLIRTypeDefault::_std>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_addmm.out(Tensor self, Tensor mat1, Tensor "
                          "mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_addmm_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_addmm(Tensor self, Tensor mat1, Tensor mat2, "
                          "*, Scalar beta=1, Scalar alpha=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_addmm>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::_addmm_(Tensor(a!) self, Tensor mat1, Tensor "
                      "mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::_addmm_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cat(Tensor[] tensors, int dim=0) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(at::TensorList, int64_t),
                                          &ATenMLIRTypeDefault::_cat>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_cat.out(Tensor[] tensors, int dim=0, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::TensorList, int64_t),
                                          &ATenMLIRTypeDefault::_cat_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_mode(Tensor self, int dim=-1, bool "
                          "keepdim=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &, int64_t,
                                              bool),
                                          &ATenMLIRTypeDefault::_mode>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_mode.values(Tensor self, int dim=-1, bool "
                          "keepdim=False, *, Tensor(a!) values, Tensor(b!) "
                          "indices) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::_mode_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_max(Tensor self, int dim, bool "
                          "keepdim=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         int64_t, bool),
                      &ATenMLIRTypeDefault::_max>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_max.max(Tensor self, int dim, bool "
                          "keepdim=False, *, Tensor(a!) max, Tensor(b!) "
                          "max_indices) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::_max_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_min(Tensor self, int dim, bool "
                          "keepdim=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         int64_t, bool),
                      &ATenMLIRTypeDefault::_min>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_min.min(Tensor self, int dim, bool "
                          "keepdim=False, *, Tensor(a!) min, Tensor(b!) "
                          "min_indices) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          int64_t, bool),
                      &ATenMLIRTypeDefault::_min_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::binary_cross_entropy.out(Tensor self, Tensor "
                          "target, Tensor? weight=None, int reduction=Mean, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t),
                      &ATenMLIRTypeDefault::binary_cross_entropy_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::binary_cross_entropy(Tensor self, Tensor target, "
                      "Tensor? weight=None, int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::binary_cross_entropy>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::binary_cross_entropy_backward.grad_input("
                          "Tensor grad_output, Tensor self, Tensor target, "
                          "Tensor? weight=None, int reduction=Mean, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::binary_cross_entropy_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::binary_cross_entropy_backward(Tensor "
                          "grad_output, Tensor self, Tensor target, Tensor? "
                          "weight=None, int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 int64_t),
                      &ATenMLIRTypeDefault::binary_cross_entropy_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mse_loss.out(Tensor self, Tensor target, int "
                          "reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::mse_loss_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mse_loss(Tensor self, Tensor target, int "
                          "reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::mse_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mse_loss_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor target, int "
                          "reduction, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t),
                      &ATenMLIRTypeDefault::mse_loss_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mse_loss_backward(Tensor grad_output, Tensor "
                          "self, Tensor target, int reduction) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::mse_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::l1_loss.out(Tensor self, Tensor target, int "
                          "reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              const at::Tensor &, int64_t),
                                          &ATenMLIRTypeDefault::l1_loss_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::l1_loss(Tensor self, Tensor target, int "
                          "reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::l1_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::l1_loss_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor target, int "
                          "reduction, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t),
                      &ATenMLIRTypeDefault::l1_loss_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::l1_loss_backward(Tensor grad_output, Tensor "
                          "self, Tensor target, int reduction) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::l1_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::multi_margin_loss.out(Tensor self, Tensor target, "
                      "Scalar p=1, Scalar margin=1, Tensor? weight=None, int "
                      "reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar,
                                   const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::multi_margin_loss_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multi_margin_loss(Tensor self, Tensor target, "
                          "Scalar p=1, Scalar margin=1, Tensor? weight=None, "
                          "int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, const at::Tensor &,
                                 int64_t),
                      &ATenMLIRTypeDefault::multi_margin_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::multi_margin_loss_backward.grad_input(Tensor "
                      "grad_output, Tensor self, Tensor target, Scalar p, "
                      "Scalar margin, Tensor? weight=None, int reduction=Mean, "
                      "*, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar, const at::Tensor &,
                                   int64_t),
                      &ATenMLIRTypeDefault::multi_margin_loss_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::multi_margin_loss_backward(Tensor grad_output, "
                      "Tensor self, Tensor target, Scalar p, Scalar margin, "
                      "Tensor? weight=None, int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::multi_margin_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multilabel_margin_loss.out(Tensor self, "
                          "Tensor target, int reduction=Mean, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::multilabel_margin_loss_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multilabel_margin_loss(Tensor self, Tensor "
                          "target, int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t),
                      &ATenMLIRTypeDefault::multilabel_margin_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multilabel_margin_loss_forward.output(Tensor "
                          "self, Tensor target, int reduction, *, Tensor(a!) "
                          "output, Tensor(b!) is_target) -> (Tensor(a!), "
                          "Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::multilabel_margin_loss_forward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multilabel_margin_loss_forward(Tensor self, "
                          "Tensor target, int reduction) -> (Tensor output, "
                          "Tensor is_target)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         const at::Tensor &,
                                                         int64_t),
                      &ATenMLIRTypeDefault::multilabel_margin_loss_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multilabel_margin_loss_backward.grad_input("
                          "Tensor grad_output, Tensor self, Tensor target, int "
                          "reduction, Tensor is_target, *, Tensor(a!) "
                          "grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t, const at::Tensor &),
                      &ATenMLIRTypeDefault::
                          multilabel_margin_loss_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::multilabel_margin_loss_backward(Tensor "
                          "grad_output, Tensor self, Tensor target, int "
                          "reduction, Tensor is_target) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::multilabel_margin_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss.out(Tensor self, Tensor target, "
                          "Tensor? weight=None, int reduction=Mean, int "
                          "ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t, int64_t),
                      &ATenMLIRTypeDefault::nll_loss_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss(Tensor self, Tensor target, Tensor? "
                          "weight=None, int reduction=Mean, int "
                          "ignore_index=-100) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t, int64_t),
                      &ATenMLIRTypeDefault::nll_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss_forward.output(Tensor self, Tensor "
                          "target, Tensor? weight, int reduction, int "
                          "ignore_index, *, Tensor(a!) output, Tensor(b!) "
                          "total_weight) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &, int64_t,
                          int64_t),
                      &ATenMLIRTypeDefault::nll_loss_forward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss_forward(Tensor self, Tensor target, "
                          "Tensor? weight, int reduction, int ignore_index) -> "
                          "(Tensor output, Tensor total_weight)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &, int64_t,
                                              int64_t),
                                          &ATenMLIRType::nll_loss_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::nll_loss_backward.grad_input(Tensor grad_output, "
                      "Tensor self, Tensor target, Tensor? weight, int "
                      "reduction, int ignore_index, Tensor total_weight, *, "
                      "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, int64_t, int64_t,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::nll_loss_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss_backward(Tensor grad_output, Tensor "
                          "self, Tensor target, Tensor? weight, int reduction, "
                          "int ignore_index, Tensor total_weight) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, const at::Tensor &),
                      &ATenMLIRType::nll_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss2d.out(Tensor self, Tensor target, "
                          "Tensor? weight=None, int reduction=Mean, int "
                          "ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t, int64_t),
                      &ATenMLIRTypeDefault::nll_loss2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss2d(Tensor self, Tensor target, "
                          "Tensor? weight=None, int reduction=Mean, int "
                          "ignore_index=-100) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t, int64_t),
                      &ATenMLIRTypeDefault::nll_loss2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::nll_loss2d_forward.output(Tensor self, Tensor "
                          "target, Tensor? weight, int reduction, int "
                          "ignore_index, *, Tensor(a!) output, Tensor(b!) "
                          "total_weight) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &, int64_t,
                          int64_t),
                      &ATenMLIRTypeDefault::nll_loss2d_forward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::nll_loss2d_forward(Tensor self, Tensor target, "
                      "Tensor? weight, int reduction, int ignore_index) -> "
                      "(Tensor output, Tensor total_weight)")
                  .impl_unboxedOnlyKernel<std::tuple<at::Tensor, at::Tensor>(
                                              const at::Tensor &,
                                              const at::Tensor &,
                                              const at::Tensor &, int64_t,
                                              int64_t),
                                          &ATenMLIRType::nll_loss2d_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::nll_loss2d_backward.grad_input(Tensor "
                      "grad_output, Tensor self, Tensor target, Tensor? "
                      "weight, int reduction, int ignore_index, Tensor "
                      "total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, int64_t, int64_t,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::nll_loss2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::nll_loss2d_backward(Tensor grad_output, Tensor "
                      "self, Tensor target, Tensor? weight, int reduction, int "
                      "ignore_index, Tensor total_weight) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, const at::Tensor &,
                                 int64_t, int64_t, const at::Tensor &),
                      &ATenMLIRType::nll_loss2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::smooth_l1_loss.out(Tensor self, Tensor target, "
                      "int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::smooth_l1_loss_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::smooth_l1_loss(Tensor self, Tensor target, "
                          "int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::smooth_l1_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::smooth_l1_loss_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor target, int "
                          "reduction, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t),
                      &ATenMLIRTypeDefault::smooth_l1_loss_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::smooth_l1_loss_backward(Tensor grad_output, "
                      "Tensor self, Tensor target, int reduction) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::smooth_l1_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::soft_margin_loss.out(Tensor self, Tensor target, "
                      "int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::soft_margin_loss_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::soft_margin_loss(Tensor self, Tensor target, "
                          "int reduction=Mean) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 int64_t),
                      &ATenMLIRTypeDefault::soft_margin_loss>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::soft_margin_loss_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor target, int "
                          "reduction, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   int64_t),
                      &ATenMLIRTypeDefault::soft_margin_loss_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::soft_margin_loss_backward(Tensor grad_output, "
                      "Tensor self, Tensor target, int reduction) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::soft_margin_loss_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::elu.out(Tensor self, Scalar alpha=1, Scalar "
                          "scale=1, Scalar input_scale=1, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, at::Scalar,
                                   at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::elu_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::elu(Tensor self, Scalar alpha=1, Scalar "
                          "scale=1, Scalar input_scale=1) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar, at::Scalar,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::elu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::elu_backward.grad_input(Tensor grad_output, "
                      "Scalar alpha, Scalar scale, Scalar input_scale, Tensor "
                      "output, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, at::Scalar,
                                   at::Scalar, at::Scalar, const at::Tensor &),
                      &ATenMLIRTypeDefault::elu_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::elu_backward(Tensor grad_output, Scalar "
                          "alpha, Scalar scale, Scalar input_scale, Tensor "
                          "output) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar, at::Scalar,
                                 at::Scalar, const at::Tensor &),
                      &ATenMLIRTypeDefault::elu_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar "
                          "scale=1, Scalar input_scale=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &, at::Scalar,
                                                       at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::elu_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::glu.out(Tensor self, int dim=-1, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::glu_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::glu(Tensor self, int dim=-1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::glu>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::glu_backward.grad_input(Tensor grad_output, "
                          "Tensor self, int dim, *, Tensor(a!) grad_input) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, int64_t),
                      &ATenMLIRTypeDefault::glu_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::glu_backward(Tensor grad_output, Tensor self, "
                          "int dim) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     int64_t),
                                          &ATenMLIRTypeDefault::glu_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hardtanh.out(Tensor self, Scalar min_val=-1, "
                          "Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::hardtanh_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hardtanh(Tensor self, Scalar min_val=-1, "
                          "Scalar max_val=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRType::hardtanh>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hardtanh_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Scalar min_val, Scalar "
                          "max_val, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRTypeDefault::hardtanh_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hardtanh_backward(Tensor grad_output, Tensor "
                          "self, Scalar min_val, Scalar max_val) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::Scalar, at::Scalar),
                                          &ATenMLIRType::hardtanh_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, "
                          "Scalar max_val=1) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, at::Scalar, at::Scalar),
                      &ATenMLIRType::hardtanh_>(at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::leaky_relu.out(Tensor self, Scalar "
                      "negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::leaky_relu_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::leaky_relu(Tensor self, Scalar "
                          "negative_slope=0.01) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::leaky_relu>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::leaky_relu_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Scalar negative_slope, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::leaky_relu_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::leaky_relu_backward(Tensor grad_output, "
                          "Tensor self, Scalar negative_slope) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::Scalar),
                      &ATenMLIRTypeDefault::leaky_relu_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::leaky_relu_(Tensor(a!) self, Scalar "
                          "negative_slope=0.01) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::leaky_relu_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_sigmoid.out(Tensor self, *, Tensor(a!) "
                          "out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::log_sigmoid_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_sigmoid(Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &),
                                          &ATenMLIRTypeDefault::log_sigmoid>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_sigmoid_forward.output(Tensor self, *, "
                          "Tensor(a!) output, Tensor(b!) buffer) -> "
                          "(Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::log_sigmoid_forward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_sigmoid_forward(Tensor self) -> (Tensor "
                          "output, Tensor buffer)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &),
                      &ATenMLIRTypeDefault::log_sigmoid_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_sigmoid_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor buffer, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::log_sigmoid_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::log_sigmoid_backward(Tensor grad_output, "
                          "Tensor self, Tensor buffer) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::log_sigmoid_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::rrelu_with_noise.out(Tensor self, Tensor noise, "
                      "Scalar lower=0.125, Scalar upper=0.3333333333333333, "
                      "bool training=False, Generator? generator=None, *, "
                      "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar,
                                   bool, at::Generator *),
                      &ATenMLIRTypeDefault::rrelu_with_noise_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rrelu_with_noise(Tensor self, Tensor noise, "
                          "Scalar lower=0.125, Scalar "
                          "upper=0.3333333333333333, bool training=False, "
                          "Generator? generator=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, bool, at::Generator *),
                      &ATenMLIRTypeDefault::rrelu_with_noise>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rrelu_with_noise_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor noise, Scalar "
                          "lower, Scalar upper, bool training, *, Tensor(a!) "
                          "grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::Scalar, at::Scalar, bool),
                      &ATenMLIRTypeDefault::rrelu_with_noise_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rrelu_with_noise_backward(Tensor grad_output, "
                          "Tensor self, Tensor noise, Scalar lower, Scalar "
                          "upper, bool training) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::Scalar, at::Scalar,
                                 bool),
                      &ATenMLIRTypeDefault::rrelu_with_noise_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::rrelu_with_noise_(Tensor(a!) self, Tensor "
                          "noise, Scalar lower=0.125, Scalar "
                          "upper=0.3333333333333333, bool training=False, "
                          "Generator? generator=None) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &, at::Scalar,
                                   at::Scalar, bool, at::Generator *),
                      &ATenMLIRTypeDefault::rrelu_with_noise_>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::softplus.out(Tensor self, Scalar beta=1, Scalar "
                      "threshold=20, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::softplus_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::softplus(Tensor self, Scalar beta=1, Scalar "
                          "threshold=20) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar, at::Scalar),
                                          &ATenMLIRTypeDefault::softplus>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::softplus_backward.grad_input(Tensor grad_output, "
                      "Tensor self, Scalar beta, Scalar threshold, Tensor "
                      "output, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar, at::Scalar,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::softplus_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::softplus_backward(Tensor grad_output, Tensor "
                          "self, Scalar beta, Scalar threshold, Tensor output) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::Scalar, at::Scalar, const at::Tensor &),
                      &ATenMLIRTypeDefault::softplus_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::softshrink.out(Tensor self, Scalar lambd=0.5, "
                          "*, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(at::Tensor &,
                                                       const at::Tensor &,
                                                       at::Scalar),
                                          &ATenMLIRTypeDefault::softshrink_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::softshrink(Tensor self, Scalar lambd=0.5) -> "
                          "Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::Scalar),
                                          &ATenMLIRTypeDefault::softshrink>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::softshrink_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Scalar lambd, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::Scalar),
                      &ATenMLIRTypeDefault::softshrink_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::softshrink_backward(Tensor grad_output, "
                          "Tensor self, Scalar lambd) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::Scalar),
                      &ATenMLIRTypeDefault::softshrink_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_avg_pool2d.out(Tensor self, int[2] "
                          "output_size, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_avg_pool2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_avg_pool2d(Tensor self, int[2] "
                          "output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_avg_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::mkldnn_adaptive_avg_pool2d(Tensor self, "
                          "int[2] output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::mkldnn_adaptive_avg_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_adaptive_avg_pool2d(Tensor self, int[2] "
                          "output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRType::_adaptive_avg_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::_adaptive_avg_pool2d_backward(Tensor "
                          "grad_output, Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRType::_adaptive_avg_pool2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_avg_pool3d.out(Tensor self, int[3] "
                          "output_size, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_avg_pool3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_avg_pool3d(Tensor self, int[3] "
                          "output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_avg_pool3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_avg_pool3d_backward.grad_input("
                          "Tensor grad_output, Tensor self, *, Tensor(a!) "
                          "grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::adaptive_avg_pool3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_avg_pool3d_backward(Tensor "
                          "grad_output, Tensor self) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::adaptive_avg_pool3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool2d.out(Tensor self, int[2] "
                          "output_size, *, Tensor(a!) out, Tensor(b!) indices) "
                          "-> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_max_pool2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool2d(Tensor self, int[2] "
                          "output_size) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_max_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool2d_backward.grad_input("
                          "Tensor grad_output, Tensor self, Tensor indices, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::adaptive_max_pool2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool2d_backward(Tensor "
                          "grad_output, Tensor self, Tensor indices) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::adaptive_max_pool2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool3d.out(Tensor self, int[3] "
                          "output_size, *, Tensor(a!) out, Tensor(b!) indices) "
                          "-> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_max_pool3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool3d(Tensor self, int[3] "
                          "output_size) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(const at::Tensor &,
                                                         at::IntArrayRef),
                      &ATenMLIRTypeDefault::adaptive_max_pool3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool3d_backward.grad_input("
                          "Tensor grad_output, Tensor self, Tensor indices, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::adaptive_max_pool3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::adaptive_max_pool3d_backward(Tensor "
                          "grad_output, Tensor self, Tensor indices) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::adaptive_max_pool3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::avg_pool2d.out(Tensor self, int[2] kernel_size, "
                      "int[2] stride=[], int[2] padding=0, bool "
                      "ceil_mode=False, bool count_include_pad=True, int? "
                      "divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, at::IntArrayRef,
                                              at::IntArrayRef, bool, bool,
                                              c10::optional<int64_t>),
                                          &ATenMLIRTypeDefault::avg_pool2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::avg_pool2d(Tensor self, int[2] kernel_size, "
                          "int[2] stride=[], int[2] padding=0, bool "
                          "ceil_mode=False, bool count_include_pad=True, int? "
                          "divisor_override=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool, bool,
                                 c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::avg_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::avg_pool2d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, int[2] kernel_size, "
                          "int[2] stride, int[2] padding, bool ceil_mode, bool "
                          "count_include_pad, int? divisor_override, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef, bool, bool,
                                   c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::avg_pool2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::avg_pool2d_backward(Tensor grad_output, "
                          "Tensor self, int[2] kernel_size, int[2] stride, "
                          "int[2] padding, bool ceil_mode, bool "
                          "count_include_pad, int? divisor_override) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, bool, bool,
                                 c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::avg_pool2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::avg_pool3d.out(Tensor self, int[3] kernel_size, "
                      "int[3] stride=[], int[3] padding=0, bool "
                      "ceil_mode=False, bool count_include_pad=True, int? "
                      "divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, at::IntArrayRef,
                                              at::IntArrayRef, bool, bool,
                                              c10::optional<int64_t>),
                                          &ATenMLIRTypeDefault::avg_pool3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::avg_pool3d(Tensor self, int[3] kernel_size, "
                          "int[3] stride=[], int[3] padding=0, bool "
                          "ceil_mode=False, bool count_include_pad=True, int? "
                          "divisor_override=None) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool, bool,
                                 c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::avg_pool3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::avg_pool3d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, int[3] kernel_size, "
                          "int[3] stride, int[3] padding, bool ceil_mode, bool "
                          "count_include_pad, int? divisor_override, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef, bool, bool,
                                   c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::avg_pool3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::avg_pool3d_backward(Tensor grad_output, "
                          "Tensor self, int[3] kernel_size, int[3] stride, "
                          "int[3] padding, bool ceil_mode, bool "
                          "count_include_pad, int? divisor_override) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, bool, bool,
                                 c10::optional<int64_t>),
                      &ATenMLIRTypeDefault::avg_pool3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool2d.output(Tensor self, "
                          "int[2] kernel_size, int[2] output_size, Tensor "
                          "random_samples, *, Tensor(a!) output, Tensor(b!) "
                          "indices) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool2d(Tensor self, int[2] "
                          "kernel_size, int[2] output_size, Tensor "
                          "random_samples) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool2d_backward.grad_input("
                          "Tensor grad_output, Tensor self, int[2] "
                          "kernel_size, int[2] output_size, Tensor indices, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool2d_backward(Tensor "
                          "grad_output, Tensor self, int[2] kernel_size, "
                          "int[2] output_size, Tensor indices) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool3d.output(Tensor self, "
                          "int[3] kernel_size, int[3] output_size, Tensor "
                          "random_samples, *, Tensor(a!) output, Tensor(b!) "
                          "indices) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool3d(Tensor self, int[3] "
                          "kernel_size, int[3] output_size, Tensor "
                          "random_samples) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool3d_backward.grad_input("
                          "Tensor grad_output, Tensor self, int[3] "
                          "kernel_size, int[3] output_size, Tensor indices, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::fractional_max_pool3d_backward(Tensor "
                          "grad_output, Tensor self, int[3] kernel_size, "
                          "int[3] output_size, Tensor indices) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::fractional_max_pool3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_pool2d_with_indices.out(Tensor self, int[2] "
                      "kernel_size, int[2] stride=[], int[2] padding=0, int[2] "
                      "dilation=1, bool ceil_mode=False, *, Tensor(a!) out, "
                      "Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::max_pool2d_with_indices_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_pool2d_with_indices(Tensor self, int[2] "
                      "kernel_size, int[2] stride=[], int[2] padding=0, int[2] "
                      "dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, bool),
                      &ATenMLIRType::max_pool2d_with_indices>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_pool2d_with_indices_backward.grad_input("
                          "Tensor grad_output, Tensor self, int[2] "
                          "kernel_size, int[2] stride, int[2] padding, int[2] "
                          "dilation, bool ceil_mode, Tensor indices, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef,
                                   at::IntArrayRef, bool, const at::Tensor &),
                      &ATenMLIRTypeDefault::
                          max_pool2d_with_indices_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_pool2d_with_indices_backward(Tensor "
                          "grad_output, Tensor self, int[2] kernel_size, "
                          "int[2] stride, int[2] padding, int[2] dilation, "
                          "bool ceil_mode, Tensor indices) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool,
                                 const at::Tensor &),
                      &ATenMLIRType::max_pool2d_with_indices_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_pool3d_with_indices.out(Tensor self, int[3] "
                      "kernel_size, int[3] stride=[], int[3] padding=0, int[3] "
                      "dilation=1, bool ceil_mode=False, *, Tensor(a!) out, "
                      "Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::max_pool3d_with_indices_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_pool3d_with_indices(Tensor self, int[3] "
                      "kernel_size, int[3] stride=[], int[3] padding=0, int[3] "
                      "dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::max_pool3d_with_indices>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_pool3d_with_indices_backward.grad_input("
                          "Tensor grad_output, Tensor self, int[3] "
                          "kernel_size, int[3] stride, int[3] padding, int[3] "
                          "dilation, bool ceil_mode, Tensor indices, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef,
                                   at::IntArrayRef, bool, const at::Tensor &),
                      &ATenMLIRTypeDefault::
                          max_pool3d_with_indices_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_pool3d_with_indices_backward(Tensor "
                          "grad_output, Tensor self, int[3] kernel_size, "
                          "int[3] stride, int[3] padding, int[3] dilation, "
                          "bool ceil_mode, Tensor indices) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef, bool,
                                 const at::Tensor &),
                      &ATenMLIRTypeDefault::max_pool3d_with_indices_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_unpool2d.out(Tensor self, Tensor indices, "
                      "int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::max_unpool2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_unpool2d(Tensor self, Tensor indices, "
                          "int[2] output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &,
                                                     at::IntArrayRef),
                                          &ATenMLIRTypeDefault::max_unpool2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_unpool2d_backward.grad_input(Tensor "
                      "grad_output, Tensor self, Tensor indices, int[2] "
                      "output_size, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::max_unpool2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_unpool2d_backward(Tensor grad_output, Tensor "
                      "self, Tensor indices, int[2] output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::max_unpool2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_unpool3d.out(Tensor self, Tensor indices, "
                          "int[3] output_size, int[3] stride, int[3] padding, "
                          "*, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::max_unpool3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::max_unpool3d(Tensor self, Tensor indices, int[3] "
                      "output_size, int[3] stride, int[3] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::max_unpool3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_unpool3d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor indices, int[3] "
                          "output_size, int[3] stride, int[3] padding, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::max_unpool3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::max_unpool3d_backward(Tensor grad_output, "
                          "Tensor self, Tensor indices, int[3] output_size, "
                          "int[3] stride, int[3] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::max_unpool3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad1d.out(Tensor self, int[2] "
                          "padding, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad1d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad1d(Tensor self, int[2] padding) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad1d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, int[2] padding, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad1d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad1d_backward(Tensor grad_output, "
                          "Tensor self, int[2] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad1d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad2d.out(Tensor self, int[4] "
                          "padding, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad2d(Tensor self, int[4] padding) "
                          "-> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad2d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, int[4] padding, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::reflection_pad2d_backward(Tensor grad_output, "
                          "Tensor self, int[4] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::reflection_pad2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad1d.out(Tensor self, int[2] "
                          "padding, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad1d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad1d(Tensor self, int[2] "
                          "padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad1d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, int[2] padding, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad1d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad1d_backward(Tensor "
                          "grad_output, Tensor self, int[2] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad1d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad2d.out(Tensor self, int[4] "
                          "padding, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad2d(Tensor self, int[4] "
                          "padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad2d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, int[4] padding, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad2d_backward(Tensor "
                          "grad_output, Tensor self, int[4] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad3d.out(Tensor self, int[6] "
                          "padding, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad3d(Tensor self, int[6] "
                          "padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad3d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, int[6] padding, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::replication_pad3d_backward(Tensor "
                          "grad_output, Tensor self, int[6] padding) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::replication_pad3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_linear1d.out(Tensor self, int[1] "
                          "output_size, bool align_corners, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_linear1d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_linear1d(Tensor self, int[1] "
                          "output_size, bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_linear1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_linear1d_backward.grad_input(Tensor "
                          "grad_output, int[1] output_size, int[3] input_size, "
                          "bool align_corners, *, Tensor(a!) grad_input) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_linear1d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_linear1d_backward(Tensor "
                          "grad_output, int[1] output_size, int[3] input_size, "
                          "bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_linear1d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bilinear2d.out(Tensor self, int[2] "
                          "output_size, bool align_corners, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bilinear2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bilinear2d(Tensor self, int[2] "
                          "output_size, bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bilinear2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bilinear2d_backward.grad_input("
                          "Tensor grad_output, int[2] output_size, int[4] "
                          "input_size, bool align_corners, *, Tensor(a!) "
                          "grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bilinear2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bilinear2d_backward(Tensor "
                          "grad_output, int[2] output_size, int[4] input_size, "
                          "bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bilinear2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bicubic2d.out(Tensor self, int[2] "
                          "output_size, bool align_corners, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bicubic2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bicubic2d(Tensor self, int[2] "
                          "output_size, bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bicubic2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bicubic2d_backward.grad_input(Tensor "
                          "grad_output, int[2] output_size, int[4] input_size, "
                          "bool align_corners, *, Tensor(a!) grad_input) -> "
                          "Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bicubic2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_bicubic2d_backward(Tensor "
                          "grad_output, int[2] output_size, int[4] input_size, "
                          "bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_bicubic2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_trilinear3d.out(Tensor self, int[3] "
                          "output_size, bool align_corners, *, Tensor(a!) out) "
                          "-> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_trilinear3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_trilinear3d(Tensor self, int[3] "
                          "output_size, bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_trilinear3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_trilinear3d_backward.grad_input("
                          "Tensor grad_output, int[3] output_size, int[5] "
                          "input_size, bool align_corners, *, Tensor(a!) "
                          "grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_trilinear3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_trilinear3d_backward(Tensor "
                          "grad_output, int[3] output_size, int[5] input_size, "
                          "bool align_corners) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, bool),
                      &ATenMLIRTypeDefault::upsample_trilinear3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest1d.out(Tensor self, int[1] "
                          "output_size, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest1d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest1d(Tensor self, int[1] "
                          "output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest1d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest1d_backward.grad_input(Tensor "
                          "grad_output, int[1] output_size, int[3] input_size, "
                          "*, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest1d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::upsample_nearest1d_backward(Tensor grad_output, "
                      "int[1] output_size, int[3] input_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest1d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest2d.out(Tensor self, int[2] "
                          "output_size, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest2d(Tensor self, int[2] "
                          "output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest2d_backward.grad_input(Tensor "
                          "grad_output, int[2] output_size, int[4] input_size, "
                          "*, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::upsample_nearest2d_backward(Tensor grad_output, "
                      "int[2] output_size, int[4] input_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest3d.out(Tensor self, int[3] "
                          "output_size, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest3d(Tensor self, int[3] "
                          "output_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::upsample_nearest3d_backward.grad_input(Tensor "
                          "grad_output, int[3] output_size, int[5] input_size, "
                          "*, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::upsample_nearest3d_backward(Tensor grad_output, "
                      "int[3] output_size, int[5] input_size) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::upsample_nearest3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::sigmoid_backward.grad_input(Tensor grad_output, "
                      "Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::sigmoid_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::sigmoid_backward(Tensor grad_output, Tensor "
                          "output) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::sigmoid_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::tanh_backward.grad_input(Tensor grad_output, "
                      "Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &),
                      &ATenMLIRTypeDefault::tanh_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::tanh_backward(Tensor grad_output, Tensor "
                          "output) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(const at::Tensor &,
                                                     const at::Tensor &),
                                          &ATenMLIRTypeDefault::tanh_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::slow_conv_transpose2d.out(Tensor self, Tensor "
                      "weight, int[2] kernel_size, Tensor? bias=None, int[2] "
                      "stride=1, int[2] padding=0, int[2] output_padding=0, "
                      "int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(
                          at::Tensor &, const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, const at::Tensor &, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::slow_conv_transpose2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slow_conv_transpose2d(Tensor self, Tensor "
                          "weight, int[2] kernel_size, Tensor? bias=None, "
                          "int[2] stride=1, int[2] padding=0, int[2] "
                          "output_padding=0, int[2] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::slow_conv_transpose2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slow_conv_transpose2d_backward.grad_output("
                          "Tensor grad_output, Tensor self, Tensor weight, "
                          "int[2] kernel_size, int[2] stride, int[2] padding, "
                          "int[2] output_padding, int[2] dilation, Tensor "
                          "columns, Tensor ones, *, Tensor?(a!) grad_input, "
                          "Tensor?(b!) grad_weight, Tensor?(c!) grad_bias) -> "
                          "(Tensor(a!), Tensor(b!), Tensor(c!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::slow_conv_transpose2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::slow_conv_transpose2d_backward.output_mask(Tensor "
                      "grad_output, Tensor self, Tensor weight, int[2] "
                      "kernel_size, int[2] stride, int[2] padding, int[2] "
                      "output_padding, int[2] dilation, Tensor columns, Tensor "
                      "ones, bool[3] output_mask) -> (Tensor grad_input, "
                      "Tensor grad_weight, Tensor grad_bias)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          const at::Tensor &, const at::Tensor &,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::slow_conv_transpose2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::slow_conv_transpose3d.out(Tensor self, Tensor "
                      "weight, int[3] kernel_size, Tensor? bias=None, int[3] "
                      "stride=1, int[3] padding=0, int[3] output_padding=0, "
                      "int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(
                          at::Tensor &, const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, const at::Tensor &, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::slow_conv_transpose3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slow_conv_transpose3d(Tensor self, Tensor "
                          "weight, int[3] kernel_size, Tensor? bias=None, "
                          "int[3] stride=1, int[3] padding=0, int[3] "
                          "output_padding=0, int[3] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::slow_conv_transpose3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slow_conv_transpose3d_backward.grad_output("
                          "Tensor grad_output, Tensor self, Tensor weight, "
                          "int[3] kernel_size, int[3] stride, int[3] padding, "
                          "int[3] output_padding, int[3] dilation, Tensor "
                          "finput, Tensor fgrad_input, *, Tensor?(a!) "
                          "grad_input, Tensor?(b!) grad_weight, Tensor?(c!) "
                          "grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          const at::Tensor &, const at::Tensor &),
                      &ATenMLIRTypeDefault::slow_conv_transpose3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::slow_conv_transpose3d_backward.output_mask(Tensor "
                      "grad_output, Tensor self, Tensor weight, int[3] "
                      "kernel_size, int[3] stride, int[3] padding, int[3] "
                      "output_padding, int[3] dilation, Tensor finput, Tensor "
                      "fgrad_input, bool[3] output_mask) -> (Tensor "
                      "grad_input, Tensor grad_weight, Tensor grad_bias)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          const at::Tensor &, const at::Tensor &,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::slow_conv_transpose3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::thnn_conv2d.out(Tensor self, Tensor weight, "
                      "int[2] kernel_size, Tensor? bias=None, int[2] stride=1, "
                      "int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv2d(Tensor self, Tensor weight, "
                          "int[2] kernel_size, Tensor? bias=None, int[2] "
                          "stride=1, int[2] padding=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv2d_forward.output(Tensor self, "
                          "Tensor weight, int[2] kernel_size, Tensor? bias, "
                          "int[2] stride, int[2] padding, *, Tensor(a!) "
                          "output, Tensor(b!) finput, Tensor(c!) fgrad_input) "
                          "-> (Tensor(a!), Tensor(b!), Tensor(c!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, const at::Tensor &, at::IntArrayRef,
                          at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv2d_forward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv2d_forward(Tensor self, Tensor "
                          "weight, int[2] kernel_size, Tensor? bias, int[2] "
                          "stride, int[2] padding) -> (Tensor output, Tensor "
                          "finput, Tensor fgrad_input)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, const at::Tensor &, at::IntArrayRef,
                          at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv2d_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv2d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor weight, int[2] "
                          "kernel_size, int[2] stride, int[2] padding, Tensor "
                          "finput, Tensor fgrad_input, *, Tensor?(a!) "
                          "grad_input, Tensor?(b!) grad_weight, Tensor?(c!) "
                          "grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, const at::Tensor &,
                          const at::Tensor &),
                      &ATenMLIRTypeDefault::thnn_conv2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv2d_backward.output_mask(Tensor "
                          "grad_output, Tensor self, Tensor weight, int[2] "
                          "kernel_size, int[2] stride, int[2] padding, Tensor "
                          "finput, Tensor fgrad_input, bool[3] output_mask) -> "
                          "(Tensor grad_input, Tensor grad_weight, Tensor "
                          "grad_bias)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, const at::Tensor &,
                          const at::Tensor &, std::array<bool, 3>),
                      &ATenMLIRTypeDefault::thnn_conv2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv_depthwise2d.out(Tensor self, Tensor "
                          "weight, int[2] kernel_size, Tensor? bias=None, "
                          "int[2] stride=1, int[2] padding=0, int[2] "
                          "dilation=1, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv_depthwise2d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::thnn_conv_depthwise2d(Tensor self, Tensor weight, "
                      "int[2] kernel_size, Tensor? bias=None, int[2] stride=1, "
                      "int[2] padding=0, int[2] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv_depthwise2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv_depthwise2d_forward.out(Tensor "
                          "self, Tensor weight, int[2] kernel_size, Tensor? "
                          "bias, int[2] stride, int[2] padding, int[2] "
                          "dilation, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv_depthwise2d_forward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::thnn_conv_depthwise2d_forward(Tensor self, Tensor "
                      "weight, int[2] kernel_size, Tensor? bias, int[2] "
                      "stride, int[2] padding, int[2] dilation) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv_depthwise2d_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::thnn_conv_depthwise2d_backward.grad_input(Tensor "
                      "grad_output, Tensor self, Tensor weight, int[2] "
                      "kernel_size, int[2] stride, int[2] padding, int[2] "
                      "dilation, *, Tensor?(a!) grad_input, Tensor?(b!) "
                      "grad_weight) -> (Tensor(a!), Tensor(b!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, const at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv_depthwise2d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv_depthwise2d_backward.output_mask("
                          "Tensor grad_output, Tensor self, Tensor weight, "
                          "int[2] kernel_size, int[2] stride, int[2] padding, "
                          "int[2] dilation, bool[2] output_mask) -> (Tensor "
                          "grad_input, Tensor grad_weight)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor>(
                          const at::Tensor &,
                          const at::Tensor &,
                          const at::Tensor &,
                          at::IntArrayRef, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, std::array<bool, 2>),
                      &ATenMLIRTypeDefault::thnn_conv_depthwise2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::thnn_conv3d.out(Tensor self, Tensor weight, "
                      "int[3] kernel_size, Tensor? bias=None, int[3] stride=1, "
                      "int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   const at::Tensor &, at::IntArrayRef,
                                   const at::Tensor &, at::IntArrayRef,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv3d_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv3d(Tensor self, Tensor weight, "
                          "int[3] kernel_size, Tensor? bias=None, int[3] "
                          "stride=1, int[3] padding=0) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv3d_forward.output(Tensor self, "
                          "Tensor weight, int[3] kernel_size, Tensor? bias, "
                          "int[3] stride, int[3] padding, *, Tensor(a!) "
                          "output, Tensor(b!) finput, Tensor(c!) fgrad_input) "
                          "-> (Tensor(a!), Tensor(b!), Tensor(c!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, const at::Tensor &, at::IntArrayRef,
                          at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv3d_forward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv3d_forward(Tensor self, Tensor "
                          "weight, int[3] kernel_size, Tensor? bias, int[3] "
                          "stride, int[3] padding) -> (Tensor output, Tensor "
                          "finput, Tensor fgrad_input)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          at::IntArrayRef, const at::Tensor &, at::IntArrayRef,
                          at::IntArrayRef),
                      &ATenMLIRTypeDefault::thnn_conv3d_forward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv3d_backward.grad_input(Tensor "
                          "grad_output, Tensor self, Tensor weight, int[3] "
                          "kernel_size, int[3] stride, int[3] padding, Tensor "
                          "finput, Tensor fgrad_input, *, Tensor?(a!) "
                          "grad_input, Tensor?(b!) grad_weight, Tensor?(c!) "
                          "grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor &, at::Tensor &, at::Tensor &>(
                          at::Tensor &, at::Tensor &, at::Tensor &,
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, const at::Tensor &,
                          const at::Tensor &),
                      &ATenMLIRTypeDefault::thnn_conv3d_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::thnn_conv3d_backward.output_mask(Tensor "
                          "grad_output, Tensor self, Tensor weight, int[3] "
                          "kernel_size, int[3] stride, int[3] padding, Tensor "
                          "finput, Tensor fgrad_input, bool[3] output_mask) -> "
                          "(Tensor grad_input, Tensor grad_weight, Tensor "
                          "grad_bias)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, const at::Tensor &,
                          const at::Tensor &, std::array<bool, 3>),
                      &ATenMLIRTypeDefault::thnn_conv3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::slow_conv_dilated2d(Tensor self, Tensor weight, "
                      "int[2] kernel_size, Tensor? bias=None, int[2] stride=1, "
                      "int[2] padding=0, int[2] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::slow_conv_dilated2d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slow_conv_dilated2d_backward(Tensor "
                          "grad_output, Tensor self, Tensor weight, int[2] "
                          "kernel_size, int[2] stride, int[2] padding, int[2] "
                          "dilation, bool[3] output_mask) -> (Tensor "
                          "grad_input, Tensor grad_weight, Tensor grad_bias)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::slow_conv_dilated2d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::slow_conv_dilated3d(Tensor self, Tensor weight, "
                      "int[3] kernel_size, Tensor? bias=None, int[3] stride=1, "
                      "int[3] padding=0, int[3] dilation=1) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, const at::Tensor &,
                                 at::IntArrayRef, const at::Tensor &,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::slow_conv_dilated3d>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::slow_conv_dilated3d_backward(Tensor "
                          "grad_output, Tensor self, Tensor weight, int[3] "
                          "kernel_size, int[3] stride, int[3] padding, int[3] "
                          "dilation, bool[3] output_mask) -> (Tensor "
                          "grad_input, Tensor grad_weight, Tensor grad_bias)")
                  .impl_unboxedOnlyKernel<
                      std::tuple<at::Tensor, at::Tensor, at::Tensor>(
                          const at::Tensor &, const at::Tensor &,
                          const at::Tensor &, at::IntArrayRef, at::IntArrayRef,
                          at::IntArrayRef, at::IntArrayRef,
                          std::array<bool, 3>),
                      &ATenMLIRTypeDefault::slow_conv_dilated3d_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::col2im.out(Tensor self, int[2] output_size, "
                      "int[2] kernel_size, int[2] dilation, int[2] padding, "
                      "int[2] stride, *, Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, at::IntArrayRef,
                                              at::IntArrayRef, at::IntArrayRef,
                                              at::IntArrayRef),
                                          &ATenMLIRTypeDefault::col2im_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::col2im(Tensor self, int[2] output_size, "
                          "int[2] kernel_size, int[2] dilation, int[2] "
                          "padding, int[2] stride) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::col2im>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::col2im_backward.grad_input(Tensor grad_output, "
                      "int[2] kernel_size, int[2] dilation, int[2] padding, "
                      "int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::col2im_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::col2im_backward(Tensor grad_output, int[2] "
                          "kernel_size, int[2] dilation, int[2] padding, "
                          "int[2] stride) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef),
                      &ATenMLIRTypeDefault::col2im_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::im2col.out(Tensor self, int[2] kernel_size, "
                          "int[2] dilation, int[2] padding, int[2] stride, *, "
                          "Tensor(a!) out) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<at::Tensor &(
                                              at::Tensor &, const at::Tensor &,
                                              at::IntArrayRef, at::IntArrayRef,
                                              at::IntArrayRef, at::IntArrayRef),
                                          &ATenMLIRTypeDefault::im2col_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema(
                      "aten::im2col(Tensor self, int[2] kernel_size, int[2] "
                      "dilation, int[2] padding, int[2] stride) -> Tensor")
                  .impl_unboxedOnlyKernel<at::Tensor(
                                              const at::Tensor &,
                                              at::IntArrayRef, at::IntArrayRef,
                                              at::IntArrayRef, at::IntArrayRef),
                                          &ATenMLIRTypeDefault::im2col>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::im2col_backward.grad_input(Tensor "
                          "grad_output, int[2] input_size, int[2] kernel_size, "
                          "int[2] dilation, int[2] padding, int[2] stride, *, "
                          "Tensor(a!) grad_input) -> Tensor(a!)")
                  .impl_unboxedOnlyKernel<
                      at::Tensor &(at::Tensor &, const at::Tensor &,
                                   at::IntArrayRef, at::IntArrayRef,
                                   at::IntArrayRef, at::IntArrayRef,
                                   at::IntArrayRef),
                      &ATenMLIRTypeDefault::im2col_backward_out>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
          .op(torch::RegisterOperators::options()
                  .schema("aten::im2col_backward(Tensor grad_output, int[2] "
                          "input_size, int[2] kernel_size, int[2] dilation, "
                          "int[2] padding, int[2] stride) -> Tensor")
                  .impl_unboxedOnlyKernel<
                      at::Tensor(const at::Tensor &, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef,
                                 at::IntArrayRef, at::IntArrayRef),
                      &ATenMLIRTypeDefault::im2col_backward>(
                      at::TensorTypeId::XLATensorId)
                  .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA));
}

} // namespace torch_mlir
